[2025-06-27 12:32:15,751] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/clc_hcmus/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-06-27 12:32:19,812] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Not using distributed mode
Namespace(batch_size=8, epochs=35, update_freq=1, save_ckpt_freq=10, model='vit_base_patch16_224', tubelet_size=2, input_size=224, with_checkpoint=False, drop=0.0, attn_drop_rate=0.0, drop_path=0.3, head_drop_rate=0.0, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=5.0, momentum=0.9, weight_decay=0.1, weight_decay_end=None, lr=0.001, layer_decay=0.9, warmup_lr=1e-08, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=5, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='./vit_b_k710_dl_from_giant.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_mean_pooling=True, data_path='./data/', data_root='', eval_data_path=None, nb_classes=10, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, sparse_sample=False, data_set='Kinetics-400', fname_tmpl='img_{:05}.jpg', start_idx=1, output_dir='./work_dir/', log_dir='./work_dir/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, validation=False, dist_eval=True, num_workers=8, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='./work_dir/deepspeed_config.json', deepscale=False, deepscale_config=None, distributed=False)
Number of the class = 10
Number of the class = 10
Number of the class = 10
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155554b99ba0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from ./vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Convert K710 head to Kinetics-400 head
Convert K710 head to Kinetics-400 head
size mismatch for head.weight: copying a param with shape torch.Size([400, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).
size mismatch for head.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([10]).
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272729203104973)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10909091681241989)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.13636364042758942)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.16363637149333954)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.19090908765792847)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.2181818187236786)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.2454545497894287)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.27272728085517883)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.30000001192092896)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=10, bias=True)
)
number of params: 86234890
LR = 0.00003125
Batch size = 8
Update frequent = 1
Number of training examples = 3870
Number of training training per epoch = 483
Assigned values = [0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.1,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_1_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_2_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_3_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_4_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_5_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_6_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441
  },
  "layer_7_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_8_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561
  },
  "layer_9_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_10_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.81
  },
  "layer_11_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.81
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.9
  },
  "layer_12_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.9
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.1,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-06-27 12:32:25,742] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown
[2025-06-27 12:32:25,742] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-06-27 12:32:25,743] [INFO] [comm.py:690:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-27 12:32:28,516] [INFO] [comm.py:745:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.204.2.101, master_port=29500
[2025-06-27 12:32:28,517] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-27 12:32:28,552] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 1
[2025-06-27 12:32:28,669] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=1
	 self.mp_world_size=1
	 self.seq_dp_world_size=1
	 self.sequence_parallel_size=1
***********************************************
[2025-06-27 12:32:29,549] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.5352809429168701 seconds
[2025-06-27 12:32:30,093] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-06-27 12:32:30,094] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-27 12:32:30,098] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-06-27 12:32:30,098] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-06-27 12:32:30,168] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = FP16_Optimizer
[2025-06-27 12:32:30,168] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-06-27 12:32:30,169] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-27 12:32:30,170] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 12:32:30,172] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-06-27 12:32:30,172] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-06-27 12:32:30,174] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-27 12:32:30,174] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-06-27 12:32:30,174] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-06-27 12:32:30,175] [INFO] [config.py:925:print]   amp_params ................... False
[2025-06-27 12:32:30,176] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-27 12:32:30,177] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False
[2025-06-27 12:32:30,178] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-06-27 12:32:30,179] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-06-27 12:32:30,180] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-06-27 12:32:30,180] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-06-27 12:32:30,181] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x15542f5c7490>
[2025-06-27 12:32:30,182] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-06-27 12:32:30,183] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-06-27 12:32:30,184] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-27 12:32:30,185] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-06-27 12:32:30,186] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-06-27 12:32:30,186] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-27 12:32:30,187] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-06-27 12:32:30,188] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-06-27 12:32:30,189] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-06-27 12:32:30,189] [INFO] [config.py:925:print]   dump_state ................... False
[2025-06-27 12:32:30,190] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-06-27 12:32:30,190] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-27 12:32:30,191] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-27 12:32:30,192] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-06-27 12:32:30,193] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-06-27 12:32:30,194] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-06-27 12:32:30,194] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-06-27 12:32:30,195] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-06-27 12:32:30,196] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-06-27 12:32:30,196] [INFO] [config.py:925:print]   float16_config ............... enabled=True auto_cast=False loss_scale=0.0 initial_scale_power=7 loss_scale_window=128 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-06-27 12:32:30,197] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-27 12:32:30,198] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-06-27 12:32:30,198] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-06-27 12:32:30,199] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 1
[2025-06-27 12:32:30,200] [INFO] [config.py:925:print]   gradient_clipping ............ 5
[2025-06-27 12:32:30,201] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-06-27 12:32:30,202] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-06-27 12:32:30,203] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-27 12:32:30,204] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-06-27 12:32:30,204] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-06-27 12:32:30,205] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-06-27 12:32:30,205] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-06-27 12:32:30,206] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-06-27 12:32:30,207] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-27 12:32:30,208] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-06-27 12:32:30,209] [INFO] [config.py:925:print]   optimizer_name ............... adam
[2025-06-27 12:32:30,209] [INFO] [config.py:925:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.1, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-06-27 12:32:30,210] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-27 12:32:30,211] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-06-27 12:32:30,212] [INFO] [config.py:925:print]   pld_params ................... False
[2025-06-27 12:32:30,213] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-06-27 12:32:30,214] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-06-27 12:32:30,215] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-06-27 12:32:30,215] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-27 12:32:30,216] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-06-27 12:32:30,217] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-06-27 12:32:30,218] [INFO] [config.py:925:print]   steps_per_print .............. 1000
[2025-06-27 12:32:30,219] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-06-27 12:32:30,220] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-06-27 12:32:30,221] [INFO] [config.py:925:print]   train_batch_size ............. 8
[2025-06-27 12:32:30,222] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  8
[2025-06-27 12:32:30,223] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-06-27 12:32:30,224] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-06-27 12:32:30,225] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-06-27 12:32:30,226] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-06-27 12:32:30,226] [INFO] [config.py:925:print]   world_size ................... 1
[2025-06-27 12:32:30,226] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  False
[2025-06-27 12:32:30,228] [INFO] [config.py:925:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-06-27 12:32:30,228] [INFO] [config.py:925:print]   zero_enabled ................. False
[2025-06-27 12:32:30,229] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-27 12:32:30,230] [INFO] [config.py:925:print]   zero_optimization_stage ...... 0
[2025-06-27 12:32:30,231] [INFO] [config.py:911:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 8, 
    "steps_per_print": 1000, 
    "gradient_clipping": 5, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.1, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 2415
Set warmup steps = 0
Max WD = 0.1000000, Min WD = 0.1000000
criterion = SoftTargetCrossEntropy()
Start training for 35 epochs
Epoch: [0]  [  0/483]  eta: 0:48:29  lr: 0.000000  min_lr: 0.000000  loss: 2.3027 (2.3027)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  time: 6.0230 (6.0230 -- 6.0230)  data: 3.9341 (3.9341 -- 3.9341)  max mem: 21487
Epoch: [0]  [ 20/483]  eta: 0:08:24  lr: 0.000000  min_lr: 0.000000  loss: 2.3027 (2.3027)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6743 (1.7452)  time: 0.8432 (0.6314 -- 0.9870)  data: 0.0003 (0.0001 -- 0.0024)  max mem: 21487
Epoch: [0]  [ 40/483]  eta: 0:07:01  lr: 0.000001  min_lr: 0.000000  loss: 2.3027 (2.3027)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6811 (1.7076)  time: 0.8043 (0.6607 -- 1.0104)  data: 0.0002 (0.0001 -- 0.0013)  max mem: 21487
Epoch: [0]  [ 60/483]  eta: 0:06:17  lr: 0.000001  min_lr: 0.000000  loss: 2.3025 (2.3026)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7696 (1.7302)  time: 0.7768 (0.5606 -- 1.0248)  data: 0.0005 (0.0001 -- 0.0048)  max mem: 21487
Epoch: [0]  [ 80/483]  eta: 0:05:47  lr: 0.000001  min_lr: 0.000000  loss: 2.3017 (2.3024)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7259 (1.7424)  time: 0.7699 (0.4402 -- 0.9901)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [0]  [100/483]  eta: 0:05:24  lr: 0.000001  min_lr: 0.000000  loss: 2.3014 (2.3022)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6073 (1.7307)  time: 0.7838 (0.5883 -- 1.0271)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [0]  [120/483]  eta: 0:05:03  lr: 0.000002  min_lr: 0.000000  loss: 2.3009 (2.3020)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6890 (1.7259)  time: 0.7760 (0.6131 -- 1.0174)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
[2025-06-27 12:34:17,471] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:34:17,472] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/483]  eta: 0:04:44  lr: 0.000002  min_lr: 0.000000  loss: 2.2999 (2.3017)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7440 (1.7311)  time: 0.7975 (0.6249 -- 1.0043)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [0]  [160/483]  eta: 0:04:28  lr: 0.000002  min_lr: 0.000001  loss: 2.2993 (2.3014)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8150 (1.7423)  time: 0.8317 (0.6487 -- 1.0275)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [0]  [180/483]  eta: 0:04:12  lr: 0.000002  min_lr: 0.000001  loss: 2.2982 (2.3010)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7418 (1.7401)  time: 0.8506 (0.5892 -- 1.0383)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [0]  [200/483]  eta: 0:03:54  lr: 0.000003  min_lr: 0.000001  loss: 2.2962 (2.3006)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6902 (1.7427)  time: 0.7844 (0.5986 -- 1.0063)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [0]  [220/483]  eta: 0:03:33  lr: 0.000003  min_lr: 0.000001  loss: 2.2958 (2.3001)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7898 (1.7493)  time: 0.6605 (0.4520 -- 0.9711)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [0]  [240/483]  eta: 0:03:11  lr: 0.000003  min_lr: 0.000001  loss: 2.2932 (2.2995)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6788 (1.7491)  time: 0.5380 (0.3513 -- 0.6107)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 12:35:49,185] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:35:49,186] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [260/483]  eta: 0:02:51  lr: 0.000003  min_lr: 0.000001  loss: 2.2912 (2.2989)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8529 (1.7553)  time: 0.5366 (0.3963 -- 0.6139)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [0]  [280/483]  eta: 0:02:32  lr: 0.000004  min_lr: 0.000001  loss: 2.2883 (2.2982)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8754 (1.7647)  time: 0.5280 (0.4521 -- 0.6022)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [0]  [300/483]  eta: 0:02:14  lr: 0.000004  min_lr: 0.000001  loss: 2.2881 (2.2975)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8315 (1.7706)  time: 0.5167 (0.4203 -- 0.6030)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [0]  [320/483]  eta: 0:01:57  lr: 0.000004  min_lr: 0.000001  loss: 2.2857 (2.2967)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7925 (1.7756)  time: 0.4946 (0.4474 -- 0.5825)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [0]  [340/483]  eta: 0:01:41  lr: 0.000004  min_lr: 0.000001  loss: 2.2800 (2.2956)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.9010 (1.7877)  time: 0.5026 (0.3988 -- 0.5633)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [0]  [360/483]  eta: 0:01:26  lr: 0.000005  min_lr: 0.000001  loss: 2.2758 (2.2945)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8721 (1.7976)  time: 0.5359 (0.4519 -- 0.6049)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [0]  [380/483]  eta: 0:01:11  lr: 0.000005  min_lr: 0.000001  loss: 2.2689 (2.2932)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0165 (1.8129)  time: 0.5321 (0.4508 -- 0.6368)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 12:36:55,703] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:36:55,704] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [400/483]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 2.2609 (2.2917)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.9737 (1.8248)  time: 0.5248 (0.3882 -- 0.6022)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [0]  [420/483]  eta: 0:00:42  lr: 0.000005  min_lr: 0.000001  loss: 2.2589 (2.2903)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0476 (1.8337)  time: 0.5487 (0.4577 -- 0.6074)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [0]  [440/483]  eta: 0:00:28  lr: 0.000006  min_lr: 0.000001  loss: 2.2560 (2.2888)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8499 (1.8404)  time: 0.5317 (0.3947 -- 0.6046)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [0]  [460/483]  eta: 0:00:15  lr: 0.000006  min_lr: 0.000002  loss: 2.2411 (2.2869)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0439 (1.8509)  time: 0.5471 (0.4081 -- 0.6062)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [0]  [480/483]  eta: 0:00:01  lr: 0.000006  min_lr: 0.000002  loss: 2.2381 (2.2851)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0239 (1.8619)  time: 0.5584 (0.4533 -- 0.6076)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [0]  [482/483]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 2.2381 (2.2849)  loss_scale: 1024.0000 (447.3375)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0239 (1.8622)  time: 0.5542 (0.4533 -- 0.6076)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [0] Total time: 0:05:18 (0.6596 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 2.2381 (2.2849)  loss_scale: 1024.0000 (447.3375)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0239 (1.8622)
Val:  [ 0/23]  eta: 0:00:54  loss: 2.2074 (2.2074)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 2.3602 (2.3602 -- 2.3602)  data: 1.5853 (1.5853 -- 1.5853)  max mem: 21487
Val:  [10/23]  eta: 0:00:05  loss: 2.2074 (2.2081)  acc1: 83.3333 (84.0909)  acc5: 100.0000 (94.6970)  time: 0.3936 (0.1397 -- 2.3602)  data: 0.1444 (0.0002 -- 1.5853)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 2.2044 (2.2057)  acc1: 91.6667 (85.7143)  acc5: 100.0000 (96.4286)  time: 0.1956 (0.1397 -- 0.2357)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 2.2091 (2.2063)  acc1: 83.3333 (84.5018)  acc5: 100.0000 (96.3100)  time: 0.1947 (0.1397 -- 0.2408)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Val: Total time: 0:00:06 (0.2916 s / it)
* Acc@1 84.502 Acc@5 96.310 loss 2.206
Accuracy of the network on the 271 val images: 84.50%
[2025-06-27 12:37:57,004] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 12:37:57,016] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 84.50%
Epoch: [1]  [  0/483]  eta: 0:34:57  lr: 0.000006  min_lr: 0.000002  loss: 2.2485 (2.2485)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1938 (2.1938)  time: 4.3423 (4.3423 -- 4.3423)  data: 3.7627 (3.7627 -- 3.7627)  max mem: 21487
Epoch: [1]  [ 20/483]  eta: 0:05:25  lr: 0.000007  min_lr: 0.000002  loss: 2.2359 (2.2348)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0409 (2.0599)  time: 0.5203 (0.4452 -- 0.6032)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 12:38:20,940] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:38:20,941] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [1]  [ 40/483]  eta: 0:04:28  lr: 0.000007  min_lr: 0.000002  loss: 2.2365 (2.2340)  loss_scale: 2048.0000 (1323.7073)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8987 (2.0020)  time: 0.5031 (0.3964 -- 0.5980)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [1]  [ 60/483]  eta: 0:04:08  lr: 0.000007  min_lr: 0.000002  loss: 2.2166 (2.2300)  loss_scale: 2048.0000 (1561.1803)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2312 (2.0700)  time: 0.5538 (0.4540 -- 0.6065)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [1]  [ 80/483]  eta: 0:03:53  lr: 0.000007  min_lr: 0.000002  loss: 2.2152 (2.2258)  loss_scale: 2048.0000 (1681.3827)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0983 (2.0893)  time: 0.5555 (0.4487 -- 0.6123)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [1]  [100/483]  eta: 0:03:38  lr: 0.000008  min_lr: 0.000002  loss: 2.2099 (2.2235)  loss_scale: 2048.0000 (1753.9802)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.9368 (2.0869)  time: 0.5322 (0.4370 -- 0.6051)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [1]  [120/483]  eta: 0:03:24  lr: 0.000008  min_lr: 0.000002  loss: 2.2033 (2.2192)  loss_scale: 2048.0000 (1802.5785)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0938 (2.0932)  time: 0.5277 (0.3351 -- 0.6021)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [1]  [140/483]  eta: 0:03:12  lr: 0.000008  min_lr: 0.000002  loss: 2.1946 (2.2151)  loss_scale: 2048.0000 (1837.3901)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1384 (2.1106)  time: 0.5539 (0.4677 -- 0.6074)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 12:39:30,613] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:39:30,613] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [1]  [160/483]  eta: 0:03:01  lr: 0.000008  min_lr: 0.000002  loss: 2.1868 (2.2110)  loss_scale: 2048.0000 (1914.4348)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1768 (2.1205)  time: 0.5612 (0.4332 -- 0.6052)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [1]  [180/483]  eta: 0:02:49  lr: 0.000009  min_lr: 0.000002  loss: 2.1683 (2.2069)  loss_scale: 4096.0000 (2155.4917)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1623 (2.1219)  time: 0.5280 (0.3836 -- 0.6042)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [1]  [200/483]  eta: 0:02:37  lr: 0.000009  min_lr: 0.000002  loss: 2.1612 (2.2024)  loss_scale: 4096.0000 (2348.5771)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0222 (2.1185)  time: 0.5272 (0.4003 -- 0.6044)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [1]  [220/483]  eta: 0:02:26  lr: 0.000009  min_lr: 0.000002  loss: 2.1507 (2.1980)  loss_scale: 4096.0000 (2506.7149)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0224 (2.1232)  time: 0.5713 (0.4648 -- 0.6150)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [1]  [240/483]  eta: 0:02:14  lr: 0.000009  min_lr: 0.000002  loss: 2.1421 (2.1929)  loss_scale: 4096.0000 (2638.6058)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0052 (2.1216)  time: 0.5178 (0.4102 -- 0.5969)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [1]  [260/483]  eta: 0:02:02  lr: 0.000010  min_lr: 0.000002  loss: 2.1294 (2.1888)  loss_scale: 4096.0000 (2750.2835)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1088 (2.1255)  time: 0.5174 (0.4589 -- 0.6009)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [1]  [280/483]  eta: 0:01:51  lr: 0.000010  min_lr: 0.000003  loss: 2.1259 (2.1843)  loss_scale: 4096.0000 (2846.0641)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1406 (2.1301)  time: 0.5479 (0.4418 -- 0.6177)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 12:40:39,099] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:40:39,100] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [1]  [300/483]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000003  loss: 2.1312 (2.1805)  loss_scale: 8192.0000 (3146.8439)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0681 (2.1281)  time: 0.5186 (0.3835 -- 0.6047)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [1]  [320/483]  eta: 0:01:29  lr: 0.000010  min_lr: 0.000003  loss: 2.1004 (2.1758)  loss_scale: 8192.0000 (3461.1838)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1444 (2.1303)  time: 0.5218 (0.4554 -- 0.5750)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [1]  [340/483]  eta: 0:01:18  lr: 0.000011  min_lr: 0.000003  loss: 2.0841 (2.1696)  loss_scale: 8192.0000 (3738.6510)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2141 (2.1400)  time: 0.5344 (0.4086 -- 0.6113)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [1]  [360/483]  eta: 0:01:07  lr: 0.000011  min_lr: 0.000003  loss: 2.0628 (2.1641)  loss_scale: 8192.0000 (3985.3740)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1831 (2.1448)  time: 0.5417 (0.4599 -- 0.6025)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [1]  [380/483]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000003  loss: 2.0338 (2.1576)  loss_scale: 8192.0000 (4206.1942)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3835 (2.1580)  time: 0.5289 (0.4045 -- 0.6036)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [1]  [400/483]  eta: 0:00:45  lr: 0.000011  min_lr: 0.000003  loss: 2.0266 (2.1515)  loss_scale: 8192.0000 (4404.9875)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2454 (2.1648)  time: 0.5106 (0.3850 -- 0.5833)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 12:41:46,699] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:41:46,699] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [1]  [420/483]  eta: 0:00:34  lr: 0.000012  min_lr: 0.000003  loss: 2.0070 (2.1461)  loss_scale: 8192.0000 (4740.5606)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2154 (2.1655)  time: 0.5205 (0.4490 -- 0.5949)  data: 0.0003 (0.0001 -- 0.0012)  max mem: 21487
Epoch: [1]  [440/483]  eta: 0:00:23  lr: 0.000012  min_lr: 0.000003  loss: 2.0454 (2.1412)  loss_scale: 16384.0000 (5268.6077)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1751 (2.1655)  time: 0.5231 (0.4779 -- 0.6028)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [1]  [460/483]  eta: 0:00:12  lr: 0.000012  min_lr: 0.000003  loss: 1.9932 (2.1350)  loss_scale: 16384.0000 (5750.8373)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1649 (2.1687)  time: 0.5382 (0.4131 -- 0.6030)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [1]  [480/483]  eta: 0:00:01  lr: 0.000012  min_lr: 0.000003  loss: 1.9919 (2.1289)  loss_scale: 16384.0000 (6192.9647)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2235 (2.1751)  time: 0.5394 (0.4096 -- 0.6054)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [1]  [482/483]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.9855 (2.1285)  loss_scale: 16384.0000 (6235.1636)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2613 (2.1754)  time: 0.5465 (0.4096 -- 0.6060)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [1] Total time: 0:04:21 (0.5414 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.9855 (2.1285)  loss_scale: 16384.0000 (6235.1636)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2613 (2.1754)
Val:  [ 0/23]  eta: 0:00:30  loss: 1.8284 (1.8284)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 1.3177 (1.3177 -- 1.3177)  data: 1.0986 (1.0986 -- 1.0986)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 1.8241 (1.8220)  acc1: 83.3333 (84.8485)  acc5: 100.0000 (96.9697)  time: 0.2977 (0.1464 -- 1.3177)  data: 0.1002 (0.0002 -- 1.0986)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 1.8064 (1.8103)  acc1: 91.6667 (86.5079)  acc5: 100.0000 (98.0159)  time: 0.1976 (0.1464 -- 0.2094)  data: 0.0003 (0.0002 -- 0.0005)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 1.8241 (1.8140)  acc1: 83.3333 (85.2399)  acc5: 100.0000 (98.1550)  time: 0.1960 (0.1182 -- 0.2094)  data: 0.0003 (0.0002 -- 0.0005)  max mem: 21487
Val: Total time: 0:00:05 (0.2432 s / it)
* Acc@1 85.240 Acc@5 98.155 loss 1.814
Accuracy of the network on the 271 val images: 85.24%
[2025-06-27 12:42:29,073] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 12:42:29,078] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 85.24%
Epoch: [2]  [  0/483]  eta: 0:33:47  lr: 0.000013  min_lr: 0.000003  loss: 2.0526 (2.0526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4192 (2.4192)  time: 4.1976 (4.1976 -- 4.1976)  data: 3.7195 (3.7195 -- 3.7195)  max mem: 21487
Epoch: [2]  [ 20/483]  eta: 0:05:25  lr: 0.000013  min_lr: 0.000003  loss: 1.9717 (1.9802)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2171 (2.2455)  time: 0.5288 (0.4769 -- 0.5908)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 12:42:56,066] [INFO] [logging.py:107:log_dist] [Rank 0] step=1000, skipped=0, lr=[np.float64(3.2872358669488814e-06), np.float64(3.2872358669488814e-06), np.float64(3.6524842966098688e-06), np.float64(3.6524842966098688e-06), np.float64(4.058315885122076e-06), np.float64(4.058315885122076e-06), np.float64(4.509239872357862e-06), np.float64(4.509239872357862e-06), np.float64(5.010266524842069e-06), np.float64(5.010266524842069e-06), np.float64(5.566962805380076e-06), np.float64(5.566962805380076e-06), np.float64(6.185514228200084e-06), np.float64(6.185514228200084e-06), np.float64(6.872793586888982e-06), np.float64(6.872793586888982e-06), np.float64(7.636437318765535e-06), np.float64(7.636437318765535e-06), np.float64(8.484930354183928e-06), np.float64(8.484930354183928e-06), np.float64(9.427700393537699e-06), np.float64(9.427700393537699e-06), np.float64(1.0475222659486332e-05), np.float64(1.0475222659486332e-05), np.float64(1.1639136288318145e-05), np.float64(1.1639136288318145e-05), np.float64(1.2932373653686828e-05), np.float64(1.2932373653686828e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 12:42:56,068] [INFO] [timer.py:264:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=13.817325639817886, CurrSamplesPerSec=15.166186378516722, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [2]  [ 40/483]  eta: 0:04:38  lr: 0.000013  min_lr: 0.000003  loss: 1.9855 (1.9809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1552 (2.2296)  time: 0.5480 (0.4623 -- 0.6084)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 12:43:09,042] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:43:09,043] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [2]  [ 60/483]  eta: 0:04:07  lr: 0.000013  min_lr: 0.000003  loss: 1.9078 (1.9699)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4234 (2.2562)  time: 0.5005 (0.4207 -- 0.5837)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [2]  [ 80/483]  eta: 0:03:47  lr: 0.000014  min_lr: 0.000003  loss: 1.9380 (1.9621)  loss_scale: 32768.0000 (21036.2469)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3074 (2.2874)  time: 0.4996 (0.4407 -- 0.5787)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [2]  [100/483]  eta: 0:03:32  lr: 0.000014  min_lr: 0.000004  loss: 1.9346 (1.9629)  loss_scale: 32768.0000 (23359.3663)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2231 (2.2912)  time: 0.5155 (0.4620 -- 0.5847)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [2]  [120/483]  eta: 0:03:19  lr: 0.000014  min_lr: 0.000004  loss: 1.9010 (1.9540)  loss_scale: 32768.0000 (24914.5124)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1533 (2.2992)  time: 0.5190 (0.3969 -- 0.6068)  data: 0.0004 (0.0001 -- 0.0027)  max mem: 21487
Epoch: [2]  [140/483]  eta: 0:03:05  lr: 0.000014  min_lr: 0.000004  loss: 1.8944 (1.9434)  loss_scale: 32768.0000 (26028.4823)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2587 (2.3115)  time: 0.4991 (0.4050 -- 0.5909)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [2]  [160/483]  eta: 0:02:54  lr: 0.000015  min_lr: 0.000004  loss: 1.9050 (1.9388)  loss_scale: 32768.0000 (26865.6894)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4526 (2.3373)  time: 0.5247 (0.3980 -- 0.6064)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [2]  [180/483]  eta: 0:02:42  lr: 0.000015  min_lr: 0.000004  loss: 1.9142 (1.9335)  loss_scale: 32768.0000 (27517.8785)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3953 (2.3457)  time: 0.5230 (0.4201 -- 0.6004)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 12:44:14,616] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:44:14,617] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [2]  [200/483]  eta: 0:02:31  lr: 0.000015  min_lr: 0.000004  loss: 1.8543 (1.9257)  loss_scale: 65536.0000 (30485.6517)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3007 (2.3463)  time: 0.5048 (0.3431 -- 0.5817)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [2]  [220/483]  eta: 0:02:20  lr: 0.000015  min_lr: 0.000004  loss: 1.8583 (1.9232)  loss_scale: 65536.0000 (33657.6290)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.5150 (2.3641)  time: 0.5208 (0.4365 -- 0.5700)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [2]  [240/483]  eta: 0:02:09  lr: 0.000016  min_lr: 0.000004  loss: 1.8516 (1.9163)  loss_scale: 65536.0000 (36303.1369)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2561 (2.3730)  time: 0.5154 (0.3503 -- 0.6037)  data: 0.0003 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [2]  [260/483]  eta: 0:01:58  lr: 0.000016  min_lr: 0.000004  loss: 1.7903 (1.9069)  loss_scale: 65536.0000 (38543.2031)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.6109 (2.3936)  time: 0.5063 (0.4451 -- 0.5462)  data: 0.0013 (0.0001 -- 0.0078)  max mem: 21487
Epoch: [2]  [280/483]  eta: 0:01:47  lr: 0.000016  min_lr: 0.000004  loss: 1.7849 (1.8997)  loss_scale: 65536.0000 (40464.3986)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2921 (2.4007)  time: 0.4941 (0.2739 -- 0.6100)  data: 0.0003 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [2]  [300/483]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000004  loss: 1.8329 (1.8958)  loss_scale: 65536.0000 (42130.2857)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.5153 (2.4335)  time: 0.5065 (0.2715 -- 0.5850)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 12:45:20,073] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:45:20,073] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [2]  [320/483]  eta: 0:01:25  lr: 0.000017  min_lr: 0.000004  loss: 1.8245 (1.8905)  loss_scale: 65536.0000 (45017.7196)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4528 (2.4356)  time: 0.5239 (0.4410 -- 0.5911)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [2]  [340/483]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000004  loss: 1.7343 (1.8814)  loss_scale: 131072.0000 (50064.8915)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.5737 (2.4501)  time: 0.5337 (0.4414 -- 0.6043)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 12:45:40,225] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 1318
[2025-06-27 12:45:40,226] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
[2025-06-27 12:45:40,226] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072, reducing to 65536.0
Epoch: [2]  [360/483]  eta: 0:01:04  lr: 0.000017  min_lr: 0.000004  loss: 1.7594 (1.8715)  loss_scale: 131072.0000 (52918.9584)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.5155 (2.4610)  time: 0.5262 (0.4460 -- 0.6094)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [2]  [380/483]  eta: 0:00:54  lr: 0.000017  min_lr: 0.000004  loss: 1.6769 (1.8635)  loss_scale: 65536.0000 (53581.2703)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4932 (2.4696)  time: 0.5092 (0.4394 -- 0.5791)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [2]  [400/483]  eta: 0:00:43  lr: 0.000018  min_lr: 0.000004  loss: 1.7470 (1.8573)  loss_scale: 65536.0000 (54177.5162)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3992 (2.4675)  time: 0.5062 (0.3972 -- 0.5768)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [2]  [420/483]  eta: 0:00:33  lr: 0.000018  min_lr: 0.000005  loss: 1.6938 (1.8512)  loss_scale: 65536.0000 (54717.1116)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3367 (2.4647)  time: 0.5321 (0.4574 -- 0.6017)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [2]  [440/483]  eta: 0:00:22  lr: 0.000018  min_lr: 0.000005  loss: 1.7050 (1.8448)  loss_scale: 65536.0000 (55207.7642)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.6266 (2.4743)  time: 0.5135 (0.3883 -- 0.6004)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [2]  [460/483]  eta: 0:00:12  lr: 0.000018  min_lr: 0.000005  loss: 1.6829 (1.8372)  loss_scale: 65536.0000 (55655.8438)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.6172 (2.4874)  time: 0.5160 (0.4590 -- 0.5757)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [2]  [480/483]  eta: 0:00:01  lr: 0.000019  min_lr: 0.000005  loss: 1.6924 (1.8311)  loss_scale: 65536.0000 (56066.6611)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4089 (2.4884)  time: 0.5329 (0.4467 -- 0.6079)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 12:46:47,126] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:46:47,126] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [482/483]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.7277 (1.8306)  loss_scale: 65536.0000 (56377.2422)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3436 (2.4850)  time: 0.5333 (0.4467 -- 0.6079)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [2] Total time: 0:04:13 (0.5246 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.7277 (1.8306)  loss_scale: 65536.0000 (56377.2422)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3436 (2.4850)
Val:  [ 0/23]  eta: 0:00:30  loss: 1.2587 (1.2587)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 1.3142 (1.3142 -- 1.3142)  data: 1.1215 (1.1215 -- 1.1215)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 1.2587 (1.2611)  acc1: 83.3333 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.2966 (0.1318 -- 1.3142)  data: 0.1021 (0.0001 -- 1.1215)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 1.2129 (1.2347)  acc1: 91.6667 (86.9048)  acc5: 100.0000 (100.0000)  time: 0.1980 (0.1318 -- 0.2104)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 1.2517 (1.2431)  acc1: 83.3333 (85.6089)  acc5: 100.0000 (100.0000)  time: 0.1937 (0.1150 -- 0.2104)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Val: Total time: 0:00:05 (0.2434 s / it)
* Acc@1 85.609 Acc@5 100.000 loss 1.243
Accuracy of the network on the 271 val images: 85.61%
[2025-06-27 12:46:53,337] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 12:46:53,346] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 85.61%
Epoch: [3]  [  0/483]  eta: 0:31:25  lr: 0.000019  min_lr: 0.000005  loss: 1.7033 (1.7033)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7490 (2.7490)  time: 3.9029 (3.9029 -- 3.9029)  data: 3.2937 (3.2937 -- 3.2937)  max mem: 21487
Epoch: [3]  [ 20/483]  eta: 0:05:16  lr: 0.000019  min_lr: 0.000005  loss: 1.6034 (1.6338)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4364 (3.0420)  time: 0.5229 (0.4185 -- 0.6016)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [3]  [ 40/483]  eta: 0:04:28  lr: 0.000019  min_lr: 0.000005  loss: 1.5388 (1.5990)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7642 (3.0396)  time: 0.5247 (0.4593 -- 0.5865)  data: 0.0009 (0.0001 -- 0.0135)  max mem: 21487
Epoch: [3]  [ 60/483]  eta: 0:04:05  lr: 0.000020  min_lr: 0.000005  loss: 1.6017 (1.6089)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.0005 (3.0167)  time: 0.5273 (0.4005 -- 0.6058)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [3]  [ 80/483]  eta: 0:03:47  lr: 0.000020  min_lr: 0.000005  loss: 1.5765 (1.6064)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.5365 (2.9554)  time: 0.5156 (0.4422 -- 0.6006)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [3]  [100/483]  eta: 0:03:30  lr: 0.000020  min_lr: 0.000005  loss: 1.6270 (1.6096)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.9269 (2.9577)  time: 0.4910 (0.2674 -- 0.6169)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [3]  [120/483]  eta: 0:03:17  lr: 0.000020  min_lr: 0.000005  loss: 1.5466 (1.5955)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7952 (2.9436)  time: 0.5196 (0.4368 -- 0.5908)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 12:48:07,769] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:48:07,769] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-06-27 12:48:11,852] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 1583
[2025-06-27 12:48:11,852] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-06-27 12:48:11,852] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [3]  [140/483]  eta: 0:03:05  lr: 0.000021  min_lr: 0.000005  loss: 1.4710 (1.5894)  loss_scale: 131072.0000 (138508.7092)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7064 (2.9376)  time: 0.5105 (0.3343 -- 0.5973)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 12:48:23,695] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 1606
[2025-06-27 12:48:23,696] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 12:48:23,696] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [160/483]  eta: 0:02:53  lr: 0.000021  min_lr: 0.000005  loss: 1.6286 (1.5910)  loss_scale: 131072.0000 (135956.6708)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.8377 (2.9267)  time: 0.5221 (0.4457 -- 0.5891)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [3]  [180/483]  eta: 0:02:42  lr: 0.000021  min_lr: 0.000005  loss: 1.5331 (1.5865)  loss_scale: 65536.0000 (128175.3812)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7554 (2.9440)  time: 0.5257 (0.4487 -- 0.5993)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [3]  [200/483]  eta: 0:02:31  lr: 0.000021  min_lr: 0.000005  loss: 1.4593 (1.5786)  loss_scale: 65536.0000 (121942.6070)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.3393 (3.0175)  time: 0.5188 (0.4048 -- 0.6021)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [3]  [220/483]  eta: 0:02:19  lr: 0.000022  min_lr: 0.000005  loss: 1.4375 (1.5684)  loss_scale: 65536.0000 (116837.9367)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.3139 (3.0692)  time: 0.5077 (0.4414 -- 0.5788)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [3]  [240/483]  eta: 0:02:09  lr: 0.000022  min_lr: 0.000006  loss: 1.4960 (1.5661)  loss_scale: 65536.0000 (112580.5145)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.6598 (3.1039)  time: 0.5163 (0.3927 -- 0.5917)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 12:49:11,244] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 1698
[2025-06-27 12:49:11,244] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 12:49:11,244] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [260/483]  eta: 0:01:58  lr: 0.000022  min_lr: 0.000006  loss: 1.4700 (1.5619)  loss_scale: 32768.0000 (107468.9962)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7801 (3.1268)  time: 0.5285 (0.4037 -- 0.5949)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [3]  [280/483]  eta: 0:01:47  lr: 0.000022  min_lr: 0.000006  loss: 1.5247 (1.5587)  loss_scale: 32768.0000 (102152.1993)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2585 (3.1528)  time: 0.5121 (0.4545 -- 0.6039)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [3]  [300/483]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000006  loss: 1.5291 (1.5574)  loss_scale: 32768.0000 (97541.9535)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2694 (3.1707)  time: 0.5050 (0.4300 -- 0.6002)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [3]  [320/483]  eta: 0:01:25  lr: 0.000023  min_lr: 0.000006  loss: 1.4406 (1.5547)  loss_scale: 32768.0000 (93506.1931)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.3891 (3.2675)  time: 0.4968 (0.4044 -- 0.5844)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [3]  [340/483]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000006  loss: 1.4730 (1.5482)  loss_scale: 32768.0000 (89943.8358)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.8221 (3.2678)  time: 0.5147 (0.4458 -- 0.5989)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [3]  [360/483]  eta: 0:01:04  lr: 0.000023  min_lr: 0.000006  loss: 1.5237 (1.5450)  loss_scale: 32768.0000 (86776.1994)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.9997 (3.2777)  time: 0.4977 (0.3885 -- 0.5849)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 12:50:16,727] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:50:16,727] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [380/483]  eta: 0:00:53  lr: 0.000024  min_lr: 0.000006  loss: 1.4118 (1.5401)  loss_scale: 32768.0000 (84199.1391)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2719 (3.2840)  time: 0.4947 (0.3982 -- 0.5974)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [3]  [400/483]  eta: 0:00:43  lr: 0.000024  min_lr: 0.000006  loss: 1.4281 (1.5332)  loss_scale: 65536.0000 (83268.3092)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.1846 (3.2893)  time: 0.5047 (0.4569 -- 0.5867)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [3]  [420/483]  eta: 0:00:32  lr: 0.000024  min_lr: 0.000006  loss: 1.3751 (1.5267)  loss_scale: 65536.0000 (82425.9192)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.0701 (3.3061)  time: 0.5281 (0.4430 -- 0.5993)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [3]  [440/483]  eta: 0:00:22  lr: 0.000024  min_lr: 0.000006  loss: 1.4208 (1.5233)  loss_scale: 65536.0000 (81659.9365)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.4306 (3.3088)  time: 0.5164 (0.4399 -- 0.5898)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [3]  [460/483]  eta: 0:00:11  lr: 0.000025  min_lr: 0.000006  loss: 1.4029 (1.5187)  loss_scale: 65536.0000 (80960.4165)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8497 (3.3370)  time: 0.5175 (0.4287 -- 0.5896)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [3]  [480/483]  eta: 0:00:01  lr: 0.000025  min_lr: 0.000006  loss: 1.2676 (1.5099)  loss_scale: 65536.0000 (80319.0686)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7872 (3.3508)  time: 0.5330 (0.4244 -- 0.6040)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [3]  [482/483]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.2577 (1.5093)  loss_scale: 65536.0000 (80257.8551)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7717 (3.3493)  time: 0.5379 (0.4244 -- 0.6040)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [3] Total time: 0:04:12 (0.5219 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.2577 (1.5093)  loss_scale: 65536.0000 (80257.8551)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7717 (3.3493)
Val:  [ 0/23]  eta: 0:00:31  loss: 0.7534 (0.7534)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 1.3560 (1.3560 -- 1.3560)  data: 1.1551 (1.1551 -- 1.1551)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.7534 (0.7847)  acc1: 83.3333 (87.1212)  acc5: 100.0000 (100.0000)  time: 0.3002 (0.1761 -- 1.3560)  data: 0.1052 (0.0002 -- 1.1551)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.6845 (0.7451)  acc1: 91.6667 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.1964 (0.1761 -- 0.2075)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.7512 (0.7575)  acc1: 85.7143 (87.8229)  acc5: 100.0000 (100.0000)  time: 0.1916 (0.1146 -- 0.2075)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:05 (0.2432 s / it)
* Acc@1 87.823 Acc@5 100.000 loss 0.757
Accuracy of the network on the 271 val images: 87.82%
[2025-06-27 12:51:16,453] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 12:51:16,457] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 87.82%
Epoch: [4]  [  0/483]  eta: 0:39:39  lr: 0.000025  min_lr: 0.000006  loss: 1.7873 (1.7873)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4795 (5.4795)  time: 4.9257 (4.9257 -- 4.9257)  data: 4.4172 (4.4172 -- 4.4172)  max mem: 21487
Epoch: [4]  [ 20/483]  eta: 0:05:46  lr: 0.000025  min_lr: 0.000006  loss: 1.2631 (1.3729)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9935 (4.0887)  time: 0.5398 (0.4036 -- 0.6083)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 12:51:38,891] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:51:38,891] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-06-27 12:51:39,965] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 1957
[2025-06-27 12:51:39,966] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 12:51:39,966] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 40/483]  eta: 0:04:48  lr: 0.000026  min_lr: 0.000006  loss: 1.3140 (1.3775)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.1349 (3.8178)  time: 0.5491 (0.3828 -- 0.6097)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [4]  [ 60/483]  eta: 0:04:20  lr: 0.000026  min_lr: 0.000007  loss: 1.3544 (1.3764)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2706 (4.0077)  time: 0.5464 (0.4669 -- 0.5904)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 12:52:03,097] [INFO] [logging.py:107:log_dist] [Rank 0] step=2000, skipped=5, lr=[np.float64(6.577762260291105e-06), np.float64(6.577762260291105e-06), np.float64(7.308624733656784e-06), np.float64(7.308624733656784e-06), np.float64(8.120694148507536e-06), np.float64(8.120694148507536e-06), np.float64(9.022993498341708e-06), np.float64(9.022993498341708e-06), np.float64(1.0025548331490786e-05), np.float64(1.0025548331490786e-05), np.float64(1.1139498146100873e-05), np.float64(1.1139498146100873e-05), np.float64(1.2377220162334303e-05), np.float64(1.2377220162334303e-05), np.float64(1.3752466847038114e-05), np.float64(1.3752466847038114e-05), np.float64(1.5280518718931238e-05), np.float64(1.5280518718931238e-05), np.float64(1.6978354132145818e-05), np.float64(1.6978354132145818e-05), np.float64(1.8864837924606465e-05), np.float64(1.8864837924606465e-05), np.float64(2.0960931027340515e-05), np.float64(2.0960931027340515e-05), np.float64(2.3289923363711684e-05), np.float64(2.3289923363711684e-05), np.float64(2.5877692626346315e-05), np.float64(2.5877692626346315e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 12:52:03,099] [INFO] [timer.py:264:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=14.727739244039862, CurrSamplesPerSec=14.842764484157712, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [4]  [ 80/483]  eta: 0:04:00  lr: 0.000026  min_lr: 0.000007  loss: 1.4428 (1.3831)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7600 (3.9448)  time: 0.5369 (0.4495 -- 0.6008)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [4]  [100/483]  eta: 0:03:45  lr: 0.000026  min_lr: 0.000007  loss: 1.4988 (1.4072)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2636 (3.8032)  time: 0.5564 (0.5049 -- 0.6037)  data: 0.0003 (0.0001 -- 0.0010)  max mem: 21487
[2025-06-27 12:52:29,293] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 2047
[2025-06-27 12:52:29,294] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 12:52:29,294] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [120/483]  eta: 0:03:31  lr: 0.000027  min_lr: 0.000007  loss: 1.3835 (1.4043)  loss_scale: 65536.0000 (64994.3802)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6450 (3.7961)  time: 0.5433 (0.4472 -- 0.6103)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [4]  [140/483]  eta: 0:03:16  lr: 0.000027  min_lr: 0.000007  loss: 1.3898 (1.3965)  loss_scale: 32768.0000 (60423.2624)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6723 (3.8200)  time: 0.5290 (0.4469 -- 0.6004)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [4]  [160/483]  eta: 0:03:04  lr: 0.000027  min_lr: 0.000007  loss: 1.4173 (1.4000)  loss_scale: 32768.0000 (56987.8261)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.4927 (4.0297)  time: 0.5437 (0.4461 -- 0.6072)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [4]  [180/483]  eta: 0:02:52  lr: 0.000027  min_lr: 0.000007  loss: 1.2963 (1.3940)  loss_scale: 32768.0000 (54311.6022)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0381 (4.0496)  time: 0.5753 (0.4666 -- 0.6464)  data: 0.0002 (0.0001 -- 0.0012)  max mem: 21487
Epoch: [4]  [200/483]  eta: 0:02:40  lr: 0.000028  min_lr: 0.000007  loss: 1.2781 (1.3899)  loss_scale: 32768.0000 (52167.9602)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6340 (4.0804)  time: 0.5448 (0.4599 -- 0.6136)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [4]  [220/483]  eta: 0:02:28  lr: 0.000028  min_lr: 0.000007  loss: 1.3292 (1.3898)  loss_scale: 32768.0000 (50412.3077)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7945 (4.0828)  time: 0.5194 (0.3958 -- 0.6105)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [4]  [240/483]  eta: 0:02:17  lr: 0.000028  min_lr: 0.000007  loss: 1.3529 (1.3901)  loss_scale: 32768.0000 (48948.0498)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5189 (4.1674)  time: 0.5635 (0.4466 -- 0.6434)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 12:53:39,863] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:53:39,864] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-06-27 12:53:46,621] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 2189
[2025-06-27 12:53:46,621] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 12:53:46,621] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [260/483]  eta: 0:02:05  lr: 0.000028  min_lr: 0.000007  loss: 1.4800 (1.3931)  loss_scale: 65536.0000 (49340.3218)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6548 (4.2271)  time: 0.5378 (0.4122 -- 0.6082)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [4]  [280/483]  eta: 0:01:53  lr: 0.000029  min_lr: 0.000007  loss: 1.2878 (1.3865)  loss_scale: 32768.0000 (48160.7972)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0603 (4.2469)  time: 0.5570 (0.4395 -- 0.6101)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [4]  [300/483]  eta: 0:01:42  lr: 0.000029  min_lr: 0.000007  loss: 1.3908 (1.3872)  loss_scale: 32768.0000 (47138.0199)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9630 (4.2771)  time: 0.5734 (0.5043 -- 0.6134)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [4]  [320/483]  eta: 0:01:31  lr: 0.000029  min_lr: 0.000007  loss: 1.2745 (1.3837)  loss_scale: 32768.0000 (46242.6916)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7733 (4.2591)  time: 0.5756 (0.4507 -- 0.6288)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [4]  [340/483]  eta: 0:01:20  lr: 0.000029  min_lr: 0.000007  loss: 1.3266 (1.3781)  loss_scale: 32768.0000 (45452.3871)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2611 (4.2145)  time: 0.5562 (0.4854 -- 0.6056)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [4]  [360/483]  eta: 0:01:09  lr: 0.000030  min_lr: 0.000008  loss: 1.2731 (1.3765)  loss_scale: 32768.0000 (44749.6510)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2564 (4.2441)  time: 0.5378 (0.3669 -- 0.6095)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 12:54:46,009] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 2295
[2025-06-27 12:54:46,009] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 12:54:46,009] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [4]  [380/483]  eta: 0:00:57  lr: 0.000030  min_lr: 0.000008  loss: 1.2425 (1.3700)  loss_scale: 16384.0000 (43346.6457)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.3882 (4.2347)  time: 0.5648 (0.4600 -- 0.6104)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [4]  [400/483]  eta: 0:00:46  lr: 0.000030  min_lr: 0.000008  loss: 1.2723 (1.3654)  loss_scale: 16384.0000 (42001.8753)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2915 (4.1970)  time: 0.5636 (0.4960 -- 0.6138)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [4]  [420/483]  eta: 0:00:35  lr: 0.000030  min_lr: 0.000008  loss: 1.3206 (1.3654)  loss_scale: 16384.0000 (40784.8741)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6334 (4.2432)  time: 0.5407 (0.4600 -- 0.6070)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [4]  [440/483]  eta: 0:00:24  lr: 0.000031  min_lr: 0.000008  loss: 1.3458 (1.3631)  loss_scale: 16384.0000 (39678.2585)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2182 (4.2787)  time: 0.5430 (0.3916 -- 0.6948)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [4]  [460/483]  eta: 0:00:12  lr: 0.000031  min_lr: 0.000008  loss: 1.2715 (1.3595)  loss_scale: 16384.0000 (38667.6616)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6607 (4.2900)  time: 0.5283 (0.4279 -- 0.6049)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [4]  [480/483]  eta: 0:00:01  lr: 0.000031  min_lr: 0.000008  loss: 1.3280 (1.3596)  loss_scale: 16384.0000 (37741.1060)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9819 (4.3034)  time: 0.5687 (0.4596 -- 0.6058)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [4]  [482/483]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.3280 (1.3597)  loss_scale: 16384.0000 (37652.6708)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7015 (4.3012)  time: 0.5606 (0.4570 -- 0.6058)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [4] Total time: 0:04:29 (0.5587 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.3280 (1.3597)  loss_scale: 16384.0000 (37652.6708)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7015 (4.3012)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.5210 (0.5210)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.3112 (1.3112 -- 1.3112)  data: 1.1035 (1.1035 -- 1.1035)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.5210 (0.5256)  acc1: 91.6667 (89.3939)  acc5: 100.0000 (100.0000)  time: 0.3011 (0.1918 -- 1.3112)  data: 0.1005 (0.0002 -- 1.1035)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.4583 (0.4905)  acc1: 91.6667 (90.8730)  acc5: 100.0000 (100.0000)  time: 0.1970 (0.1549 -- 0.2069)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.4781 (0.5037)  acc1: 91.6667 (90.0369)  acc5: 100.0000 (100.0000)  time: 0.1931 (0.1172 -- 0.2069)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:05 (0.2422 s / it)
* Acc@1 90.037 Acc@5 100.000 loss 0.504
Accuracy of the network on the 271 val images: 90.04%
[2025-06-27 12:55:57,102] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 12:55:57,111] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 90.04%
Epoch: [5]  [  0/483]  eta: 0:42:23  lr: 0.000031  min_lr: 0.000008  loss: 1.2727 (1.2727)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3346 (5.3346)  time: 5.2650 (5.2650 -- 5.2650)  data: 4.6610 (4.6610 -- 4.6610)  max mem: 21487
[2025-06-27 12:56:11,967] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:56:11,967] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [ 20/483]  eta: 0:06:01  lr: 0.000031  min_lr: 0.000008  loss: 1.3015 (1.2867)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.4563 (3.8423)  time: 0.5564 (0.4949 -- 0.6019)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [5]  [ 40/483]  eta: 0:04:58  lr: 0.000031  min_lr: 0.000008  loss: 1.4105 (1.3263)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1681 (4.0532)  time: 0.5628 (0.4562 -- 0.6075)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [5]  [ 60/483]  eta: 0:04:28  lr: 0.000031  min_lr: 0.000008  loss: 1.2658 (1.3050)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3635 (4.2221)  time: 0.5561 (0.4928 -- 0.6073)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [ 80/483]  eta: 0:04:11  lr: 0.000031  min_lr: 0.000008  loss: 1.3055 (1.3114)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1162 (4.3679)  time: 0.5890 (0.5058 -- 0.6288)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [5]  [100/483]  eta: 0:03:54  lr: 0.000031  min_lr: 0.000008  loss: 1.3365 (1.3102)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.7461 (4.2006)  time: 0.5608 (0.4478 -- 0.6101)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [120/483]  eta: 0:03:38  lr: 0.000031  min_lr: 0.000008  loss: 1.3133 (1.3201)  loss_scale: 32768.0000 (31549.3554)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1567 (4.3598)  time: 0.5577 (0.4862 -- 0.6062)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 12:57:24,104] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:57:24,104] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [140/483]  eta: 0:03:24  lr: 0.000031  min_lr: 0.000008  loss: 1.3198 (1.3211)  loss_scale: 32768.0000 (32651.8014)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.3624 (4.3042)  time: 0.5660 (0.4591 -- 0.6189)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 12:57:27,402] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 2558
[2025-06-27 12:57:27,402] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 12:57:27,402] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [160/483]  eta: 0:03:11  lr: 0.000031  min_lr: 0.000008  loss: 1.3792 (1.3270)  loss_scale: 32768.0000 (33073.2919)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0683 (4.4450)  time: 0.5543 (0.4568 -- 0.6058)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [180/483]  eta: 0:02:57  lr: 0.000031  min_lr: 0.000008  loss: 1.3230 (1.3268)  loss_scale: 32768.0000 (33039.5580)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0029 (4.5392)  time: 0.5269 (0.3453 -- 0.6083)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [5]  [200/483]  eta: 0:02:44  lr: 0.000031  min_lr: 0.000008  loss: 1.2631 (1.3228)  loss_scale: 32768.0000 (33012.5373)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6199 (4.6039)  time: 0.5538 (0.4598 -- 0.6076)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [220/483]  eta: 0:02:32  lr: 0.000031  min_lr: 0.000008  loss: 1.3162 (1.3222)  loss_scale: 32768.0000 (32990.4072)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5883 (4.6213)  time: 0.5515 (0.4919 -- 0.6060)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [5]  [240/483]  eta: 0:02:20  lr: 0.000031  min_lr: 0.000008  loss: 1.3491 (1.3232)  loss_scale: 32768.0000 (32971.9502)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6388 (4.6680)  time: 0.5526 (0.4637 -- 0.6090)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [5]  [260/483]  eta: 0:02:07  lr: 0.000031  min_lr: 0.000008  loss: 1.3545 (1.3255)  loss_scale: 32768.0000 (32956.3218)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1429 (4.6470)  time: 0.5280 (0.4390 -- 0.6061)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 12:58:37,593] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 12:58:37,594] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [280/483]  eta: 0:01:55  lr: 0.000031  min_lr: 0.000008  loss: 1.2762 (1.3240)  loss_scale: 32768.0000 (33992.4270)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8840 (4.7038)  time: 0.5323 (0.4559 -- 0.5994)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [5]  [300/483]  eta: 0:01:44  lr: 0.000031  min_lr: 0.000008  loss: 1.4042 (1.3317)  loss_scale: 65536.0000 (36088.3455)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4274 (4.7036)  time: 0.5436 (0.4601 -- 0.6069)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [320/483]  eta: 0:01:32  lr: 0.000031  min_lr: 0.000008  loss: 1.2291 (1.3306)  loss_scale: 65536.0000 (37923.0903)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5614 (4.7257)  time: 0.5441 (0.4151 -- 0.6069)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [5]  [340/483]  eta: 0:01:21  lr: 0.000031  min_lr: 0.000008  loss: 1.2547 (1.3232)  loss_scale: 65536.0000 (39542.6158)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2018 (4.7182)  time: 0.5761 (0.4949 -- 0.6334)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [5]  [360/483]  eta: 0:01:09  lr: 0.000031  min_lr: 0.000008  loss: 1.2887 (1.3232)  loss_scale: 65536.0000 (40982.6925)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8844 (4.7363)  time: 0.5281 (0.4089 -- 0.6053)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 12:59:27,204] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 2778
[2025-06-27 12:59:27,204] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 12:59:27,205] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [380/483]  eta: 0:00:57  lr: 0.000031  min_lr: 0.000008  loss: 1.1426 (1.3141)  loss_scale: 32768.0000 (40723.4856)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6773 (4.7167)  time: 0.5100 (0.4468 -- 0.5712)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [400/483]  eta: 0:00:46  lr: 0.000031  min_lr: 0.000008  loss: 1.2594 (1.3108)  loss_scale: 32768.0000 (40326.7032)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.5144 (4.6734)  time: 0.5271 (0.4487 -- 0.6096)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [420/483]  eta: 0:00:35  lr: 0.000031  min_lr: 0.000008  loss: 1.1598 (1.3069)  loss_scale: 32768.0000 (39967.6200)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3747 (4.6721)  time: 0.5762 (0.4780 -- 0.7656)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [5]  [440/483]  eta: 0:00:24  lr: 0.000031  min_lr: 0.000008  loss: 1.1541 (1.3022)  loss_scale: 32768.0000 (39641.1066)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2883 (4.6626)  time: 0.8105 (0.5603 -- 1.0029)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [460/483]  eta: 0:00:13  lr: 0.000031  min_lr: 0.000008  loss: 1.1844 (1.2971)  loss_scale: 32768.0000 (39342.9241)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6952 (4.6777)  time: 0.7905 (0.6751 -- 1.0201)  data: 0.0002 (0.0001 -- 0.0020)  max mem: 21487
Epoch: [5]  [480/483]  eta: 0:00:01  lr: 0.000031  min_lr: 0.000008  loss: 1.2896 (1.2941)  loss_scale: 32768.0000 (39069.5385)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4774 (4.7079)  time: 0.8592 (0.5990 -- 1.0185)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5]  [482/483]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.2925 (1.2954)  loss_scale: 32768.0000 (39043.4451)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4774 (4.7076)  time: 0.8464 (0.5990 -- 1.0185)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [5] Total time: 0:04:47 (0.5943 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.2925 (1.2954)  loss_scale: 32768.0000 (39043.4451)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4774 (4.7076)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.3824 (0.3824)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.3338 (1.3338 -- 1.3338)  data: 1.0623 (1.0623 -- 1.0623)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.3824 (0.4172)  acc1: 100.0000 (95.4545)  acc5: 100.0000 (99.2424)  time: 0.3536 (0.2266 -- 1.3338)  data: 0.0968 (0.0002 -- 1.0623)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.3393 (0.3860)  acc1: 100.0000 (96.0317)  acc5: 100.0000 (99.6032)  time: 0.2587 (0.2266 -- 0.2720)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.3510 (0.3940)  acc1: 100.0000 (95.9410)  acc5: 100.0000 (99.6310)  time: 0.2538 (0.1681 -- 0.2720)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:06 (0.3016 s / it)
* Acc@1 95.941 Acc@5 99.631 loss 0.394
Accuracy of the network on the 271 val images: 95.94%
[2025-06-27 13:00:55,657] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 13:00:55,669] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 95.94%
Epoch: [6]  [  0/483]  eta: 0:42:04  lr: 0.000031  min_lr: 0.000008  loss: 1.7124 (1.7124)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0957 (5.0957)  time: 5.2269 (5.2269 -- 5.2269)  data: 4.5116 (4.5116 -- 4.5116)  max mem: 21487
[2025-06-27 13:01:12,625] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:01:12,625] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 20/483]  eta: 0:07:52  lr: 0.000031  min_lr: 0.000008  loss: 1.2225 (1.2461)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3051 (4.6974)  time: 0.8110 (0.6084 -- 0.9985)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:01:37,561] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 2937
[2025-06-27 13:01:37,561] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:01:37,561] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 40/483]  eta: 0:06:54  lr: 0.000031  min_lr: 0.000008  loss: 1.2778 (1.2947)  loss_scale: 65536.0000 (56744.5854)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6775 (4.9469)  time: 0.8473 (0.7151 -- 0.9998)  data: 0.0002 (0.0001 -- 0.0012)  max mem: 21487
Epoch: [6]  [ 60/483]  eta: 0:06:22  lr: 0.000031  min_lr: 0.000008  loss: 1.2625 (1.2879)  loss_scale: 32768.0000 (48883.4098)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7507 (5.2439)  time: 0.8386 (0.6648 -- 1.0187)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [6]  [ 80/483]  eta: 0:05:55  lr: 0.000031  min_lr: 0.000008  loss: 1.2478 (1.2838)  loss_scale: 32768.0000 (44904.2963)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7335 (4.9695)  time: 0.8152 (0.6318 -- 1.0456)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [6]  [100/483]  eta: 0:05:34  lr: 0.000031  min_lr: 0.000008  loss: 1.2670 (1.2804)  loss_scale: 32768.0000 (42501.0693)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8788 (4.9455)  time: 0.8328 (0.6073 -- 1.0007)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:02:28,938] [INFO] [logging.py:107:log_dist] [Rank 0] step=3000, skipped=11, lr=[np.float64(7.911568079276141e-06), np.float64(7.911568079276141e-06), np.float64(8.790631199195712e-06), np.float64(8.790631199195712e-06), np.float64(9.767367999106344e-06), np.float64(9.767367999106344e-06), np.float64(1.0852631110118162e-05), np.float64(1.0852631110118162e-05), np.float64(1.2058479011242401e-05), np.float64(1.2058479011242401e-05), np.float64(1.3398310012491557e-05), np.float64(1.3398310012491557e-05), np.float64(1.4887011124990618e-05), np.float64(1.4887011124990618e-05), np.float64(1.6541123472211798e-05), np.float64(1.6541123472211798e-05), np.float64(1.837902608023533e-05), np.float64(1.837902608023533e-05), np.float64(2.0421140089150366e-05), np.float64(2.0421140089150366e-05), np.float64(2.269015565461152e-05), np.float64(2.269015565461152e-05), np.float64(2.5211284060679465e-05), np.float64(2.5211284060679465e-05), np.float64(2.8012537845199403e-05), np.float64(2.8012537845199403e-05), np.float64(3.112504205022156e-05), np.float64(3.112504205022156e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 13:02:28,940] [INFO] [timer.py:264:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=14.3563318187283, CurrSamplesPerSec=12.412235105250762, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [6]  [120/483]  eta: 0:05:12  lr: 0.000031  min_lr: 0.000008  loss: 1.1664 (1.2752)  loss_scale: 32768.0000 (40892.2975)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5951 (5.0146)  time: 0.8102 (0.6287 -- 1.0473)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [6]  [140/483]  eta: 0:04:51  lr: 0.000031  min_lr: 0.000008  loss: 1.2176 (1.2714)  loss_scale: 32768.0000 (39739.9149)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.9560 (4.9065)  time: 0.7828 (0.5509 -- 1.0084)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [6]  [160/483]  eta: 0:04:33  lr: 0.000031  min_lr: 0.000008  loss: 1.2700 (1.2730)  loss_scale: 32768.0000 (38873.8385)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9971 (4.8915)  time: 0.8228 (0.6116 -- 1.0426)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 13:03:23,570] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:03:23,571] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [180/483]  eta: 0:04:16  lr: 0.000031  min_lr: 0.000008  loss: 1.2345 (1.2716)  loss_scale: 65536.0000 (40552.6630)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3590 (4.8776)  time: 0.8489 (0.4351 -- 1.0033)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 13:03:45,052] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 3093
[2025-06-27 13:03:45,052] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:03:45,052] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [200/483]  eta: 0:03:57  lr: 0.000031  min_lr: 0.000008  loss: 1.1606 (1.2698)  loss_scale: 65536.0000 (42060.4179)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8382 (4.8832)  time: 0.7700 (0.4407 -- 1.0312)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [6]  [220/483]  eta: 0:03:40  lr: 0.000031  min_lr: 0.000008  loss: 1.2921 (1.2705)  loss_scale: 32768.0000 (41219.4751)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8807 (4.8996)  time: 0.8289 (0.6234 -- 1.0353)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [6]  [240/483]  eta: 0:03:22  lr: 0.000031  min_lr: 0.000008  loss: 1.2650 (1.2672)  loss_scale: 32768.0000 (40518.1079)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3439 (4.9281)  time: 0.7949 (0.5997 -- 0.9919)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [6]  [260/483]  eta: 0:03:05  lr: 0.000031  min_lr: 0.000008  loss: 1.2037 (1.2632)  loss_scale: 32768.0000 (39924.2299)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4360 (4.9108)  time: 0.7967 (0.6490 -- 1.0310)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [6]  [280/483]  eta: 0:02:49  lr: 0.000031  min_lr: 0.000008  loss: 1.1699 (1.2610)  loss_scale: 32768.0000 (39414.8897)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4118 (4.9256)  time: 0.8458 (0.6481 -- 1.0201)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [6]  [300/483]  eta: 0:02:32  lr: 0.000031  min_lr: 0.000008  loss: 1.3005 (1.2664)  loss_scale: 32768.0000 (38973.2359)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4591 (4.9258)  time: 0.8077 (0.5801 -- 1.0337)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [6]  [320/483]  eta: 0:02:14  lr: 0.000031  min_lr: 0.000008  loss: 1.2909 (1.2702)  loss_scale: 32768.0000 (38586.6168)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6476 (5.0293)  time: 0.6918 (0.4926 -- 1.0264)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 13:05:27,953] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:05:27,953] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [340/483]  eta: 0:01:57  lr: 0.000031  min_lr: 0.000008  loss: 1.3105 (1.2712)  loss_scale: 65536.0000 (39878.9443)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9664 (5.0065)  time: 0.8274 (0.6453 -- 1.0088)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:05:52,303] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 3252
[2025-06-27 13:05:52,303] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:05:52,303] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [360/483]  eta: 0:01:41  lr: 0.000031  min_lr: 0.000008  loss: 1.3332 (1.2726)  loss_scale: 65536.0000 (40664.9972)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6629 (4.9667)  time: 0.8203 (0.6010 -- 1.0203)  data: 0.0003 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [6]  [380/483]  eta: 0:01:24  lr: 0.000031  min_lr: 0.000008  loss: 1.1137 (1.2689)  loss_scale: 32768.0000 (40250.4567)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5028 (4.9770)  time: 0.8449 (0.6627 -- 1.0081)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [6]  [400/483]  eta: 0:01:08  lr: 0.000031  min_lr: 0.000008  loss: 1.1217 (1.2652)  loss_scale: 32768.0000 (39877.2668)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7286 (4.9948)  time: 0.8622 (0.6056 -- 1.0298)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [6]  [420/483]  eta: 0:00:52  lr: 0.000031  min_lr: 0.000008  loss: 1.2129 (1.2650)  loss_scale: 32768.0000 (39539.5344)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5633 (5.0022)  time: 0.8524 (0.7060 -- 1.0432)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [6]  [440/483]  eta: 0:00:35  lr: 0.000031  min_lr: 0.000008  loss: 1.1597 (1.2610)  loss_scale: 32768.0000 (39232.4354)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1461 (5.0198)  time: 0.8034 (0.5928 -- 1.0393)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [6]  [460/483]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.2204 (1.2592)  loss_scale: 32768.0000 (38951.9826)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0866 (5.0127)  time: 0.8219 (0.6798 -- 0.9858)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [6]  [480/483]  eta: 0:00:02  lr: 0.000031  min_lr: 0.000008  loss: 1.2860 (1.2606)  loss_scale: 32768.0000 (38694.8524)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9488 (5.0433)  time: 0.8331 (0.6567 -- 1.0012)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [6]  [482/483]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.2860 (1.2606)  loss_scale: 32768.0000 (38670.3106)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4033 (5.0606)  time: 0.8310 (0.6567 -- 1.0012)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [6] Total time: 0:06:39 (0.8264 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.2860 (1.2606)  loss_scale: 32768.0000 (38670.3106)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4033 (5.0606)
Val:  [ 0/23]  eta: 0:00:32  loss: 0.3346 (0.3346)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.4162 (1.4162 -- 1.4162)  data: 1.1432 (1.1432 -- 1.1432)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.3102 (0.3465)  acc1: 100.0000 (95.4545)  acc5: 100.0000 (99.2424)  time: 0.3677 (0.2498 -- 1.4162)  data: 0.1041 (0.0002 -- 1.1432)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.2775 (0.3161)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.2592 (0.2270 -- 0.2778)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.2864 (0.3231)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.2560 (0.1785 -- 0.2778)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:07 (0.3067 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.323
Accuracy of the network on the 271 val images: 96.31%
[2025-06-27 13:07:46,418] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 13:07:46,427] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 96.31%
[2025-06-27 13:07:53,858] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:07:53,858] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [  0/483]  eta: 0:27:06  lr: 0.000031  min_lr: 0.000008  loss: 0.9344 (0.9344)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3476 (5.3476)  time: 3.3670 (3.3670 -- 3.3670)  data: 2.6400 (2.6400 -- 2.6400)  max mem: 21487
[2025-06-27 13:08:02,725] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 3392
[2025-06-27 13:08:02,725] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:08:02,725] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 20/483]  eta: 0:07:10  lr: 0.000031  min_lr: 0.000008  loss: 1.1427 (1.2117)  loss_scale: 32768.0000 (49932.1905)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6406 (5.0057)  time: 0.8088 (0.5831 -- 1.0062)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [7]  [ 40/483]  eta: 0:06:28  lr: 0.000031  min_lr: 0.000008  loss: 1.3206 (1.2549)  loss_scale: 32768.0000 (41559.4146)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3429 (5.4859)  time: 0.8210 (0.6568 -- 0.9568)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [7]  [ 60/483]  eta: 0:06:01  lr: 0.000031  min_lr: 0.000008  loss: 1.2139 (1.2416)  loss_scale: 32768.0000 (38676.9836)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4962 (5.3807)  time: 0.8090 (0.6299 -- 1.0640)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [7]  [ 80/483]  eta: 0:05:39  lr: 0.000031  min_lr: 0.000008  loss: 1.1849 (1.2423)  loss_scale: 32768.0000 (37217.9753)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9276 (5.3773)  time: 0.8026 (0.6539 -- 1.0130)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [7]  [100/483]  eta: 0:05:19  lr: 0.000031  min_lr: 0.000008  loss: 1.1881 (1.2384)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9295 (5.1338)  time: 0.7982 (0.6432 -- 1.0460)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [7]  [120/483]  eta: 0:05:00  lr: 0.000031  min_lr: 0.000008  loss: 1.1248 (1.2259)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0772 (5.1721)  time: 0.7969 (0.6670 -- 0.9889)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:09:47,311] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:09:47,311] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [140/483]  eta: 0:04:44  lr: 0.000031  min_lr: 0.000008  loss: 1.2396 (1.2249)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9416 (5.3770)  time: 0.8354 (0.6345 -- 0.9885)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [7]  [160/483]  eta: 0:04:27  lr: 0.000031  min_lr: 0.000008  loss: 1.2614 (1.2274)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4612 (5.2877)  time: 0.8309 (0.6802 -- 1.0073)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 13:10:20,349] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 3561
[2025-06-27 13:10:20,349] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:10:20,349] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [180/483]  eta: 0:04:10  lr: 0.000031  min_lr: 0.000008  loss: 1.2820 (1.2360)  loss_scale: 65536.0000 (42000.9724)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3280 (5.3196)  time: 0.8205 (0.6103 -- 1.0504)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [7]  [200/483]  eta: 0:03:54  lr: 0.000031  min_lr: 0.000008  loss: 1.1785 (1.2395)  loss_scale: 32768.0000 (41082.2687)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8099 (5.3002)  time: 0.8270 (0.6478 -- 1.0243)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [7]  [220/483]  eta: 0:03:37  lr: 0.000031  min_lr: 0.000008  loss: 1.2088 (1.2380)  loss_scale: 32768.0000 (40329.8462)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1468 (5.2745)  time: 0.8061 (0.6789 -- 0.9889)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [7]  [240/483]  eta: 0:03:20  lr: 0.000031  min_lr: 0.000008  loss: 1.2860 (1.2446)  loss_scale: 32768.0000 (39702.3071)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1909 (5.2948)  time: 0.8122 (0.5978 -- 1.0008)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [7]  [260/483]  eta: 0:03:04  lr: 0.000031  min_lr: 0.000008  loss: 1.2018 (1.2429)  loss_scale: 32768.0000 (39170.9425)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0541 (5.2207)  time: 0.8458 (0.6251 -- 1.0648)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [7]  [280/483]  eta: 0:02:48  lr: 0.000031  min_lr: 0.000008  loss: 1.2258 (1.2443)  loss_scale: 32768.0000 (38715.2171)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7577 (5.2141)  time: 0.8560 (0.6399 -- 1.0294)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [7]  [300/483]  eta: 0:02:31  lr: 0.000031  min_lr: 0.000008  loss: 1.3641 (1.2517)  loss_scale: 32768.0000 (38320.0532)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8658 (5.2184)  time: 0.8017 (0.6025 -- 1.0079)  data: 0.0001 (0.0001 -- 0.0010)  max mem: 21487
[2025-06-27 13:12:06,100] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:12:06,101] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [320/483]  eta: 0:02:14  lr: 0.000031  min_lr: 0.000008  loss: 1.1883 (1.2530)  loss_scale: 65536.0000 (39199.1028)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8028 (5.2456)  time: 0.7976 (0.6520 -- 1.0177)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [7]  [340/483]  eta: 0:01:57  lr: 0.000031  min_lr: 0.000008  loss: 1.2569 (1.2537)  loss_scale: 65536.0000 (40743.7889)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3302 (5.2881)  time: 0.7709 (0.4044 -- 1.0408)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [7]  [360/483]  eta: 0:01:40  lr: 0.000031  min_lr: 0.000008  loss: 1.3136 (1.2585)  loss_scale: 65536.0000 (42117.3186)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4129 (5.3204)  time: 0.8080 (0.6164 -- 0.9892)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 13:12:48,308] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 3743
[2025-06-27 13:12:48,308] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:12:48,308] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [380/483]  eta: 0:01:24  lr: 0.000031  min_lr: 0.000008  loss: 1.2386 (1.2562)  loss_scale: 32768.0000 (41712.5459)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7351 (5.3390)  time: 0.8056 (0.6111 -- 0.9805)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [7]  [400/483]  eta: 0:01:08  lr: 0.000031  min_lr: 0.000008  loss: 1.2107 (1.2525)  loss_scale: 32768.0000 (41266.4339)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2703 (5.3146)  time: 0.8153 (0.6021 -- 1.0169)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [7]  [420/483]  eta: 0:00:51  lr: 0.000031  min_lr: 0.000008  loss: 1.1801 (1.2533)  loss_scale: 32768.0000 (40862.7078)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1578 (5.2918)  time: 0.8245 (0.6611 -- 1.0554)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [7]  [440/483]  eta: 0:00:35  lr: 0.000031  min_lr: 0.000008  loss: 1.1519 (1.2497)  loss_scale: 32768.0000 (40495.6009)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9705 (5.2640)  time: 0.8331 (0.5568 -- 0.9879)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [7]  [460/483]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.2110 (1.2471)  loss_scale: 32768.0000 (40160.3471)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8856 (5.2641)  time: 0.8317 (0.6360 -- 1.0398)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [7]  [480/483]  eta: 0:00:02  lr: 0.000030  min_lr: 0.000008  loss: 1.2254 (1.2454)  loss_scale: 32768.0000 (39852.9730)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2128 (5.2647)  time: 0.8148 (0.6661 -- 1.0196)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [7]  [482/483]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.2614 (1.2461)  loss_scale: 32768.0000 (39823.6356)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2128 (5.2656)  time: 0.8098 (0.6595 -- 1.0196)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [7] Total time: 0:06:36 (0.8208 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.2614 (1.2461)  loss_scale: 32768.0000 (39823.6356)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2128 (5.2656)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.2570 (0.2570)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.3433 (1.3433 -- 1.3433)  data: 1.0727 (1.0727 -- 1.0727)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.2574 (0.3027)  acc1: 91.6667 (94.6970)  acc5: 100.0000 (99.2424)  time: 0.3635 (0.2446 -- 1.3433)  data: 0.0977 (0.0002 -- 1.0727)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.2427 (0.2684)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.2631 (0.2429 -- 0.2840)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.2502 (0.2757)  acc1: 100.0000 (96.6790)  acc5: 100.0000 (99.6310)  time: 0.2557 (0.1127 -- 0.2840)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val: Total time: 0:00:06 (0.3037 s / it)
* Acc@1 96.679 Acc@5 99.631 loss 0.276
Accuracy of the network on the 271 val images: 96.68%
[2025-06-27 13:14:33,967] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 13:14:33,970] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 96.68%
Epoch: [8]  [  0/483]  eta: 0:27:06  lr: 0.000030  min_lr: 0.000008  loss: 1.3107 (1.3107)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5223 (6.5223)  time: 3.3665 (3.3665 -- 3.3665)  data: 2.7766 (2.7766 -- 2.7766)  max mem: 21487
[2025-06-27 13:14:46,893] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:14:46,894] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 20/483]  eta: 0:06:46  lr: 0.000030  min_lr: 0.000008  loss: 1.0894 (1.1820)  loss_scale: 65536.0000 (53052.9524)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6265 (5.5403)  time: 0.7531 (0.4273 -- 1.0096)  data: 0.0046 (0.0001 -- 0.0889)  max mem: 21487
Epoch: [8]  [ 40/483]  eta: 0:06:08  lr: 0.000030  min_lr: 0.000008  loss: 1.2092 (1.1905)  loss_scale: 65536.0000 (59142.2439)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3389 (5.5653)  time: 0.7840 (0.5940 -- 1.0152)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 13:15:22,036] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 3916
[2025-06-27 13:15:22,036] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:15:22,036] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 60/483]  eta: 0:05:49  lr: 0.000030  min_lr: 0.000008  loss: 1.1915 (1.2018)  loss_scale: 65536.0000 (56403.9344)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7523 (5.7375)  time: 0.8111 (0.5493 -- 1.0270)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [8]  [ 80/483]  eta: 0:05:31  lr: 0.000030  min_lr: 0.000008  loss: 1.0637 (1.1904)  loss_scale: 32768.0000 (50567.9012)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3129 (5.5444)  time: 0.8156 (0.6126 -- 1.0049)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [8]  [100/483]  eta: 0:05:17  lr: 0.000030  min_lr: 0.000008  loss: 1.2453 (1.1994)  loss_scale: 32768.0000 (47043.1683)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6711 (5.5313)  time: 0.8572 (0.7197 -- 1.0251)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [8]  [120/483]  eta: 0:04:58  lr: 0.000030  min_lr: 0.000008  loss: 1.2570 (1.2093)  loss_scale: 32768.0000 (44683.6364)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1279 (5.5445)  time: 0.7889 (0.6518 -- 0.9785)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 13:16:28,277] [INFO] [logging.py:107:log_dist] [Rank 0] step=4000, skipped=17, lr=[np.float64(7.711639931134794e-06), np.float64(7.711639931134794e-06), np.float64(8.568488812371993e-06), np.float64(8.568488812371993e-06), np.float64(9.520543124857768e-06), np.float64(9.520543124857768e-06), np.float64(1.0578381249841966e-05), np.float64(1.0578381249841966e-05), np.float64(1.175375694426885e-05), np.float64(1.175375694426885e-05), np.float64(1.3059729938076501e-05), np.float64(1.3059729938076501e-05), np.float64(1.4510811042307222e-05), np.float64(1.4510811042307222e-05), np.float64(1.6123123380341355e-05), np.float64(1.6123123380341355e-05), np.float64(1.791458153371262e-05), np.float64(1.791458153371262e-05), np.float64(1.990509059301402e-05), np.float64(1.990509059301402e-05), np.float64(2.2116767325571135e-05), np.float64(2.2116767325571135e-05), np.float64(2.457418591730126e-05), np.float64(2.457418591730126e-05), np.float64(2.730465101922362e-05), np.float64(2.730465101922362e-05), np.float64(3.0338501132470688e-05), np.float64(3.0338501132470688e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 13:16:28,279] [INFO] [timer.py:264:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=12.938654914300649, CurrSamplesPerSec=8.73733879454709, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [8]  [140/483]  eta: 0:04:37  lr: 0.000030  min_lr: 0.000008  loss: 1.2513 (1.2109)  loss_scale: 32768.0000 (42993.4752)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3089 (5.4766)  time: 0.7205 (0.2807 -- 1.0161)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [8]  [160/483]  eta: 0:04:20  lr: 0.000030  min_lr: 0.000008  loss: 1.2204 (1.2111)  loss_scale: 32768.0000 (41723.2298)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5006 (5.5501)  time: 0.7953 (0.6362 -- 0.9859)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 13:16:54,201] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 4032
[2025-06-27 13:16:54,201] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 13:16:54,202] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [8]  [180/483]  eta: 0:04:04  lr: 0.000030  min_lr: 0.000008  loss: 1.1772 (1.2164)  loss_scale: 16384.0000 (39556.9503)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5453 (5.4845)  time: 0.8100 (0.5909 -- 0.9967)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [8]  [200/483]  eta: 0:03:49  lr: 0.000030  min_lr: 0.000008  loss: 1.3479 (1.2248)  loss_scale: 16384.0000 (37251.1841)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8516 (5.5230)  time: 0.8570 (0.6214 -- 1.0359)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [8]  [220/483]  eta: 0:03:33  lr: 0.000030  min_lr: 0.000008  loss: 1.0734 (1.2125)  loss_scale: 16384.0000 (35362.7511)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3610 (5.5026)  time: 0.7978 (0.6052 -- 1.0205)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [8]  [240/483]  eta: 0:03:16  lr: 0.000030  min_lr: 0.000008  loss: 1.1405 (1.2111)  loss_scale: 16384.0000 (33787.7510)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4307 (5.5583)  time: 0.7999 (0.6196 -- 1.0706)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [8]  [260/483]  eta: 0:03:00  lr: 0.000030  min_lr: 0.000008  loss: 1.1887 (1.2121)  loss_scale: 16384.0000 (32454.1303)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8140 (5.5298)  time: 0.8215 (0.6363 -- 1.0220)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [8]  [280/483]  eta: 0:02:44  lr: 0.000030  min_lr: 0.000008  loss: 1.1676 (1.2132)  loss_scale: 16384.0000 (31310.3488)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2501 (5.5666)  time: 0.8380 (0.6294 -- 1.0574)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 13:18:41,184] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:18:41,184] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [8]  [300/483]  eta: 0:02:29  lr: 0.000030  min_lr: 0.000008  loss: 1.3296 (1.2182)  loss_scale: 16384.0000 (30536.2924)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1823 (5.6543)  time: 0.8601 (0.6828 -- 1.0073)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [8]  [320/483]  eta: 0:02:12  lr: 0.000030  min_lr: 0.000008  loss: 1.3190 (1.2260)  loss_scale: 32768.0000 (30675.3396)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6541 (5.6239)  time: 0.8085 (0.5643 -- 1.0199)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [8]  [340/483]  eta: 0:01:56  lr: 0.000030  min_lr: 0.000008  loss: 1.1838 (1.2229)  loss_scale: 32768.0000 (30798.0762)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2917 (5.5890)  time: 0.8251 (0.6300 -- 0.9684)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [8]  [360/483]  eta: 0:01:40  lr: 0.000030  min_lr: 0.000008  loss: 1.3285 (1.2263)  loss_scale: 32768.0000 (30907.2133)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8531 (5.5631)  time: 0.8467 (0.6575 -- 1.0557)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [8]  [380/483]  eta: 0:01:24  lr: 0.000030  min_lr: 0.000008  loss: 1.0890 (1.2194)  loss_scale: 32768.0000 (31004.8924)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2063 (5.5348)  time: 0.8145 (0.6716 -- 0.9943)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [8]  [400/483]  eta: 0:01:07  lr: 0.000030  min_lr: 0.000008  loss: 1.1891 (1.2175)  loss_scale: 32768.0000 (31092.8279)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4796 (5.5135)  time: 0.8427 (0.6440 -- 1.3003)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [8]  [420/483]  eta: 0:00:51  lr: 0.000030  min_lr: 0.000008  loss: 1.0901 (1.2151)  loss_scale: 32768.0000 (31172.4086)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9586 (5.4946)  time: 0.8151 (0.5558 -- 1.0463)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:20:26,744] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:20:26,744] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [440/483]  eta: 0:00:35  lr: 0.000030  min_lr: 0.000008  loss: 1.1955 (1.2139)  loss_scale: 65536.0000 (32433.6327)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1283 (5.4825)  time: 0.8154 (0.5618 -- 1.0165)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [8]  [460/483]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.2429 (1.2166)  loss_scale: 65536.0000 (33869.7440)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7383 (5.4606)  time: 0.8139 (0.6484 -- 1.0246)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [8]  [480/483]  eta: 0:00:02  lr: 0.000030  min_lr: 0.000008  loss: 1.2060 (1.2147)  loss_scale: 65536.0000 (35186.4283)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4970 (5.4791)  time: 0.8060 (0.6061 -- 1.0276)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [8]  [482/483]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.2107 (1.2158)  loss_scale: 65536.0000 (35312.0994)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4912 (5.4815)  time: 0.8312 (0.6061 -- 1.0276)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [8] Total time: 0:06:35 (0.8184 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.2107 (1.2158)  loss_scale: 65536.0000 (35312.0994)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4912 (5.4815)
Val:  [ 0/23]  eta: 0:00:31  loss: 0.2460 (0.2460)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3626 (1.3626 -- 1.3626)  data: 1.0874 (1.0874 -- 1.0874)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.2260 (0.2739)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3628 (0.2028 -- 1.3626)  data: 0.0990 (0.0001 -- 1.0874)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.2064 (0.2391)  acc1: 100.0000 (97.6190)  acc5: 100.0000 (99.6032)  time: 0.2674 (0.2028 -- 0.2880)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.2108 (0.2450)  acc1: 100.0000 (97.4170)  acc5: 100.0000 (99.6310)  time: 0.2631 (0.1534 -- 0.2880)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:07 (0.3110 s / it)
* Acc@1 97.417 Acc@5 99.631 loss 0.245
Accuracy of the network on the 271 val images: 97.42%
[2025-06-27 13:21:20,561] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is begin to save!
[2025-06-27 13:21:20,571] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-best/mp_rank_00_model_states.pt
Max accuracy: 97.42%
Epoch: [9]  [  0/483]  eta: 0:34:14  lr: 0.000030  min_lr: 0.000008  loss: 1.1533 (1.1533)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5185 (4.5185)  time: 4.2527 (4.2527 -- 4.2527)  data: 3.2484 (3.2484 -- 3.2484)  max mem: 21487
Epoch: [9]  [ 20/483]  eta: 0:07:26  lr: 0.000030  min_lr: 0.000008  loss: 1.1508 (1.1910)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2069 (5.0867)  time: 0.7999 (0.6526 -- 0.9931)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [9]  [ 40/483]  eta: 0:06:42  lr: 0.000030  min_lr: 0.000008  loss: 1.2497 (1.2158)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3195 (5.3340)  time: 0.8516 (0.6631 -- 0.9963)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [9]  [ 60/483]  eta: 0:06:17  lr: 0.000030  min_lr: 0.000008  loss: 1.2327 (1.2413)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4223 (5.1901)  time: 0.8571 (0.6057 -- 1.0311)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:22:26,757] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:22:26,758] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-06-27 13:22:29,158] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 4420
[2025-06-27 13:22:29,158] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 13:22:29,158] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 80/483]  eta: 0:05:49  lr: 0.000030  min_lr: 0.000008  loss: 1.1252 (1.2219)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9366 (5.0854)  time: 0.7934 (0.6356 -- 0.9747)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:22:41,237] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 4435
[2025-06-27 13:22:41,238] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:22:41,238] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [100/483]  eta: 0:05:25  lr: 0.000030  min_lr: 0.000008  loss: 1.3074 (1.2424)  loss_scale: 32768.0000 (63264.9505)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4973 (5.1566)  time: 0.7768 (0.5845 -- 1.0106)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [9]  [120/483]  eta: 0:05:06  lr: 0.000030  min_lr: 0.000008  loss: 1.2389 (1.2468)  loss_scale: 32768.0000 (58224.1322)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3559 (5.4862)  time: 0.8217 (0.6615 -- 1.0297)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [9]  [140/483]  eta: 0:04:48  lr: 0.000030  min_lr: 0.000008  loss: 1.2104 (1.2391)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9963 (5.3382)  time: 0.8116 (0.6502 -- 1.0397)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [9]  [160/483]  eta: 0:04:29  lr: 0.000030  min_lr: 0.000008  loss: 1.1483 (1.2341)  loss_scale: 32768.0000 (51899.6273)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8213 (5.3323)  time: 0.7800 (0.6525 -- 1.0154)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [9]  [180/483]  eta: 0:04:12  lr: 0.000030  min_lr: 0.000008  loss: 1.2510 (1.2351)  loss_scale: 32768.0000 (49785.6354)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0698 (5.3190)  time: 0.8331 (0.6024 -- 1.0265)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [9]  [200/483]  eta: 0:03:54  lr: 0.000030  min_lr: 0.000008  loss: 1.1587 (1.2299)  loss_scale: 32768.0000 (48092.3383)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2706 (5.2728)  time: 0.8008 (0.5765 -- 1.0285)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:24:25,487] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:24:25,488] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [220/483]  eta: 0:03:38  lr: 0.000030  min_lr: 0.000008  loss: 1.2756 (1.2321)  loss_scale: 32768.0000 (47298.6063)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7904 (5.2326)  time: 0.8371 (0.6558 -- 1.0071)  data: 0.0001 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [9]  [240/483]  eta: 0:03:20  lr: 0.000030  min_lr: 0.000008  loss: 1.2552 (1.2347)  loss_scale: 65536.0000 (48812.0830)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1511 (5.2398)  time: 0.7878 (0.5052 -- 0.9934)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [9]  [260/483]  eta: 0:03:04  lr: 0.000030  min_lr: 0.000008  loss: 1.1730 (1.2301)  loss_scale: 65536.0000 (50093.6092)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4901 (5.3441)  time: 0.8224 (0.6262 -- 1.0257)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:25:03,366] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 4611
[2025-06-27 13:25:03,366] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:25:03,367] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [280/483]  eta: 0:02:47  lr: 0.000029  min_lr: 0.000007  loss: 1.2375 (1.2309)  loss_scale: 32768.0000 (49210.3060)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0211 (5.3774)  time: 0.7959 (0.5603 -- 1.0248)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [9]  [300/483]  eta: 0:02:30  lr: 0.000029  min_lr: 0.000007  loss: 1.3514 (1.2383)  loss_scale: 32768.0000 (48117.7940)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1843 (5.4297)  time: 0.8133 (0.6305 -- 1.0021)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [9]  [320/483]  eta: 0:02:13  lr: 0.000029  min_lr: 0.000007  loss: 1.3106 (1.2417)  loss_scale: 32768.0000 (47161.4206)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8446 (5.4498)  time: 0.7766 (0.6553 -- 0.9479)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [9]  [340/483]  eta: 0:01:57  lr: 0.000029  min_lr: 0.000007  loss: 1.1826 (1.2380)  loss_scale: 32768.0000 (46317.2317)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9880 (5.4258)  time: 0.7969 (0.5957 -- 1.0048)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [9]  [360/483]  eta: 0:01:40  lr: 0.000029  min_lr: 0.000007  loss: 1.2612 (1.2408)  loss_scale: 32768.0000 (45566.5817)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2542 (5.4192)  time: 0.8338 (0.6240 -- 1.0428)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [9]  [380/483]  eta: 0:01:24  lr: 0.000029  min_lr: 0.000007  loss: 1.1543 (1.2384)  loss_scale: 32768.0000 (44894.7402)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3570 (5.4167)  time: 0.8756 (0.6898 -- 1.0295)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
[2025-06-27 13:26:49,032] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:26:49,032] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [400/483]  eta: 0:01:08  lr: 0.000029  min_lr: 0.000007  loss: 1.1921 (1.2345)  loss_scale: 32768.0000 (44943.6409)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8471 (5.3926)  time: 0.8377 (0.6220 -- 1.0099)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [9]  [420/483]  eta: 0:00:51  lr: 0.000029  min_lr: 0.000007  loss: 1.2530 (1.2328)  loss_scale: 65536.0000 (45921.9002)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8055 (5.4152)  time: 0.8225 (0.6194 -- 1.0529)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 13:27:20,459] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 4778
[2025-06-27 13:27:20,459] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:27:20,459] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [440/483]  eta: 0:00:35  lr: 0.000029  min_lr: 0.000007  loss: 1.2658 (1.2327)  loss_scale: 32768.0000 (46068.3900)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1917 (5.4336)  time: 0.8189 (0.5860 -- 1.0098)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [9]  [460/483]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.2243 (1.2303)  loss_scale: 32768.0000 (45491.3666)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4451 (5.4219)  time: 0.8333 (0.5981 -- 1.0185)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [9]  [480/483]  eta: 0:00:02  lr: 0.000029  min_lr: 0.000007  loss: 1.1504 (1.2285)  loss_scale: 32768.0000 (44962.3285)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5199 (5.4416)  time: 0.8372 (0.5532 -- 1.0638)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [9]  [482/483]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.2286 (1.2294)  loss_scale: 32768.0000 (44911.8344)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3707 (5.4460)  time: 0.8090 (0.5532 -- 1.0638)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [9] Total time: 0:06:37 (0.8238 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.2286 (1.2294)  loss_scale: 32768.0000 (44911.8344)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3707 (5.4460)
[2025-06-27 13:28:02,695] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is begin to save!
[2025-06-27 13:28:02,699] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-9/mp_rank_00_model_states.pt
Val:  [ 0/23]  eta: 0:00:33  loss: 0.2246 (0.2246)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.4450 (1.4450 -- 1.4450)  data: 1.1943 (1.1943 -- 1.1943)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.2106 (0.2591)  acc1: 100.0000 (95.4545)  acc5: 100.0000 (99.2424)  time: 0.3652 (0.2352 -- 1.4450)  data: 0.1087 (0.0001 -- 1.1943)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1885 (0.2279)  acc1: 100.0000 (97.2222)  acc5: 100.0000 (99.6032)  time: 0.2584 (0.2352 -- 0.2737)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1952 (0.2341)  acc1: 100.0000 (97.0480)  acc5: 100.0000 (99.6310)  time: 0.2538 (0.1394 -- 0.2839)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val: Total time: 0:00:07 (0.3061 s / it)
* Acc@1 97.048 Acc@5 99.631 loss 0.234
Accuracy of the network on the 271 val images: 97.05%
Max accuracy: 97.42%
Epoch: [10]  [  0/483]  eta: 0:29:08  lr: 0.000029  min_lr: 0.000007  loss: 1.5690 (1.5690)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4695 (4.4695)  time: 3.6197 (3.6197 -- 3.6197)  data: 2.9438 (2.9438 -- 2.9438)  max mem: 21487
Epoch: [10]  [ 20/483]  eta: 0:07:04  lr: 0.000029  min_lr: 0.000007  loss: 1.1682 (1.2047)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3517 (4.9277)  time: 0.7823 (0.6186 -- 0.9623)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [10]  [ 40/483]  eta: 0:06:21  lr: 0.000029  min_lr: 0.000007  loss: 1.0679 (1.1532)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8002 (4.8666)  time: 0.8034 (0.4258 -- 0.9988)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [10]  [ 60/483]  eta: 0:05:46  lr: 0.000029  min_lr: 0.000007  loss: 1.1643 (1.1680)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2836 (5.7316)  time: 0.7319 (0.4227 -- 1.0162)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:29:19,497] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:29:19,497] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 80/483]  eta: 0:05:37  lr: 0.000029  min_lr: 0.000007  loss: 1.1489 (1.1674)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9429 (5.5337)  time: 0.8911 (0.7406 -- 1.0313)  data: 0.0002 (0.0001 -- 0.0012)  max mem: 21487
Epoch: [10]  [100/483]  eta: 0:05:20  lr: 0.000029  min_lr: 0.000007  loss: 1.1931 (1.1881)  loss_scale: 65536.0000 (40554.4554)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1829 (5.4524)  time: 0.8297 (0.6160 -- 1.0002)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [10]  [120/483]  eta: 0:05:01  lr: 0.000029  min_lr: 0.000007  loss: 1.2286 (1.1960)  loss_scale: 65536.0000 (44683.6364)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7854 (5.4463)  time: 0.8136 (0.5900 -- 1.0476)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:30:01,956] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 4958
[2025-06-27 13:30:01,956] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:30:01,956] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [140/483]  eta: 0:04:46  lr: 0.000029  min_lr: 0.000007  loss: 1.2296 (1.1942)  loss_scale: 32768.0000 (44620.2553)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8195 (5.3326)  time: 0.8524 (0.6056 -- 0.9899)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [10]  [160/483]  eta: 0:04:29  lr: 0.000029  min_lr: 0.000007  loss: 1.2791 (1.2044)  loss_scale: 32768.0000 (43147.9255)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6097 (5.2814)  time: 0.8208 (0.6287 -- 1.0058)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
[2025-06-27 13:30:36,103] [INFO] [logging.py:107:log_dist] [Rank 0] step=5000, skipped=23, lr=[np.float64(7.336780181759317e-06), np.float64(7.336780181759317e-06), np.float64(8.151977979732576e-06), np.float64(8.151977979732576e-06), np.float64(9.057753310813971e-06), np.float64(9.057753310813971e-06), np.float64(1.0064170345348859e-05), np.float64(1.0064170345348859e-05), np.float64(1.1182411494832064e-05), np.float64(1.1182411494832064e-05), np.float64(1.2424901660924516e-05), np.float64(1.2424901660924516e-05), np.float64(1.3805446289916128e-05), np.float64(1.3805446289916128e-05), np.float64(1.5339384766573474e-05), np.float64(1.5339384766573474e-05), np.float64(1.7043760851748303e-05), np.float64(1.7043760851748303e-05), np.float64(1.8937512057498113e-05), np.float64(1.8937512057498113e-05), np.float64(2.1041680063886797e-05), np.float64(2.1041680063886797e-05), np.float64(2.3379644515429773e-05), np.float64(2.3379644515429773e-05), np.float64(2.597738279492197e-05), np.float64(2.597738279492197e-05), np.float64(2.886375866102441e-05), np.float64(2.886375866102441e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 13:30:36,106] [INFO] [timer.py:264:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=12.189050189827809, CurrSamplesPerSec=10.37746083717512, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [10]  [180/483]  eta: 0:04:11  lr: 0.000029  min_lr: 0.000007  loss: 1.2113 (1.2090)  loss_scale: 32768.0000 (42000.9724)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7526 (5.2506)  time: 0.8186 (0.6627 -- 0.9865)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [10]  [200/483]  eta: 0:03:54  lr: 0.000029  min_lr: 0.000007  loss: 1.1526 (1.2015)  loss_scale: 32768.0000 (41082.2687)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6807 (5.2802)  time: 0.8142 (0.6351 -- 1.0034)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [10]  [220/483]  eta: 0:03:38  lr: 0.000029  min_lr: 0.000007  loss: 1.1910 (1.2008)  loss_scale: 32768.0000 (40329.8462)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7690 (5.2697)  time: 0.8425 (0.6839 -- 1.0190)  data: 0.0002 (0.0001 -- 0.0012)  max mem: 21487
Epoch: [10]  [240/483]  eta: 0:03:22  lr: 0.000029  min_lr: 0.000007  loss: 1.2274 (1.2012)  loss_scale: 32768.0000 (39702.3071)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6947 (5.3657)  time: 0.8550 (0.6018 -- 1.0495)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
[2025-06-27 13:31:50,121] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:31:50,122] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [260/483]  eta: 0:03:06  lr: 0.000029  min_lr: 0.000007  loss: 1.2992 (1.2041)  loss_scale: 32768.0000 (39673.1341)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5351 (5.3314)  time: 0.8485 (0.6446 -- 1.0301)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [10]  [280/483]  eta: 0:02:49  lr: 0.000029  min_lr: 0.000007  loss: 1.1725 (1.2004)  loss_scale: 65536.0000 (41513.9075)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4554 (5.3548)  time: 0.8317 (0.6060 -- 0.9981)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [10]  [300/483]  eta: 0:02:31  lr: 0.000029  min_lr: 0.000007  loss: 1.3686 (1.2105)  loss_scale: 65536.0000 (43110.0598)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2346 (5.4065)  time: 0.7812 (0.4058 -- 1.0058)  data: 0.0001 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [10]  [320/483]  eta: 0:02:15  lr: 0.000029  min_lr: 0.000007  loss: 1.2535 (1.2133)  loss_scale: 65536.0000 (44507.3146)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0340 (5.4244)  time: 0.8017 (0.6095 -- 1.0234)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [10]  [340/483]  eta: 0:01:58  lr: 0.000029  min_lr: 0.000007  loss: 1.2677 (1.2120)  loss_scale: 65536.0000 (45740.6686)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9254 (5.4132)  time: 0.8094 (0.6052 -- 1.0269)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [10]  [360/483]  eta: 0:01:41  lr: 0.000029  min_lr: 0.000007  loss: 1.2537 (1.2128)  loss_scale: 65536.0000 (46837.3629)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8920 (5.3650)  time: 0.8318 (0.6202 -- 1.0209)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
[2025-06-27 13:33:27,616] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 5207
[2025-06-27 13:33:27,617] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:33:27,617] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [380/483]  eta: 0:01:25  lr: 0.000028  min_lr: 0.000007  loss: 1.2147 (1.2117)  loss_scale: 65536.0000 (47474.8976)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0583 (5.3904)  time: 0.8148 (0.5293 -- 1.0140)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [10]  [400/483]  eta: 0:01:08  lr: 0.000028  min_lr: 0.000007  loss: 1.1606 (1.2080)  loss_scale: 32768.0000 (46741.3865)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3971 (5.3749)  time: 0.8243 (0.5694 -- 1.0628)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [10]  [420/483]  eta: 0:00:52  lr: 0.000028  min_lr: 0.000007  loss: 1.2493 (1.2123)  loss_scale: 32768.0000 (46077.5677)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3483 (5.4140)  time: 0.8235 (0.6043 -- 1.0461)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [10]  [440/483]  eta: 0:00:35  lr: 0.000028  min_lr: 0.000007  loss: 1.2466 (1.2139)  loss_scale: 32768.0000 (45473.9592)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5421 (5.3882)  time: 0.8378 (0.6948 -- 0.9961)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [10]  [460/483]  eta: 0:00:19  lr: 0.000028  min_lr: 0.000007  loss: 1.2770 (1.2164)  loss_scale: 32768.0000 (44922.7245)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8336 (5.3745)  time: 0.8187 (0.6448 -- 1.0168)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [10]  [480/483]  eta: 0:00:02  lr: 0.000028  min_lr: 0.000007  loss: 1.2415 (1.2169)  loss_scale: 32768.0000 (44417.3306)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5299 (5.3571)  time: 0.8397 (0.6548 -- 0.9989)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [10]  [482/483]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.2415 (1.2158)  loss_scale: 32768.0000 (44369.0932)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5140 (5.3467)  time: 0.8436 (0.6548 -- 0.9989)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [10] Total time: 0:06:39 (0.8277 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.2415 (1.2158)  loss_scale: 32768.0000 (44369.0932)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5140 (5.3467)
Val:  [ 0/23]  eta: 0:00:31  loss: 0.2258 (0.2258)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.3644 (1.3644 -- 1.3644)  data: 1.0901 (1.0901 -- 1.0901)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.2053 (0.2477)  acc1: 91.6667 (94.6970)  acc5: 100.0000 (99.2424)  time: 0.3658 (0.2523 -- 1.3644)  data: 0.0993 (0.0002 -- 1.0901)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1831 (0.2187)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.2621 (0.1934 -- 0.2869)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1853 (0.2231)  acc1: 100.0000 (96.6790)  acc5: 100.0000 (99.6310)  time: 0.2571 (0.1680 -- 0.2869)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Val: Total time: 0:00:07 (0.3058 s / it)
* Acc@1 96.679 Acc@5 99.631 loss 0.223
Accuracy of the network on the 271 val images: 96.68%
Max accuracy: 97.42%
Epoch: [11]  [  0/483]  eta: 0:32:42  lr: 0.000028  min_lr: 0.000007  loss: 1.1324 (1.1324)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.6872 (7.6872)  time: 4.0638 (4.0638 -- 4.0638)  data: 3.3621 (3.3621 -- 3.3621)  max mem: 21487
Epoch: [11]  [ 20/483]  eta: 0:07:36  lr: 0.000028  min_lr: 0.000007  loss: 1.1837 (1.1436)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6907 (5.5752)  time: 0.8330 (0.6158 -- 0.9977)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:35:24,643] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:35:24,643] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 40/483]  eta: 0:06:46  lr: 0.000028  min_lr: 0.000007  loss: 1.1799 (1.1773)  loss_scale: 65536.0000 (47153.9512)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2567 (5.3116)  time: 0.8428 (0.6469 -- 1.0188)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 13:35:44,629] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 5360
[2025-06-27 13:35:44,629] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:35:44,629] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 60/483]  eta: 0:06:14  lr: 0.000028  min_lr: 0.000007  loss: 1.1672 (1.1817)  loss_scale: 32768.0000 (45660.3279)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4693 (5.5377)  time: 0.8212 (0.6594 -- 1.0206)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [11]  [ 80/483]  eta: 0:05:49  lr: 0.000028  min_lr: 0.000007  loss: 1.1991 (1.1763)  loss_scale: 32768.0000 (42477.0370)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9880 (5.4793)  time: 0.8167 (0.6198 -- 1.0139)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [11]  [100/483]  eta: 0:05:26  lr: 0.000028  min_lr: 0.000007  loss: 1.2993 (1.2008)  loss_scale: 32768.0000 (40554.4554)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5054 (5.5318)  time: 0.7904 (0.6233 -- 1.0392)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 13:36:41,044] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 5429
[2025-06-27 13:36:41,044] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 13:36:41,044] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [120/483]  eta: 0:05:08  lr: 0.000028  min_lr: 0.000007  loss: 1.2035 (1.1994)  loss_scale: 32768.0000 (38590.4132)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2522 (5.5952)  time: 0.8422 (0.6052 -- 1.0189)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [11]  [140/483]  eta: 0:04:51  lr: 0.000028  min_lr: 0.000007  loss: 1.2182 (1.2013)  loss_scale: 16384.0000 (35440.5674)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1622 (5.4593)  time: 0.8339 (0.6596 -- 0.9861)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [11]  [160/483]  eta: 0:04:34  lr: 0.000028  min_lr: 0.000007  loss: 1.2729 (1.2004)  loss_scale: 16384.0000 (33073.2919)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7583 (5.4812)  time: 0.8620 (0.6395 -- 1.0303)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [11]  [180/483]  eta: 0:04:17  lr: 0.000028  min_lr: 0.000007  loss: 1.2494 (1.2027)  loss_scale: 16384.0000 (31229.1713)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8426 (5.4505)  time: 0.8333 (0.6058 -- 1.0267)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [11]  [200/483]  eta: 0:03:58  lr: 0.000028  min_lr: 0.000007  loss: 1.1163 (1.1964)  loss_scale: 16384.0000 (29752.0398)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8028 (5.4577)  time: 0.8086 (0.5249 -- 1.0180)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [11]  [220/483]  eta: 0:03:41  lr: 0.000028  min_lr: 0.000007  loss: 1.1590 (1.1986)  loss_scale: 16384.0000 (28542.2624)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1507 (5.5320)  time: 0.8263 (0.6093 -- 1.0036)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [11]  [240/483]  eta: 0:03:24  lr: 0.000028  min_lr: 0.000007  loss: 1.1772 (1.1953)  loss_scale: 16384.0000 (27533.2780)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4651 (5.4814)  time: 0.8189 (0.6606 -- 1.0296)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:38:28,077] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:38:28,078] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [260/483]  eta: 0:03:07  lr: 0.000028  min_lr: 0.000007  loss: 1.1154 (1.1932)  loss_scale: 32768.0000 (27683.3103)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5752 (5.4859)  time: 0.8358 (0.6613 -- 1.0026)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [11]  [280/483]  eta: 0:02:49  lr: 0.000028  min_lr: 0.000007  loss: 1.2500 (1.1954)  loss_scale: 32768.0000 (28045.2100)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0540 (5.5071)  time: 0.7978 (0.6217 -- 1.0001)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [11]  [300/483]  eta: 0:02:33  lr: 0.000028  min_lr: 0.000007  loss: 1.2870 (1.1995)  loss_scale: 32768.0000 (28359.0166)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9765 (5.5443)  time: 0.8589 (0.6346 -- 1.0272)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [11]  [320/483]  eta: 0:02:16  lr: 0.000028  min_lr: 0.000007  loss: 1.2839 (1.2040)  loss_scale: 32768.0000 (28633.7196)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6239 (5.5760)  time: 0.8137 (0.6282 -- 1.0235)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [11]  [340/483]  eta: 0:01:59  lr: 0.000028  min_lr: 0.000007  loss: 1.1232 (1.2003)  loss_scale: 32768.0000 (28876.1994)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8896 (5.5671)  time: 0.8179 (0.5949 -- 1.0379)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [11]  [360/483]  eta: 0:01:42  lr: 0.000028  min_lr: 0.000007  loss: 1.2265 (1.2001)  loss_scale: 32768.0000 (29091.8116)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4612 (5.5601)  time: 0.8370 (0.6235 -- 1.0319)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 13:40:14,444] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:40:14,444] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-06-27 13:40:15,124] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 5687
[2025-06-27 13:40:15,124] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:40:15,124] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [380/483]  eta: 0:01:26  lr: 0.000027  min_lr: 0.000007  loss: 1.2128 (1.2007)  loss_scale: 32768.0000 (29370.7927)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7019 (5.5293)  time: 0.8521 (0.6723 -- 0.9827)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [11]  [400/483]  eta: 0:01:09  lr: 0.000027  min_lr: 0.000007  loss: 1.0991 (1.1954)  loss_scale: 32768.0000 (29540.2294)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0671 (5.4836)  time: 0.8291 (0.6520 -- 1.0372)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [11]  [420/483]  eta: 0:00:52  lr: 0.000027  min_lr: 0.000007  loss: 1.1612 (1.1934)  loss_scale: 32768.0000 (29693.5677)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0917 (5.4823)  time: 0.7930 (0.4490 -- 1.0296)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [11]  [440/483]  eta: 0:00:35  lr: 0.000027  min_lr: 0.000007  loss: 1.2281 (1.1962)  loss_scale: 32768.0000 (29832.9977)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9264 (5.4760)  time: 0.8039 (0.6181 -- 1.0022)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 13:41:19,784] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 5766
[2025-06-27 13:41:19,784] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 13:41:19,784] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [460/483]  eta: 0:00:19  lr: 0.000027  min_lr: 0.000007  loss: 1.1771 (1.1960)  loss_scale: 32768.0000 (29676.0087)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5213 (5.4517)  time: 0.8400 (0.5551 -- 1.0079)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [11]  [480/483]  eta: 0:00:02  lr: 0.000027  min_lr: 0.000007  loss: 1.1108 (1.1941)  loss_scale: 16384.0000 (29123.3264)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7841 (5.4463)  time: 0.8335 (0.5615 -- 1.0069)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [11]  [482/483]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.1567 (1.1948)  loss_scale: 16384.0000 (29070.5756)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7841 (5.4471)  time: 0.8153 (0.5615 -- 1.0069)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [11] Total time: 0:06:42 (0.8329 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.1567 (1.1948)  loss_scale: 16384.0000 (29070.5756)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7841 (5.4471)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.2288 (0.2288)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.3301 (1.3301 -- 1.3301)  data: 1.0702 (1.0702 -- 1.0702)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.2131 (0.2405)  acc1: 91.6667 (94.6970)  acc5: 100.0000 (99.2424)  time: 0.3599 (0.2528 -- 1.3301)  data: 0.0975 (0.0001 -- 1.0702)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1559 (0.2143)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.2600 (0.1950 -- 0.2735)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1690 (0.2215)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.2538 (0.1469 -- 0.2735)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Val: Total time: 0:00:06 (0.3018 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.221
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [12]  [  0/483]  eta: 0:34:21  lr: 0.000027  min_lr: 0.000007  loss: 1.4238 (1.4238)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5919 (7.5919)  time: 4.2674 (4.2674 -- 4.2674)  data: 3.4654 (3.4654 -- 3.4654)  max mem: 21487
Epoch: [12]  [ 20/483]  eta: 0:07:32  lr: 0.000027  min_lr: 0.000007  loss: 1.1457 (1.1737)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8636 (5.5435)  time: 0.8122 (0.6463 -- 1.0144)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [12]  [ 40/483]  eta: 0:06:43  lr: 0.000027  min_lr: 0.000007  loss: 1.2932 (1.2317)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5401 (5.1151)  time: 0.8426 (0.6660 -- 1.0794)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 13:42:30,832] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 5840
[2025-06-27 13:42:30,832] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-06-27 13:42:30,832] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [12]  [ 60/483]  eta: 0:06:05  lr: 0.000027  min_lr: 0.000007  loss: 1.2437 (1.2211)  loss_scale: 8192.0000 (14100.9836)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2017 (5.3188)  time: 0.7695 (0.5759 -- 1.0186)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [12]  [ 80/483]  eta: 0:05:42  lr: 0.000027  min_lr: 0.000007  loss: 1.2658 (1.2363)  loss_scale: 8192.0000 (12641.9753)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6339 (5.3265)  time: 0.8048 (0.5392 -- 1.0277)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [12]  [100/483]  eta: 0:05:22  lr: 0.000027  min_lr: 0.000007  loss: 1.1387 (1.2206)  loss_scale: 8192.0000 (11760.7921)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3412 (5.2249)  time: 0.8065 (0.6101 -- 1.0189)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [12]  [120/483]  eta: 0:04:58  lr: 0.000027  min_lr: 0.000007  loss: 1.1502 (1.2185)  loss_scale: 8192.0000 (11170.9091)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4642 (5.2631)  time: 0.7208 (0.4712 -- 1.0300)  data: 0.0001 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [12]  [140/483]  eta: 0:04:42  lr: 0.000027  min_lr: 0.000007  loss: 1.2658 (1.2235)  loss_scale: 8192.0000 (10748.3688)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4445 (5.1908)  time: 0.8320 (0.5550 -- 1.0758)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [12]  [160/483]  eta: 0:04:25  lr: 0.000027  min_lr: 0.000007  loss: 1.2931 (1.2276)  loss_scale: 8192.0000 (10430.8075)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6758 (5.2704)  time: 0.8171 (0.6042 -- 1.0199)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 13:44:13,372] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:44:13,372] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [12]  [180/483]  eta: 0:04:09  lr: 0.000027  min_lr: 0.000007  loss: 1.2111 (1.2294)  loss_scale: 8192.0000 (10545.5028)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6717 (5.2679)  time: 0.8272 (0.6539 -- 1.0414)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [12]  [200/483]  eta: 0:03:52  lr: 0.000027  min_lr: 0.000007  loss: 1.2045 (1.2255)  loss_scale: 16384.0000 (11126.4478)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0045 (5.3519)  time: 0.8017 (0.5733 -- 1.0112)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:44:37,563] [INFO] [logging.py:107:log_dist] [Rank 0] step=6000, skipped=29, lr=[np.float64(6.804540963145721e-06), np.float64(6.804540963145721e-06), np.float64(7.560601070161914e-06), np.float64(7.560601070161914e-06), np.float64(8.400667855735457e-06), np.float64(8.400667855735457e-06), np.float64(9.33407539526162e-06), np.float64(9.33407539526162e-06), np.float64(1.0371194883624022e-05), np.float64(1.0371194883624022e-05), np.float64(1.1523549870693359e-05), np.float64(1.1523549870693359e-05), np.float64(1.2803944300770398e-05), np.float64(1.2803944300770398e-05), np.float64(1.4226604778633774e-05), np.float64(1.4226604778633774e-05), np.float64(1.5807338642926417e-05), np.float64(1.5807338642926417e-05), np.float64(1.756370960325157e-05), np.float64(1.756370960325157e-05), np.float64(1.9515232892501747e-05), np.float64(1.9515232892501747e-05), np.float64(2.1683592102779718e-05), np.float64(2.1683592102779718e-05), np.float64(2.4092880114199685e-05), np.float64(2.4092880114199685e-05), np.float64(2.6769866793555207e-05), np.float64(2.6769866793555207e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 13:44:37,563] [INFO] [timer.py:264:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=11.729847268821683, CurrSamplesPerSec=13.258721318227925, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [12]  [220/483]  eta: 0:03:35  lr: 0.000027  min_lr: 0.000007  loss: 1.3379 (1.2270)  loss_scale: 16384.0000 (11602.2443)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5096 (5.3937)  time: 0.8250 (0.5317 -- 1.0551)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [12]  [240/483]  eta: 0:03:14  lr: 0.000027  min_lr: 0.000007  loss: 1.1501 (1.2202)  loss_scale: 16384.0000 (11999.0705)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1118 (5.4455)  time: 0.5942 (0.3451 -- 0.9610)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [12]  [260/483]  eta: 0:02:54  lr: 0.000027  min_lr: 0.000007  loss: 1.2213 (1.2191)  loss_scale: 16384.0000 (12335.0805)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6167 (5.4156)  time: 0.5466 (0.4497 -- 0.6056)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [12]  [280/483]  eta: 0:02:35  lr: 0.000027  min_lr: 0.000007  loss: 1.3057 (1.2220)  loss_scale: 16384.0000 (12623.2598)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2808 (5.4403)  time: 0.5633 (0.4473 -- 0.6093)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [12]  [300/483]  eta: 0:02:17  lr: 0.000027  min_lr: 0.000007  loss: 1.3039 (1.2279)  loss_scale: 16384.0000 (12873.1429)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4219 (5.4556)  time: 0.5372 (0.4589 -- 0.6040)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 13:45:37,410] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:45:37,411] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [320/483]  eta: 0:02:00  lr: 0.000026  min_lr: 0.000007  loss: 1.0992 (1.2238)  loss_scale: 32768.0000 (14112.6978)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0285 (5.4464)  time: 0.5278 (0.3952 -- 0.6062)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [12]  [340/483]  eta: 0:01:43  lr: 0.000026  min_lr: 0.000007  loss: 1.0940 (1.2168)  loss_scale: 32768.0000 (15206.8504)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7973 (5.4826)  time: 0.5325 (0.4167 -- 0.6107)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [12]  [360/483]  eta: 0:01:28  lr: 0.000026  min_lr: 0.000007  loss: 1.2260 (1.2157)  loss_scale: 32768.0000 (16179.7673)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8186 (5.4853)  time: 0.5537 (0.4630 -- 0.6048)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [12]  [380/483]  eta: 0:01:12  lr: 0.000026  min_lr: 0.000007  loss: 1.2699 (1.2170)  loss_scale: 32768.0000 (17050.5407)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9749 (5.4729)  time: 0.5677 (0.4570 -- 0.6094)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [12]  [400/483]  eta: 0:00:58  lr: 0.000026  min_lr: 0.000007  loss: 1.1263 (1.2124)  loss_scale: 32768.0000 (17834.4539)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1695 (5.4678)  time: 0.5701 (0.5055 -- 0.6066)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [12]  [420/483]  eta: 0:00:43  lr: 0.000026  min_lr: 0.000007  loss: 1.0947 (1.2103)  loss_scale: 32768.0000 (18543.8860)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7003 (5.4540)  time: 0.5325 (0.4553 -- 0.6076)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:46:47,304] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:46:47,304] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [440/483]  eta: 0:00:29  lr: 0.000026  min_lr: 0.000007  loss: 1.1833 (1.2103)  loss_scale: 65536.0000 (20080.6168)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0846 (5.4361)  time: 0.5238 (0.4138 -- 0.6052)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:47:01,977] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 6253
[2025-06-27 13:47:01,978] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:47:01,978] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [460/483]  eta: 0:00:15  lr: 0.000026  min_lr: 0.000007  loss: 1.3281 (1.2116)  loss_scale: 65536.0000 (21768.3297)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7074 (5.4472)  time: 0.5266 (0.4470 -- 0.6033)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [12]  [480/483]  eta: 0:00:02  lr: 0.000026  min_lr: 0.000007  loss: 1.2296 (1.2139)  loss_scale: 32768.0000 (22225.6965)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5878 (5.4441)  time: 0.5409 (0.4773 -- 0.6180)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [12]  [482/483]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.2296 (1.2138)  loss_scale: 32768.0000 (22269.3499)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5878 (5.4410)  time: 0.5341 (0.4624 -- 0.6180)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [12] Total time: 0:05:24 (0.6724 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.2296 (1.2138)  loss_scale: 32768.0000 (22269.3499)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5878 (5.4410)
Val:  [ 0/23]  eta: 0:00:29  loss: 0.1686 (0.1686)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.2669 (1.2669 -- 1.2669)  data: 1.0545 (1.0545 -- 1.0545)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1764 (0.2400)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.2884 (0.1236 -- 1.2669)  data: 0.0960 (0.0002 -- 1.0545)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1651 (0.2160)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.1952 (0.1236 -- 0.2089)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1651 (0.2201)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.1946 (0.1164 -- 0.2079)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:05 (0.2389 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.220
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [13]  [  0/483]  eta: 0:25:43  lr: 0.000026  min_lr: 0.000007  loss: 1.2500 (1.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.8416 (8.8416)  time: 3.1950 (3.1950 -- 3.1950)  data: 2.7157 (2.7157 -- 2.7157)  max mem: 21487
Epoch: [13]  [ 20/483]  eta: 0:05:12  lr: 0.000026  min_lr: 0.000007  loss: 1.2001 (1.2282)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7125 (5.0215)  time: 0.5491 (0.4575 -- 0.6060)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [13]  [ 40/483]  eta: 0:04:29  lr: 0.000026  min_lr: 0.000007  loss: 1.3044 (1.2462)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3077 (4.7853)  time: 0.5396 (0.4057 -- 0.6079)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [13]  [ 60/483]  eta: 0:04:10  lr: 0.000026  min_lr: 0.000007  loss: 1.2045 (1.2355)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8033 (5.0011)  time: 0.5591 (0.4467 -- 0.6090)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 13:47:59,868] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 6344
[2025-06-27 13:47:59,869] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 13:47:59,869] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [ 80/483]  eta: 0:03:55  lr: 0.000026  min_lr: 0.000007  loss: 1.1580 (1.2161)  loss_scale: 16384.0000 (29531.6543)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9134 (4.9976)  time: 0.5615 (0.3975 -- 0.6125)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [13]  [100/483]  eta: 0:03:40  lr: 0.000026  min_lr: 0.000007  loss: 1.2563 (1.2207)  loss_scale: 16384.0000 (26928.1584)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7886 (4.9416)  time: 0.5411 (0.4513 -- 0.6096)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [13]  [120/483]  eta: 0:03:28  lr: 0.000026  min_lr: 0.000007  loss: 1.1453 (1.2104)  loss_scale: 16384.0000 (25185.3223)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1274 (5.0948)  time: 0.5644 (0.4631 -- 0.6307)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [13]  [140/483]  eta: 0:03:15  lr: 0.000026  min_lr: 0.000007  loss: 1.2531 (1.2146)  loss_scale: 16384.0000 (23936.9078)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1352 (4.9920)  time: 0.5534 (0.4708 -- 0.6047)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [13]  [160/483]  eta: 0:03:03  lr: 0.000026  min_lr: 0.000007  loss: 1.1735 (1.2088)  loss_scale: 16384.0000 (22998.6584)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7583 (4.8936)  time: 0.5435 (0.4610 -- 0.6051)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [13]  [180/483]  eta: 0:02:51  lr: 0.000026  min_lr: 0.000007  loss: 1.2407 (1.2039)  loss_scale: 16384.0000 (22267.7569)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9143 (4.9539)  time: 0.5445 (0.4562 -- 0.6024)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
[2025-06-27 13:49:11,125] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:49:11,125] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [200/483]  eta: 0:02:39  lr: 0.000026  min_lr: 0.000007  loss: 1.0264 (1.1917)  loss_scale: 16384.0000 (22252.8955)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2772 (4.9841)  time: 0.5644 (0.3520 -- 0.6260)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [13]  [220/483]  eta: 0:02:28  lr: 0.000026  min_lr: 0.000006  loss: 1.0531 (1.1846)  loss_scale: 32768.0000 (23204.4887)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0163 (5.0182)  time: 0.5490 (0.4058 -- 0.6083)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [13]  [240/483]  eta: 0:02:16  lr: 0.000025  min_lr: 0.000006  loss: 1.2405 (1.1897)  loss_scale: 32768.0000 (23998.1411)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4600 (5.0090)  time: 0.5372 (0.4571 -- 0.6065)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [13]  [260/483]  eta: 0:02:05  lr: 0.000025  min_lr: 0.000006  loss: 1.1277 (1.1907)  loss_scale: 32768.0000 (24670.1609)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1574 (5.0086)  time: 0.5788 (0.4497 -- 0.6335)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [13]  [280/483]  eta: 0:01:54  lr: 0.000025  min_lr: 0.000006  loss: 1.1852 (1.1943)  loss_scale: 32768.0000 (25246.5196)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2774 (5.0339)  time: 0.5561 (0.4554 -- 0.6077)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [13]  [300/483]  eta: 0:01:42  lr: 0.000025  min_lr: 0.000006  loss: 1.2093 (1.1984)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0350 (5.1070)  time: 0.5467 (0.4682 -- 0.6081)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [13]  [320/483]  eta: 0:01:31  lr: 0.000025  min_lr: 0.000006  loss: 1.3078 (1.2006)  loss_scale: 32768.0000 (26183.7757)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0786 (5.0549)  time: 0.5350 (0.4584 -- 0.6064)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 13:50:21,787] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:50:21,788] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [340/483]  eta: 0:01:19  lr: 0.000025  min_lr: 0.000006  loss: 1.1121 (1.1971)  loss_scale: 65536.0000 (28395.7302)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3368 (5.0257)  time: 0.5319 (0.4003 -- 0.6151)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [13]  [360/483]  eta: 0:01:08  lr: 0.000025  min_lr: 0.000006  loss: 1.2079 (1.1978)  loss_scale: 65536.0000 (30453.3629)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2907 (5.0430)  time: 0.5292 (0.2855 -- 0.6076)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 13:50:47,415] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 6649
[2025-06-27 13:50:47,415] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:50:47,415] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [380/483]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000006  loss: 1.1297 (1.1960)  loss_scale: 32768.0000 (31348.9134)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3851 (5.0731)  time: 0.5571 (0.4552 -- 0.6077)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [13]  [400/483]  eta: 0:00:46  lr: 0.000025  min_lr: 0.000006  loss: 1.0371 (1.1888)  loss_scale: 32768.0000 (31419.6908)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5632 (5.0681)  time: 0.5342 (0.3958 -- 0.6074)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [13]  [420/483]  eta: 0:00:34  lr: 0.000025  min_lr: 0.000006  loss: 1.1389 (1.1886)  loss_scale: 32768.0000 (31483.7435)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4959 (5.0743)  time: 0.5428 (0.4515 -- 0.6077)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [13]  [440/483]  eta: 0:00:23  lr: 0.000025  min_lr: 0.000006  loss: 1.3137 (1.1921)  loss_scale: 32768.0000 (31541.9864)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6925 (5.1099)  time: 0.5307 (0.4202 -- 0.6047)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [13]  [460/483]  eta: 0:00:12  lr: 0.000025  min_lr: 0.000006  loss: 1.1587 (1.1912)  loss_scale: 32768.0000 (31595.1757)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9420 (5.1154)  time: 0.5656 (0.4673 -- 0.6128)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [13]  [480/483]  eta: 0:00:01  lr: 0.000025  min_lr: 0.000006  loss: 1.1379 (1.1882)  loss_scale: 32768.0000 (31643.9418)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3844 (5.1603)  time: 0.5684 (0.5058 -- 0.6075)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [13]  [482/483]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.1425 (1.1890)  loss_scale: 32768.0000 (31648.5963)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4241 (5.1570)  time: 0.5605 (0.4768 -- 0.6075)  data: 0.0001 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [13] Total time: 0:04:27 (0.5547 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.1425 (1.1890)  loss_scale: 32768.0000 (31648.5963)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4241 (5.1570)
Val:  [ 0/23]  eta: 0:00:29  loss: 0.1594 (0.1594)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.2761 (1.2761 -- 1.2761)  data: 1.0634 (1.0634 -- 1.0634)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1594 (0.2436)  acc1: 100.0000 (95.4545)  acc5: 100.0000 (99.2424)  time: 0.2946 (0.1768 -- 1.2761)  data: 0.0969 (0.0002 -- 1.0634)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1522 (0.2135)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.1897 (0.1162 -- 0.2087)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1529 (0.2199)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.1858 (0.1162 -- 0.2087)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val: Total time: 0:00:05 (0.2346 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.220
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [14]  [  0/483]  eta: 0:32:02  lr: 0.000025  min_lr: 0.000006  loss: 1.4708 (1.4708)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5634 (7.5634)  time: 3.9813 (3.9813 -- 3.9813)  data: 3.3931 (3.3931 -- 3.3931)  max mem: 21487
[2025-06-27 13:52:06,969] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:52:06,969] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 20/483]  eta: 0:05:27  lr: 0.000025  min_lr: 0.000006  loss: 1.1118 (1.1769)  loss_scale: 32768.0000 (40569.9048)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2556 (5.9894)  time: 0.5446 (0.4772 -- 0.6063)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [14]  [ 40/483]  eta: 0:04:40  lr: 0.000025  min_lr: 0.000006  loss: 1.2302 (1.2238)  loss_scale: 65536.0000 (52748.4878)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1093 (5.8183)  time: 0.5553 (0.4116 -- 0.6265)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 13:52:22,329] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 6806
[2025-06-27 13:52:22,329] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:52:22,329] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 60/483]  eta: 0:04:14  lr: 0.000025  min_lr: 0.000006  loss: 1.1371 (1.2010)  loss_scale: 32768.0000 (47809.0492)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5215 (6.0462)  time: 0.5368 (0.4433 -- 0.6077)  data: 0.0001 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [14]  [ 80/483]  eta: 0:03:55  lr: 0.000025  min_lr: 0.000006  loss: 1.2023 (1.1971)  loss_scale: 32768.0000 (44095.2099)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1057 (5.8945)  time: 0.5284 (0.4142 -- 0.6069)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [14]  [100/483]  eta: 0:03:39  lr: 0.000025  min_lr: 0.000006  loss: 1.1076 (1.1881)  loss_scale: 32768.0000 (41852.1980)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0942 (5.9001)  time: 0.5337 (0.4401 -- 0.6181)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [14]  [120/483]  eta: 0:03:25  lr: 0.000024  min_lr: 0.000006  loss: 1.1630 (1.1828)  loss_scale: 32768.0000 (40350.6777)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5619 (5.9313)  time: 0.5273 (0.4580 -- 0.6063)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [14]  [140/483]  eta: 0:03:12  lr: 0.000024  min_lr: 0.000006  loss: 1.2185 (1.1816)  loss_scale: 32768.0000 (39275.1206)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5211 (5.7925)  time: 0.5375 (0.4604 -- 0.6045)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [14]  [160/483]  eta: 0:03:00  lr: 0.000024  min_lr: 0.000006  loss: 1.0994 (1.1722)  loss_scale: 32768.0000 (38466.7826)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1024 (5.7327)  time: 0.5312 (0.4604 -- 0.5857)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:53:31,817] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:53:31,817] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [180/483]  eta: 0:02:49  lr: 0.000024  min_lr: 0.000006  loss: 1.1935 (1.1737)  loss_scale: 32768.0000 (39285.3923)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2860 (5.7815)  time: 0.5634 (0.4470 -- 0.6394)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 13:53:44,973] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 6959
[2025-06-27 13:53:44,973] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:53:44,973] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [200/483]  eta: 0:02:38  lr: 0.000024  min_lr: 0.000006  loss: 1.0941 (1.1701)  loss_scale: 65536.0000 (41245.2935)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1413 (5.7982)  time: 0.5639 (0.4509 -- 0.6085)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [14]  [220/483]  eta: 0:02:26  lr: 0.000024  min_lr: 0.000006  loss: 1.2940 (1.1749)  loss_scale: 32768.0000 (40478.1176)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4483 (5.8254)  time: 0.5431 (0.4498 -- 0.6056)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
[2025-06-27 13:54:06,918] [INFO] [logging.py:107:log_dist] [Rank 0] step=7000, skipped=34, lr=[np.float64(6.139843417225412e-06), np.float64(6.139843417225412e-06), np.float64(6.822048241361569e-06), np.float64(6.822048241361569e-06), np.float64(7.580053601512853e-06), np.float64(7.580053601512853e-06), np.float64(8.422281779458726e-06), np.float64(8.422281779458726e-06), np.float64(9.358090866065251e-06), np.float64(9.358090866065251e-06), np.float64(1.03978787400725e-05), np.float64(1.03978787400725e-05), np.float64(1.1553198600080557e-05), np.float64(1.1553198600080557e-05), np.float64(1.283688733342284e-05), np.float64(1.283688733342284e-05), np.float64(1.42632081482476e-05), np.float64(1.42632081482476e-05), np.float64(1.5848009053608444e-05), np.float64(1.5848009053608444e-05), np.float64(1.7608898948453828e-05), np.float64(1.7608898948453828e-05), np.float64(1.9565443276059806e-05), np.float64(1.9565443276059806e-05), np.float64(2.1739381417844228e-05), np.float64(2.1739381417844228e-05), np.float64(2.4154868242049142e-05), np.float64(2.4154868242049142e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 13:54:06,920] [INFO] [timer.py:264:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=12.082779557838997, CurrSamplesPerSec=13.224322064306778, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [14]  [240/483]  eta: 0:02:15  lr: 0.000024  min_lr: 0.000006  loss: 1.1295 (1.1765)  loss_scale: 32768.0000 (39838.2739)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8711 (5.7982)  time: 0.5509 (0.4227 -- 0.6164)  data: 0.0001 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 13:54:12,470] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 7009
[2025-06-27 13:54:12,470] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 13:54:12,470] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [260/483]  eta: 0:02:03  lr: 0.000024  min_lr: 0.000006  loss: 1.1506 (1.1776)  loss_scale: 16384.0000 (38417.6552)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1159 (5.8473)  time: 0.5388 (0.4502 -- 0.6108)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [14]  [280/483]  eta: 0:01:52  lr: 0.000024  min_lr: 0.000006  loss: 1.1294 (1.1785)  loss_scale: 16384.0000 (36849.4235)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4878 (6.0542)  time: 0.5389 (0.3442 -- 0.6068)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [14]  [300/483]  eta: 0:01:41  lr: 0.000024  min_lr: 0.000006  loss: 1.2846 (1.1890)  loss_scale: 16384.0000 (35489.5947)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9256 (6.0075)  time: 0.5564 (0.4323 -- 0.6052)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [14]  [320/483]  eta: 0:01:30  lr: 0.000024  min_lr: 0.000006  loss: 1.0648 (1.1830)  loss_scale: 16384.0000 (34299.2150)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1242 (5.9332)  time: 0.5659 (0.4447 -- 0.6380)  data: 0.0003 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [14]  [340/483]  eta: 0:01:19  lr: 0.000024  min_lr: 0.000006  loss: 1.1277 (1.1826)  loss_scale: 16384.0000 (33248.4692)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3081 (5.8823)  time: 0.5434 (0.4567 -- 0.6073)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [14]  [360/483]  eta: 0:01:08  lr: 0.000024  min_lr: 0.000006  loss: 1.1430 (1.1797)  loss_scale: 16384.0000 (32314.1496)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8415 (5.8774)  time: 0.5624 (0.3439 -- 0.6078)  data: 0.0001 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 13:55:22,724] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:55:22,725] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [380/483]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000006  loss: 1.2232 (1.1785)  loss_scale: 16384.0000 (31692.9344)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8187 (5.8427)  time: 0.5009 (0.3956 -- 0.5520)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [14]  [400/483]  eta: 0:00:45  lr: 0.000024  min_lr: 0.000006  loss: 1.2924 (1.1827)  loss_scale: 32768.0000 (31746.5536)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7460 (5.8852)  time: 0.5497 (0.4555 -- 0.6075)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [14]  [420/483]  eta: 0:00:34  lr: 0.000024  min_lr: 0.000006  loss: 1.2152 (1.1827)  loss_scale: 32768.0000 (31795.0784)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4015 (5.8853)  time: 0.5449 (0.3955 -- 0.6086)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [14]  [440/483]  eta: 0:00:23  lr: 0.000024  min_lr: 0.000006  loss: 1.0713 (1.1813)  loss_scale: 32768.0000 (31839.2018)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9887 (5.8582)  time: 0.5449 (0.2808 -- 0.6291)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [14]  [460/483]  eta: 0:00:12  lr: 0.000024  min_lr: 0.000006  loss: 1.1587 (1.1815)  loss_scale: 32768.0000 (31879.4967)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9548 (5.8356)  time: 0.5649 (0.4388 -- 0.6247)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [14]  [480/483]  eta: 0:00:01  lr: 0.000023  min_lr: 0.000006  loss: 1.2112 (1.1816)  loss_scale: 32768.0000 (31916.4407)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2583 (5.8140)  time: 0.5667 (0.4398 -- 0.6090)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [14]  [482/483]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.2383 (1.1826)  loss_scale: 32768.0000 (31919.9669)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2459 (5.8085)  time: 0.5615 (0.4398 -- 0.6090)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [14] Total time: 0:04:27 (0.5528 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.2383 (1.1826)  loss_scale: 32768.0000 (31919.9669)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2459 (5.8085)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.1353 (0.1353)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3329 (1.3329 -- 1.3329)  data: 1.1329 (1.1329 -- 1.1329)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1551 (0.2375)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.2958 (0.1460 -- 1.3329)  data: 0.1032 (0.0001 -- 1.1329)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1475 (0.2127)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.1958 (0.1460 -- 0.2065)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1475 (0.2167)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.1922 (0.1236 -- 0.2065)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val: Total time: 0:00:05 (0.2423 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.217
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [15]  [  0/483]  eta: 0:31:33  lr: 0.000023  min_lr: 0.000006  loss: 1.2502 (1.2502)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0548 (4.0548)  time: 3.9193 (3.9193 -- 3.9193)  data: 3.2409 (3.2409 -- 3.2409)  max mem: 21487
Epoch: [15]  [ 20/483]  eta: 0:05:34  lr: 0.000023  min_lr: 0.000006  loss: 1.2198 (1.1857)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3937 (5.7303)  time: 0.5618 (0.4586 -- 0.6151)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 13:56:42,531] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:56:42,531] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 40/483]  eta: 0:04:40  lr: 0.000023  min_lr: 0.000006  loss: 1.1112 (1.1585)  loss_scale: 65536.0000 (48752.3902)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0768 (5.5326)  time: 0.5397 (0.4427 -- 0.6066)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [15]  [ 60/483]  eta: 0:04:15  lr: 0.000023  min_lr: 0.000006  loss: 1.1371 (1.1636)  loss_scale: 65536.0000 (54255.2131)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9257 (5.4422)  time: 0.5443 (0.4427 -- 0.6084)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 13:57:10,568] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 7318
[2025-06-27 13:57:10,568] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 13:57:10,568] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 80/483]  eta: 0:03:55  lr: 0.000023  min_lr: 0.000006  loss: 1.0872 (1.1616)  loss_scale: 65536.0000 (53804.2469)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9324 (5.3898)  time: 0.5205 (0.3420 -- 0.6077)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [15]  [100/483]  eta: 0:03:40  lr: 0.000023  min_lr: 0.000006  loss: 1.2296 (1.1668)  loss_scale: 32768.0000 (49638.6535)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2574 (5.3245)  time: 0.5397 (0.4340 -- 0.6107)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [15]  [120/483]  eta: 0:03:26  lr: 0.000023  min_lr: 0.000006  loss: 1.1670 (1.1625)  loss_scale: 32768.0000 (46850.1157)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7594 (5.2869)  time: 0.5436 (0.3938 -- 0.6067)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 13:57:37,798] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 7369
[2025-06-27 13:57:37,799] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 13:57:37,799] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [140/483]  eta: 0:03:12  lr: 0.000023  min_lr: 0.000006  loss: 1.1898 (1.1652)  loss_scale: 16384.0000 (42877.2766)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6890 (5.2290)  time: 0.5205 (0.4125 -- 0.6070)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [15]  [160/483]  eta: 0:03:00  lr: 0.000023  min_lr: 0.000006  loss: 1.1503 (1.1629)  loss_scale: 16384.0000 (39586.1863)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1205 (5.1269)  time: 0.5395 (0.4244 -- 0.6076)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [15]  [180/483]  eta: 0:02:48  lr: 0.000023  min_lr: 0.000006  loss: 1.3015 (1.1734)  loss_scale: 16384.0000 (37022.4088)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9333 (5.1819)  time: 0.5387 (0.3951 -- 0.6062)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [15]  [200/483]  eta: 0:02:37  lr: 0.000023  min_lr: 0.000006  loss: 1.0697 (1.1650)  loss_scale: 16384.0000 (34968.8358)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7081 (5.3024)  time: 0.5523 (0.4523 -- 0.6085)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [15]  [220/483]  eta: 0:02:25  lr: 0.000023  min_lr: 0.000006  loss: 1.1675 (1.1662)  loss_scale: 16384.0000 (33286.9502)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0637 (5.3268)  time: 0.5136 (0.2826 -- 0.6112)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [15]  [240/483]  eta: 0:02:14  lr: 0.000023  min_lr: 0.000006  loss: 1.2158 (1.1709)  loss_scale: 16384.0000 (31884.2158)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9284 (5.3905)  time: 0.5469 (0.4266 -- 0.6273)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 13:58:47,272] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:58:47,272] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [260/483]  eta: 0:02:03  lr: 0.000023  min_lr: 0.000006  loss: 1.1301 (1.1701)  loss_scale: 16384.0000 (31198.6513)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7520 (5.3940)  time: 0.5630 (0.4723 -- 0.6102)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [15]  [280/483]  eta: 0:01:52  lr: 0.000023  min_lr: 0.000006  loss: 1.1152 (1.1684)  loss_scale: 32768.0000 (31310.3488)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7687 (5.4477)  time: 0.5400 (0.4633 -- 0.6035)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [15]  [300/483]  eta: 0:01:41  lr: 0.000023  min_lr: 0.000006  loss: 1.2596 (1.1757)  loss_scale: 32768.0000 (31407.2027)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2793 (5.5416)  time: 0.5574 (0.4650 -- 0.6301)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [15]  [320/483]  eta: 0:01:30  lr: 0.000022  min_lr: 0.000006  loss: 0.9698 (1.1666)  loss_scale: 32768.0000 (31491.9875)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9876 (5.4812)  time: 0.5454 (0.2805 -- 0.6699)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [15]  [340/483]  eta: 0:01:18  lr: 0.000022  min_lr: 0.000006  loss: 1.2641 (1.1712)  loss_scale: 32768.0000 (31566.8270)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7313 (5.4566)  time: 0.5294 (0.4151 -- 0.6056)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [15]  [360/483]  eta: 0:01:07  lr: 0.000022  min_lr: 0.000006  loss: 1.2548 (1.1736)  loss_scale: 32768.0000 (31633.3740)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0511 (5.4844)  time: 0.5362 (0.4127 -- 0.6076)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [15]  [380/483]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000006  loss: 1.0846 (1.1684)  loss_scale: 32768.0000 (31692.9344)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3925 (5.4434)  time: 0.5257 (0.4207 -- 0.6263)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 13:59:56,449] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 13:59:56,449] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [400/483]  eta: 0:00:45  lr: 0.000022  min_lr: 0.000006  loss: 1.2101 (1.1689)  loss_scale: 65536.0000 (33380.8678)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6282 (5.4321)  time: 0.5008 (0.3978 -- 0.6109)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [15]  [420/483]  eta: 0:00:34  lr: 0.000022  min_lr: 0.000006  loss: 1.1787 (1.1729)  loss_scale: 65536.0000 (34908.4276)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1666 (5.4314)  time: 0.5446 (0.3956 -- 0.6072)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [15]  [440/483]  eta: 0:00:23  lr: 0.000022  min_lr: 0.000006  loss: 1.1254 (1.1732)  loss_scale: 65536.0000 (36297.4331)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6253 (5.3999)  time: 0.5529 (0.4578 -- 0.6346)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [15]  [460/483]  eta: 0:00:12  lr: 0.000022  min_lr: 0.000006  loss: 1.2488 (1.1762)  loss_scale: 65536.0000 (37565.9176)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5753 (5.3943)  time: 0.5448 (0.4656 -- 0.6111)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [15]  [480/483]  eta: 0:00:01  lr: 0.000022  min_lr: 0.000006  loss: 0.9338 (1.1691)  loss_scale: 65536.0000 (38728.9148)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4612 (5.4110)  time: 0.5493 (0.4568 -- 0.6105)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [15]  [482/483]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 0.9338 (1.1708)  loss_scale: 65536.0000 (38839.9172)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4339 (5.4074)  time: 0.5557 (0.4568 -- 0.6105)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [15] Total time: 0:04:24 (0.5468 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 0.9338 (1.1708)  loss_scale: 65536.0000 (38839.9172)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4339 (5.4074)
Val:  [ 0/23]  eta: 0:00:31  loss: 0.1421 (0.1421)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3493 (1.3493 -- 1.3493)  data: 1.1512 (1.1512 -- 1.1512)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1448 (0.2212)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.2967 (0.1357 -- 1.3493)  data: 0.1048 (0.0001 -- 1.1512)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1448 (0.1966)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.1947 (0.1357 -- 0.2094)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1448 (0.2011)  acc1: 100.0000 (96.6790)  acc5: 100.0000 (99.6310)  time: 0.1908 (0.1187 -- 0.2088)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val: Total time: 0:00:05 (0.2423 s / it)
* Acc@1 96.679 Acc@5 99.631 loss 0.201
Accuracy of the network on the 271 val images: 96.68%
Max accuracy: 97.42%
Epoch: [16]  [  0/483]  eta: 0:30:47  lr: 0.000022  min_lr: 0.000006  loss: 1.1368 (1.1368)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.0794 (8.0794)  time: 3.8259 (3.8259 -- 3.8259)  data: 3.2683 (3.2683 -- 3.2683)  max mem: 21487
Epoch: [16]  [ 20/483]  eta: 0:05:16  lr: 0.000022  min_lr: 0.000006  loss: 1.0968 (1.1268)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1442 (5.4310)  time: 0.5271 (0.4587 -- 0.6015)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 14:01:12,637] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 7751
[2025-06-27 14:01:12,638] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:01:12,638] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 40/483]  eta: 0:04:32  lr: 0.000022  min_lr: 0.000006  loss: 1.1533 (1.1607)  loss_scale: 32768.0000 (51150.0488)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4311 (5.2967)  time: 0.5448 (0.3502 -- 0.6135)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [16]  [ 60/483]  eta: 0:04:14  lr: 0.000022  min_lr: 0.000006  loss: 1.1600 (1.1709)  loss_scale: 32768.0000 (45123.1475)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2882 (5.5830)  time: 0.5739 (0.4430 -- 0.6148)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [16]  [ 80/483]  eta: 0:03:59  lr: 0.000022  min_lr: 0.000006  loss: 1.1069 (1.1654)  loss_scale: 32768.0000 (42072.4938)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1250 (5.6807)  time: 0.5719 (0.4127 -- 0.6120)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [16]  [100/483]  eta: 0:03:45  lr: 0.000022  min_lr: 0.000006  loss: 1.2774 (1.1779)  loss_scale: 32768.0000 (40230.0198)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7520 (5.6469)  time: 0.5597 (0.4294 -- 0.6132)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [16]  [120/483]  eta: 0:03:32  lr: 0.000022  min_lr: 0.000005  loss: 1.1529 (1.1809)  loss_scale: 32768.0000 (38996.6281)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6614 (5.8545)  time: 0.5706 (0.4645 -- 0.6149)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [16]  [140/483]  eta: 0:03:19  lr: 0.000022  min_lr: 0.000005  loss: 1.2083 (1.1808)  loss_scale: 32768.0000 (38113.1348)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3483 (5.8223)  time: 0.5614 (0.4568 -- 0.6394)  data: 0.0003 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 14:02:24,885] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:02:24,886] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [160/483]  eta: 0:03:05  lr: 0.000021  min_lr: 0.000005  loss: 1.0738 (1.1754)  loss_scale: 32768.0000 (39280.8944)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0841 (5.7371)  time: 0.5268 (0.4510 -- 0.6047)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [16]  [180/483]  eta: 0:02:53  lr: 0.000021  min_lr: 0.000005  loss: 1.2263 (1.1827)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5399 (5.6213)  time: 0.5657 (0.4808 -- 0.6120)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [16]  [200/483]  eta: 0:02:42  lr: 0.000021  min_lr: 0.000005  loss: 1.1581 (1.1808)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5007 (5.6325)  time: 0.5690 (0.4369 -- 0.6142)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [16]  [220/483]  eta: 0:02:30  lr: 0.000021  min_lr: 0.000005  loss: 1.1324 (1.1801)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4773 (5.5195)  time: 0.5544 (0.4594 -- 0.6297)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [16]  [240/483]  eta: 0:02:18  lr: 0.000021  min_lr: 0.000005  loss: 1.0559 (1.1778)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9266 (5.5097)  time: 0.5315 (0.3484 -- 0.6114)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [16]  [260/483]  eta: 0:02:06  lr: 0.000021  min_lr: 0.000005  loss: 1.1168 (1.1758)  loss_scale: 65536.0000 (49340.3218)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6789 (5.5326)  time: 0.5431 (0.4553 -- 0.6111)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 14:03:30,503] [INFO] [logging.py:107:log_dist] [Rank 0] step=8000, skipped=38, lr=[np.float64(5.373810808320179e-06), np.float64(5.373810808320179e-06), np.float64(5.970900898133533e-06), np.float64(5.970900898133533e-06), np.float64(6.6343343312594795e-06), np.float64(6.6343343312594795e-06), np.float64(7.371482590288311e-06), np.float64(7.371482590288311e-06), np.float64(8.190536211431456e-06), np.float64(8.190536211431456e-06), np.float64(9.100595790479395e-06), np.float64(9.100595790479395e-06), np.float64(1.0111773100532661e-05), np.float64(1.0111773100532661e-05), np.float64(1.123530344503629e-05), np.float64(1.123530344503629e-05), np.float64(1.2483670494484767e-05), np.float64(1.2483670494484767e-05), np.float64(1.3870744993871961e-05), np.float64(1.3870744993871961e-05), np.float64(1.541193888207996e-05), np.float64(1.541193888207996e-05), np.float64(1.7124376535644397e-05), np.float64(1.7124376535644397e-05), np.float64(1.9027085039604884e-05), np.float64(1.9027085039604884e-05), np.float64(2.1141205599560983e-05), np.float64(2.1141205599560983e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 14:03:30,505] [INFO] [timer.py:264:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=12.376911869072751, CurrSamplesPerSec=13.866314677806573, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
[2025-06-27 14:03:35,644] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:03:35,644] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [280/483]  eta: 0:01:54  lr: 0.000021  min_lr: 0.000005  loss: 1.1788 (1.1729)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3209 (5.5529)  time: 0.5620 (0.4131 -- 0.6313)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
[2025-06-27 14:03:36,237] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 8009
[2025-06-27 14:03:36,237] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 14:03:36,237] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-06-27 14:03:37,254] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 8011
[2025-06-27 14:03:37,254] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:03:37,254] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [300/483]  eta: 0:01:43  lr: 0.000021  min_lr: 0.000005  loss: 1.1917 (1.1741)  loss_scale: 32768.0000 (49750.7508)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5785 (5.5758)  time: 0.5535 (0.4633 -- 0.6105)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [16]  [320/483]  eta: 0:01:32  lr: 0.000021  min_lr: 0.000005  loss: 1.0602 (1.1732)  loss_scale: 32768.0000 (48692.6355)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5582 (5.5569)  time: 0.5590 (0.4623 -- 0.6132)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [16]  [340/483]  eta: 0:01:20  lr: 0.000021  min_lr: 0.000005  loss: 1.1229 (1.1695)  loss_scale: 32768.0000 (47758.6393)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7693 (5.5119)  time: 0.5357 (0.4589 -- 0.6108)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [16]  [360/483]  eta: 0:01:08  lr: 0.000021  min_lr: 0.000005  loss: 1.2128 (1.1717)  loss_scale: 32768.0000 (46928.1330)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5699 (5.5438)  time: 0.5187 (0.2793 -- 0.6139)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [16]  [380/483]  eta: 0:00:57  lr: 0.000021  min_lr: 0.000005  loss: 1.1007 (1.1710)  loss_scale: 32768.0000 (46184.8189)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1618 (5.5412)  time: 0.5573 (0.4604 -- 0.6116)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [16]  [400/483]  eta: 0:00:46  lr: 0.000021  min_lr: 0.000005  loss: 1.1394 (1.1697)  loss_scale: 32768.0000 (45515.6509)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3887 (5.5622)  time: 0.5596 (0.4475 -- 0.6120)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 14:04:47,728] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:04:47,728] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-06-27 14:04:51,046] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 8146
[2025-06-27 14:04:51,047] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:04:51,047] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [420/483]  eta: 0:00:35  lr: 0.000021  min_lr: 0.000005  loss: 1.0131 (1.1682)  loss_scale: 32768.0000 (45377.0641)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7923 (5.5606)  time: 0.5350 (0.4521 -- 0.7106)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [16]  [440/483]  eta: 0:00:23  lr: 0.000021  min_lr: 0.000005  loss: 1.1943 (1.1689)  loss_scale: 32768.0000 (44805.2245)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1082 (5.5678)  time: 0.5236 (0.2993 -- 0.6252)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [16]  [460/483]  eta: 0:00:12  lr: 0.000021  min_lr: 0.000005  loss: 1.1366 (1.1680)  loss_scale: 32768.0000 (44283.0022)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8441 (5.5690)  time: 0.5342 (0.4544 -- 0.6125)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [16]  [480/483]  eta: 0:00:01  lr: 0.000020  min_lr: 0.000005  loss: 1.1354 (1.1687)  loss_scale: 32768.0000 (43804.2079)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0352 (5.5540)  time: 0.5367 (0.4205 -- 0.6107)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [16]  [482/483]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.1955 (1.1682)  loss_scale: 32768.0000 (43758.5093)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1894 (5.5613)  time: 0.5365 (0.4205 -- 0.6107)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [16] Total time: 0:04:28 (0.5556 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.1955 (1.1682)  loss_scale: 32768.0000 (43758.5093)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1894 (5.5613)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.1456 (0.1456)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3062 (1.3062 -- 1.3062)  data: 1.0995 (1.0995 -- 1.0995)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1456 (0.2191)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.2970 (0.1342 -- 1.3062)  data: 0.1002 (0.0002 -- 1.0995)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1410 (0.1982)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.1978 (0.1342 -- 0.2109)  data: 0.0002 (0.0002 -- 0.0004)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1410 (0.2024)  acc1: 100.0000 (96.6790)  acc5: 100.0000 (99.6310)  time: 0.1942 (0.1212 -- 0.2109)  data: 0.0002 (0.0002 -- 0.0005)  max mem: 21487
Val: Total time: 0:00:05 (0.2433 s / it)
* Acc@1 96.679 Acc@5 99.631 loss 0.202
Accuracy of the network on the 271 val images: 96.68%
Max accuracy: 97.42%
Epoch: [17]  [  0/483]  eta: 0:29:19  lr: 0.000020  min_lr: 0.000005  loss: 1.4237 (1.4237)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5481 (4.5481)  time: 3.6438 (3.6438 -- 3.6438)  data: 3.0997 (3.0997 -- 3.0997)  max mem: 21487
Epoch: [17]  [ 20/483]  eta: 0:05:23  lr: 0.000020  min_lr: 0.000005  loss: 1.0116 (1.1155)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7495 (5.0449)  time: 0.5526 (0.4466 -- 0.6115)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [17]  [ 40/483]  eta: 0:04:23  lr: 0.000020  min_lr: 0.000005  loss: 1.2118 (1.1456)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7536 (5.0840)  time: 0.4855 (0.2841 -- 0.6124)  data: 0.0003 (0.0001 -- 0.0018)  max mem: 21487
Epoch: [17]  [ 60/483]  eta: 0:04:07  lr: 0.000020  min_lr: 0.000005  loss: 1.2549 (1.1827)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6892 (5.3425)  time: 0.5637 (0.4617 -- 0.6123)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 14:06:08,589] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:06:08,589] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 80/483]  eta: 0:03:53  lr: 0.000020  min_lr: 0.000005  loss: 1.1375 (1.1729)  loss_scale: 65536.0000 (39645.2346)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4009 (5.1360)  time: 0.5628 (0.4520 -- 0.6113)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [17]  [100/483]  eta: 0:03:37  lr: 0.000020  min_lr: 0.000005  loss: 1.1303 (1.1664)  loss_scale: 65536.0000 (44772.1188)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3531 (5.1914)  time: 0.5240 (0.4588 -- 0.6100)  data: 0.0003 (0.0001 -- 0.0027)  max mem: 21487
Epoch: [17]  [120/483]  eta: 0:03:25  lr: 0.000020  min_lr: 0.000005  loss: 1.1622 (1.1586)  loss_scale: 65536.0000 (48204.1653)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0389 (5.1312)  time: 0.5460 (0.4601 -- 0.6173)  data: 0.0003 (0.0001 -- 0.0018)  max mem: 21487
Epoch: [17]  [140/483]  eta: 0:03:12  lr: 0.000020  min_lr: 0.000005  loss: 1.1986 (1.1591)  loss_scale: 65536.0000 (50662.5816)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2069 (5.1846)  time: 0.5349 (0.4150 -- 0.6087)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [17]  [160/483]  eta: 0:02:59  lr: 0.000020  min_lr: 0.000005  loss: 1.1826 (1.1585)  loss_scale: 65536.0000 (52510.2112)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5546 (5.2413)  time: 0.5208 (0.4102 -- 0.6075)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 14:07:01,593] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 8374
[2025-06-27 14:07:01,593] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:07:01,593] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [180/483]  eta: 0:02:46  lr: 0.000020  min_lr: 0.000005  loss: 1.2665 (1.1681)  loss_scale: 32768.0000 (50690.8287)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8455 (5.2219)  time: 0.5074 (0.3978 -- 0.5943)  data: 0.0002 (0.0001 -- 0.0024)  max mem: 21487
Epoch: [17]  [200/483]  eta: 0:02:34  lr: 0.000020  min_lr: 0.000005  loss: 1.0889 (1.1668)  loss_scale: 32768.0000 (48907.4627)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5295 (5.3240)  time: 0.5028 (0.2928 -- 0.6047)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [17]  [220/483]  eta: 0:02:22  lr: 0.000020  min_lr: 0.000005  loss: 1.0651 (1.1619)  loss_scale: 32768.0000 (47446.8778)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9575 (5.3141)  time: 0.4981 (0.3959 -- 0.6030)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [17]  [240/483]  eta: 0:02:10  lr: 0.000020  min_lr: 0.000005  loss: 1.2486 (1.1684)  loss_scale: 32768.0000 (46228.7137)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4243 (5.5098)  time: 0.4976 (0.2799 -- 0.6100)  data: 0.0005 (0.0001 -- 0.0067)  max mem: 21487
Epoch: [17]  [260/483]  eta: 0:01:59  lr: 0.000020  min_lr: 0.000005  loss: 1.0494 (1.1620)  loss_scale: 32768.0000 (45197.2414)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4899 (5.4317)  time: 0.5185 (0.3927 -- 0.6092)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [17]  [280/483]  eta: 0:01:48  lr: 0.000020  min_lr: 0.000005  loss: 1.1076 (1.1643)  loss_scale: 32768.0000 (44312.5979)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8740 (5.4570)  time: 0.5463 (0.4853 -- 0.6476)  data: 0.0007 (0.0001 -- 0.0094)  max mem: 21487
[2025-06-27 14:08:07,877] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:08:07,877] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [300/483]  eta: 0:01:38  lr: 0.000019  min_lr: 0.000005  loss: 1.3063 (1.1749)  loss_scale: 32768.0000 (44525.2890)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3701 (5.4655)  time: 0.5292 (0.3981 -- 0.6071)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [17]  [320/483]  eta: 0:01:27  lr: 0.000019  min_lr: 0.000005  loss: 1.2054 (1.1774)  loss_scale: 65536.0000 (45834.3676)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4102 (5.4114)  time: 0.5622 (0.4650 -- 0.6111)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 14:08:27,794] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 8539
[2025-06-27 14:08:27,794] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:08:27,794] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [340/483]  eta: 0:01:17  lr: 0.000019  min_lr: 0.000005  loss: 1.1618 (1.1760)  loss_scale: 32768.0000 (45740.6686)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7134 (5.3537)  time: 0.5644 (0.4480 -- 0.6116)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [17]  [360/483]  eta: 0:01:06  lr: 0.000019  min_lr: 0.000005  loss: 1.1786 (1.1773)  loss_scale: 32768.0000 (45021.9612)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7780 (5.3442)  time: 0.5478 (0.4579 -- 0.6192)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [17]  [380/483]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000005  loss: 1.1070 (1.1748)  loss_scale: 32768.0000 (44378.7087)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6563 (5.3408)  time: 0.5510 (0.4102 -- 0.6129)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [17]  [400/483]  eta: 0:00:44  lr: 0.000019  min_lr: 0.000005  loss: 1.1346 (1.1750)  loss_scale: 32768.0000 (43799.6209)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5229 (5.3393)  time: 0.5392 (0.4168 -- 0.6145)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [17]  [420/483]  eta: 0:00:33  lr: 0.000019  min_lr: 0.000005  loss: 1.1856 (1.1768)  loss_scale: 32768.0000 (43275.5534)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3447 (5.3053)  time: 0.5124 (0.2849 -- 0.6106)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [17]  [440/483]  eta: 0:00:23  lr: 0.000019  min_lr: 0.000005  loss: 1.0689 (1.1744)  loss_scale: 32768.0000 (42799.0204)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5759 (5.3003)  time: 0.5572 (0.4421 -- 0.6203)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 14:09:37,743] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:09:37,743] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [460/483]  eta: 0:00:12  lr: 0.000019  min_lr: 0.000005  loss: 1.0790 (1.1709)  loss_scale: 32768.0000 (42648.1562)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5297 (5.3051)  time: 0.5282 (0.4031 -- 0.6092)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [17]  [480/483]  eta: 0:00:01  lr: 0.000019  min_lr: 0.000005  loss: 1.1031 (1.1674)  loss_scale: 65536.0000 (43599.8337)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9243 (5.3181)  time: 0.5756 (0.4664 -- 0.6134)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [17]  [482/483]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.1031 (1.1687)  loss_scale: 65536.0000 (43690.6667)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9243 (5.3273)  time: 0.5761 (0.4664 -- 0.6134)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [17] Total time: 0:04:21 (0.5411 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.1031 (1.1687)  loss_scale: 65536.0000 (43690.6667)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9243 (5.3273)
Val:  [ 0/23]  eta: 0:00:35  loss: 0.1456 (0.1456)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.5471 (1.5471 -- 1.5471)  data: 1.3468 (1.3468 -- 1.3468)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1648 (0.2185)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3211 (0.1877 -- 1.5471)  data: 0.1227 (0.0002 -- 1.3468)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1402 (0.2014)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.1967 (0.1595 -- 0.2105)  data: 0.0003 (0.0002 -- 0.0006)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1402 (0.2059)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.1936 (0.1274 -- 0.2105)  data: 0.0003 (0.0002 -- 0.0006)  max mem: 21487
Val: Total time: 0:00:05 (0.2526 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.206
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [18]  [  0/483]  eta: 0:32:11  lr: 0.000019  min_lr: 0.000005  loss: 0.9908 (0.9908)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2880 (6.2880)  time: 3.9998 (3.9998 -- 3.9998)  data: 3.4245 (3.4245 -- 3.4245)  max mem: 21487
Epoch: [18]  [ 20/483]  eta: 0:05:19  lr: 0.000019  min_lr: 0.000005  loss: 1.1203 (1.1304)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9677 (6.9243)  time: 0.5235 (0.3973 -- 0.6098)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [18]  [ 40/483]  eta: 0:04:30  lr: 0.000019  min_lr: 0.000005  loss: 1.1844 (1.1697)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2562 (5.9616)  time: 0.5281 (0.4225 -- 0.6194)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [18]  [ 60/483]  eta: 0:04:26  lr: 0.000019  min_lr: 0.000005  loss: 1.1950 (1.1888)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7903 (6.0150)  time: 0.6699 (0.5067 -- 0.9851)  data: 0.0005 (0.0001 -- 0.0068)  max mem: 21487
[2025-06-27 14:10:38,522] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 8757
[2025-06-27 14:10:38,523] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:10:38,524] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-06-27 14:10:50,036] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 8771
[2025-06-27 14:10:50,037] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 14:10:50,037] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [18]  [ 80/483]  eta: 0:04:31  lr: 0.000019  min_lr: 0.000005  loss: 1.2163 (1.1903)  loss_scale: 32768.0000 (57445.1358)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3612 (5.8610)  time: 0.8040 (0.5552 -- 1.0185)  data: 0.0007 (0.0001 -- 0.0069)  max mem: 21487
Epoch: [18]  [100/483]  eta: 0:04:29  lr: 0.000019  min_lr: 0.000005  loss: 1.2288 (1.1999)  loss_scale: 16384.0000 (49314.2178)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6146 (5.6188)  time: 0.8330 (0.6338 -- 0.9979)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [18]  [120/483]  eta: 0:04:24  lr: 0.000018  min_lr: 0.000005  loss: 1.0601 (1.1852)  loss_scale: 16384.0000 (43871.2066)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3106 (5.7157)  time: 0.8563 (0.6070 -- 1.0580)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [18]  [140/483]  eta: 0:04:14  lr: 0.000018  min_lr: 0.000005  loss: 1.1991 (1.1910)  loss_scale: 16384.0000 (39972.3121)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5324 (5.5987)  time: 0.8199 (0.6164 -- 1.0173)  data: 0.0002 (0.0001 -- 0.0025)  max mem: 21487
Epoch: [18]  [160/483]  eta: 0:04:02  lr: 0.000018  min_lr: 0.000005  loss: 1.3005 (1.1971)  loss_scale: 16384.0000 (37042.0870)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1348 (5.6101)  time: 0.8025 (0.6170 -- 1.0011)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [18]  [180/483]  eta: 0:03:50  lr: 0.000018  min_lr: 0.000005  loss: 1.2040 (1.1960)  loss_scale: 16384.0000 (34759.4254)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7665 (5.6101)  time: 0.8375 (0.6834 -- 1.0079)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [18]  [200/483]  eta: 0:03:36  lr: 0.000018  min_lr: 0.000005  loss: 1.0125 (1.1872)  loss_scale: 16384.0000 (32931.0249)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9831 (5.6337)  time: 0.8006 (0.6070 -- 0.9915)  data: 0.0003 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 14:12:36,270] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:12:36,270] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [220/483]  eta: 0:03:22  lr: 0.000018  min_lr: 0.000005  loss: 1.1671 (1.1824)  loss_scale: 32768.0000 (32545.5928)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2000 (5.6168)  time: 0.8268 (0.5723 -- 0.9974)  data: 0.0004 (0.0001 -- 0.0031)  max mem: 21487
Epoch: [18]  [240/483]  eta: 0:03:07  lr: 0.000018  min_lr: 0.000005  loss: 1.2525 (1.1884)  loss_scale: 32768.0000 (32564.0498)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0830 (5.6208)  time: 0.7815 (0.4539 -- 1.0218)  data: 0.0004 (0.0001 -- 0.0020)  max mem: 21487
Epoch: [18]  [260/483]  eta: 0:02:51  lr: 0.000018  min_lr: 0.000005  loss: 1.1459 (1.1858)  loss_scale: 32768.0000 (32579.6782)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7189 (5.6657)  time: 0.7726 (0.5929 -- 1.0170)  data: 0.0009 (0.0001 -- 0.0089)  max mem: 21487
Epoch: [18]  [280/483]  eta: 0:02:37  lr: 0.000018  min_lr: 0.000005  loss: 1.2850 (1.1904)  loss_scale: 32768.0000 (32593.0819)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5499 (5.6026)  time: 0.8287 (0.6601 -- 1.0153)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [18]  [300/483]  eta: 0:02:22  lr: 0.000018  min_lr: 0.000005  loss: 1.1787 (1.1930)  loss_scale: 32768.0000 (32604.7043)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9549 (5.5930)  time: 0.8436 (0.6583 -- 0.9989)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 14:13:56,583] [INFO] [logging.py:107:log_dist] [Rank 0] step=9000, skipped=45, lr=[np.float64(4.542311232396982e-06), np.float64(4.542311232396982e-06), np.float64(5.047012480441092e-06), np.float64(5.047012480441092e-06), np.float64(5.6077916449345455e-06), np.float64(5.6077916449345455e-06), np.float64(6.230879605482829e-06), np.float64(6.230879605482829e-06), np.float64(6.923199561647587e-06), np.float64(6.923199561647587e-06), np.float64(7.692443957386208e-06), np.float64(7.692443957386208e-06), np.float64(8.547159952651342e-06), np.float64(8.547159952651342e-06), np.float64(9.496844391834823e-06), np.float64(9.496844391834823e-06), np.float64(1.0552049324260915e-05), np.float64(1.0552049324260915e-05), np.float64(1.1724499249178794e-05), np.float64(1.1724499249178794e-05), np.float64(1.3027221387976439e-05), np.float64(1.3027221387976439e-05), np.float64(1.447469043108493e-05), np.float64(1.447469043108493e-05), np.float64(1.6082989367872143e-05), np.float64(1.6082989367872143e-05), np.float64(1.7869988186524604e-05), np.float64(1.7869988186524604e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 14:13:56,588] [INFO] [timer.py:264:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=12.480930300310758, CurrSamplesPerSec=10.236172698826488, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [18]  [320/483]  eta: 0:02:07  lr: 0.000018  min_lr: 0.000005  loss: 1.1389 (1.1895)  loss_scale: 32768.0000 (32614.8785)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2950 (5.5473)  time: 0.8501 (0.6729 -- 1.0192)  data: 0.0008 (0.0001 -- 0.0118)  max mem: 21487
[2025-06-27 14:14:21,040] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:14:21,041] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [340/483]  eta: 0:01:52  lr: 0.000018  min_lr: 0.000005  loss: 1.2441 (1.1918)  loss_scale: 32768.0000 (33296.5161)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6205 (5.5234)  time: 0.8227 (0.6199 -- 1.0048)  data: 0.0002 (0.0001 -- 0.0014)  max mem: 21487
[2025-06-27 14:14:39,723] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 9051
[2025-06-27 14:14:39,724] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:14:39,724] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [360/483]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000004  loss: 1.1141 (1.1869)  loss_scale: 65536.0000 (34719.5568)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4850 (5.4690)  time: 0.8023 (0.5979 -- 1.0114)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [18]  [380/483]  eta: 0:01:21  lr: 0.000018  min_lr: 0.000004  loss: 1.1134 (1.1825)  loss_scale: 32768.0000 (34617.1129)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3568 (5.4639)  time: 0.8162 (0.6254 -- 1.0274)  data: 0.0003 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 14:15:07,609] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 9086
[2025-06-27 14:15:07,609] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 14:15:07,609] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [18]  [400/483]  eta: 0:01:05  lr: 0.000018  min_lr: 0.000004  loss: 1.1666 (1.1809)  loss_scale: 32768.0000 (34157.1671)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6950 (5.4103)  time: 0.7963 (0.5822 -- 0.9895)  data: 0.0003 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [18]  [420/483]  eta: 0:00:49  lr: 0.000017  min_lr: 0.000004  loss: 1.1900 (1.1802)  loss_scale: 16384.0000 (33312.8361)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6637 (5.4172)  time: 0.8133 (0.5822 -- 1.0216)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [18]  [440/483]  eta: 0:00:33  lr: 0.000017  min_lr: 0.000004  loss: 1.1623 (1.1788)  loss_scale: 16384.0000 (32545.0884)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0229 (5.3810)  time: 0.8025 (0.6095 -- 1.0223)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [18]  [460/483]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.0829 (1.1758)  loss_scale: 16384.0000 (31843.9566)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8751 (5.3833)  time: 0.8086 (0.6454 -- 1.0242)  data: 0.0005 (0.0001 -- 0.0066)  max mem: 21487
Epoch: [18]  [480/483]  eta: 0:00:02  lr: 0.000017  min_lr: 0.000004  loss: 1.1241 (1.1733)  loss_scale: 16384.0000 (31201.1310)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5039 (5.4069)  time: 0.7961 (0.5684 -- 0.9950)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [18]  [482/483]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.1241 (1.1730)  loss_scale: 16384.0000 (31139.7764)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5039 (5.3971)  time: 0.7992 (0.5684 -- 0.9950)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [18] Total time: 0:06:22 (0.7918 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.1241 (1.1730)  loss_scale: 16384.0000 (31139.7764)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5039 (5.3971)
Val:  [ 0/23]  eta: 0:00:34  loss: 0.1348 (0.1348)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.4854 (1.4854 -- 1.4854)  data: 1.2303 (1.2303 -- 1.2303)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1531 (0.2132)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3708 (0.2426 -- 1.4854)  data: 0.1121 (0.0002 -- 1.2303)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1386 (0.2000)  acc1: 100.0000 (96.0317)  acc5: 100.0000 (99.6032)  time: 0.2593 (0.2172 -- 0.2757)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1386 (0.2045)  acc1: 100.0000 (95.9410)  acc5: 100.0000 (99.6310)  time: 0.2568 (0.1637 -- 0.2996)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:07 (0.3104 s / it)
* Acc@1 95.941 Acc@5 99.631 loss 0.204
Accuracy of the network on the 271 val images: 95.94%
Max accuracy: 97.42%
Epoch: [19]  [  0/483]  eta: 0:35:36  lr: 0.000017  min_lr: 0.000004  loss: 1.3559 (1.3559)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.3237 (8.3237)  time: 4.4232 (4.4232 -- 4.4232)  data: 3.6563 (3.6563 -- 3.6563)  max mem: 21487
Epoch: [19]  [ 20/483]  eta: 0:07:39  lr: 0.000017  min_lr: 0.000004  loss: 1.0631 (1.0985)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5195 (5.1539)  time: 0.8212 (0.6441 -- 1.0269)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
[2025-06-27 14:17:02,391] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:17:02,392] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [ 40/483]  eta: 0:06:39  lr: 0.000017  min_lr: 0.000004  loss: 1.3032 (1.1755)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6075 (5.2253)  time: 0.8086 (0.5654 -- 1.0423)  data: 0.0002 (0.0001 -- 0.0013)  max mem: 21487
Epoch: [19]  [ 60/483]  eta: 0:06:12  lr: 0.000017  min_lr: 0.000004  loss: 1.1926 (1.1820)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0524 (5.2038)  time: 0.8329 (0.6973 -- 1.0238)  data: 0.0006 (0.0001 -- 0.0090)  max mem: 21487
Epoch: [19]  [ 80/483]  eta: 0:05:44  lr: 0.000017  min_lr: 0.000004  loss: 1.0742 (1.1623)  loss_scale: 32768.0000 (25081.6790)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9894 (5.1081)  time: 0.7765 (0.6390 -- 1.0287)  data: 0.0008 (0.0001 -- 0.0100)  max mem: 21487
Epoch: [19]  [100/483]  eta: 0:05:27  lr: 0.000017  min_lr: 0.000004  loss: 1.2865 (1.1750)  loss_scale: 32768.0000 (26603.7228)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5845 (5.2103)  time: 0.8588 (0.6062 -- 1.0556)  data: 0.0002 (0.0001 -- 0.0013)  max mem: 21487
Epoch: [19]  [120/483]  eta: 0:05:07  lr: 0.000017  min_lr: 0.000004  loss: 1.1146 (1.1634)  loss_scale: 32768.0000 (27622.6116)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5306 (5.2500)  time: 0.8033 (0.6399 -- 1.0178)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [19]  [140/483]  eta: 0:04:49  lr: 0.000017  min_lr: 0.000004  loss: 1.2425 (1.1633)  loss_scale: 32768.0000 (28352.4539)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3757 (5.1939)  time: 0.8216 (0.6569 -- 0.9924)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [19]  [160/483]  eta: 0:04:31  lr: 0.000017  min_lr: 0.000004  loss: 1.0581 (1.1531)  loss_scale: 32768.0000 (28900.9689)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0263 (5.1349)  time: 0.8311 (0.6273 -- 1.0551)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 14:18:47,806] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:18:47,807] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [180/483]  eta: 0:04:13  lr: 0.000017  min_lr: 0.000004  loss: 1.0977 (1.1514)  loss_scale: 65536.0000 (32043.8453)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0856 (5.1647)  time: 0.7863 (0.5360 -- 1.0353)  data: 0.0012 (0.0001 -- 0.0115)  max mem: 21487
Epoch: [19]  [200/483]  eta: 0:03:56  lr: 0.000017  min_lr: 0.000004  loss: 1.1829 (1.1479)  loss_scale: 65536.0000 (35376.3980)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2727 (5.2106)  time: 0.8232 (0.5530 -- 1.0055)  data: 0.0003 (0.0001 -- 0.0019)  max mem: 21487
[2025-06-27 14:19:18,089] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 9381
[2025-06-27 14:19:18,089] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:19:18,089] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [220/483]  eta: 0:03:38  lr: 0.000017  min_lr: 0.000004  loss: 1.1940 (1.1541)  loss_scale: 32768.0000 (35585.1584)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4377 (5.2376)  time: 0.7952 (0.5826 -- 1.0219)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [19]  [240/483]  eta: 0:03:20  lr: 0.000016  min_lr: 0.000004  loss: 1.2000 (1.1552)  loss_scale: 32768.0000 (35351.3693)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0518 (5.2619)  time: 0.7543 (0.5467 -- 1.0018)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [19]  [260/483]  eta: 0:03:03  lr: 0.000016  min_lr: 0.000004  loss: 1.2218 (1.1592)  loss_scale: 32768.0000 (35153.4100)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3794 (5.2456)  time: 0.7911 (0.6223 -- 1.0016)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [19]  [280/483]  eta: 0:02:46  lr: 0.000016  min_lr: 0.000004  loss: 1.0889 (1.1612)  loss_scale: 32768.0000 (34983.6299)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9777 (5.2602)  time: 0.8197 (0.5971 -- 1.0057)  data: 0.0011 (0.0001 -- 0.0181)  max mem: 21487
Epoch: [19]  [300/483]  eta: 0:02:30  lr: 0.000016  min_lr: 0.000004  loss: 1.2237 (1.1673)  loss_scale: 32768.0000 (34836.4120)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6472 (5.3343)  time: 0.8192 (0.6212 -- 1.0481)  data: 0.0008 (0.0001 -- 0.0128)  max mem: 21487
Epoch: [19]  [320/483]  eta: 0:02:13  lr: 0.000016  min_lr: 0.000004  loss: 1.1955 (1.1713)  loss_scale: 32768.0000 (34707.5389)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2003 (5.3455)  time: 0.7816 (0.5625 -- 1.0068)  data: 0.0006 (0.0001 -- 0.0104)  max mem: 21487
[2025-06-27 14:20:59,651] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:20:59,651] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-06-27 14:21:04,116] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 9516
[2025-06-27 14:21:04,116] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:21:04,116] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [340/483]  eta: 0:01:56  lr: 0.000016  min_lr: 0.000004  loss: 1.0047 (1.1609)  loss_scale: 32768.0000 (35170.3460)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9780 (5.3465)  time: 0.7329 (0.4241 -- 0.9951)  data: 0.0011 (0.0001 -- 0.0121)  max mem: 21487
Epoch: [19]  [360/483]  eta: 0:01:39  lr: 0.000016  min_lr: 0.000004  loss: 1.2119 (1.1645)  loss_scale: 32768.0000 (35037.2521)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3491 (5.3878)  time: 0.7777 (0.5159 -- 1.0366)  data: 0.0006 (0.0001 -- 0.0077)  max mem: 21487
Epoch: [19]  [380/483]  eta: 0:01:23  lr: 0.000016  min_lr: 0.000004  loss: 1.0532 (1.1608)  loss_scale: 32768.0000 (34918.1312)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5962 (5.4012)  time: 0.8473 (0.6815 -- 1.0143)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [19]  [400/483]  eta: 0:01:07  lr: 0.000016  min_lr: 0.000004  loss: 1.0826 (1.1568)  loss_scale: 32768.0000 (34810.8928)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8519 (5.3724)  time: 0.8200 (0.6626 -- 1.0585)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 14:22:07,032] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 9595
[2025-06-27 14:22:07,032] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 14:22:07,032] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [420/483]  eta: 0:00:51  lr: 0.000016  min_lr: 0.000004  loss: 1.1403 (1.1543)  loss_scale: 32768.0000 (34597.0926)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6533 (5.3705)  time: 0.7341 (0.5873 -- 0.9948)  data: 0.0004 (0.0001 -- 0.0034)  max mem: 21487
Epoch: [19]  [440/483]  eta: 0:00:34  lr: 0.000016  min_lr: 0.000004  loss: 1.1233 (1.1537)  loss_scale: 16384.0000 (33771.1020)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3570 (5.3580)  time: 0.8393 (0.6783 -- 1.0550)  data: 0.0002 (0.0001 -- 0.0017)  max mem: 21487
Epoch: [19]  [460/483]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000004  loss: 1.2215 (1.1553)  loss_scale: 16384.0000 (33016.7809)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1295 (5.4369)  time: 0.8432 (0.6120 -- 1.0633)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [19]  [480/483]  eta: 0:00:02  lr: 0.000016  min_lr: 0.000004  loss: 1.1297 (1.1550)  loss_scale: 16384.0000 (32325.1892)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1636 (5.4356)  time: 0.8281 (0.5970 -- 1.0072)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [19]  [482/483]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.1674 (1.1558)  loss_scale: 16384.0000 (32259.1801)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2041 (5.4353)  time: 0.8191 (0.5970 -- 1.0072)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [19] Total time: 0:06:33 (0.8138 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.1674 (1.1558)  loss_scale: 16384.0000 (32259.1801)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2041 (5.4353)
[2025-06-27 14:23:00,514] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is begin to save!
[2025-06-27 14:23:00,522] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-19/mp_rank_00_model_states.pt
Val:  [ 0/23]  eta: 0:00:31  loss: 0.1397 (0.1397)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3777 (1.3777 -- 1.3777)  data: 1.1179 (1.1179 -- 1.1179)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1627 (0.2102)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3600 (0.2389 -- 1.3777)  data: 0.1018 (0.0002 -- 1.1179)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1425 (0.1974)  acc1: 100.0000 (95.6349)  acc5: 100.0000 (99.6032)  time: 0.2625 (0.2389 -- 0.2920)  data: 0.0002 (0.0002 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1425 (0.2025)  acc1: 100.0000 (95.5720)  acc5: 100.0000 (99.6310)  time: 0.2585 (0.1638 -- 0.2920)  data: 0.0002 (0.0002 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:07 (0.3065 s / it)
* Acc@1 95.572 Acc@5 99.631 loss 0.202
Accuracy of the network on the 271 val images: 95.57%
Max accuracy: 97.42%
Epoch: [20]  [  0/483]  eta: 0:30:17  lr: 0.000016  min_lr: 0.000004  loss: 1.6584 (1.6584)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9453 (5.9453)  time: 3.7636 (3.7636 -- 3.7636)  data: 3.0540 (3.0540 -- 3.0540)  max mem: 21487
Epoch: [20]  [ 20/483]  eta: 0:06:58  lr: 0.000016  min_lr: 0.000004  loss: 1.2044 (1.2274)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4089 (4.9175)  time: 0.7607 (0.6066 -- 0.9732)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [20]  [ 40/483]  eta: 0:06:24  lr: 0.000016  min_lr: 0.000004  loss: 1.2276 (1.2314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6265 (4.8057)  time: 0.8322 (0.5966 -- 1.0063)  data: 0.0004 (0.0001 -- 0.0061)  max mem: 21487
Epoch: [20]  [ 60/483]  eta: 0:05:53  lr: 0.000015  min_lr: 0.000004  loss: 1.1972 (1.2143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5698 (4.7089)  time: 0.7687 (0.6148 -- 1.0156)  data: 0.0005 (0.0001 -- 0.0067)  max mem: 21487
[2025-06-27 14:24:06,466] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:24:06,467] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [ 80/483]  eta: 0:05:35  lr: 0.000015  min_lr: 0.000004  loss: 1.1443 (1.1996)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7958 (4.7617)  time: 0.8188 (0.6321 -- 1.0126)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [20]  [100/483]  eta: 0:05:15  lr: 0.000015  min_lr: 0.000004  loss: 1.1928 (1.2026)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8974 (4.8980)  time: 0.7976 (0.5546 -- 1.0150)  data: 0.0004 (0.0001 -- 0.0066)  max mem: 21487
Epoch: [20]  [120/483]  eta: 0:04:55  lr: 0.000015  min_lr: 0.000004  loss: 1.0335 (1.1886)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7188 (5.0893)  time: 0.7561 (0.5591 -- 0.9363)  data: 0.0010 (0.0001 -- 0.0159)  max mem: 21487
Epoch: [20]  [140/483]  eta: 0:04:36  lr: 0.000015  min_lr: 0.000004  loss: 1.3078 (1.1984)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7060 (5.3448)  time: 0.7582 (0.5928 -- 1.0079)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [20]  [160/483]  eta: 0:04:20  lr: 0.000015  min_lr: 0.000004  loss: 1.1318 (1.1954)  loss_scale: 32768.0000 (26255.1056)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1198 (5.3738)  time: 0.8108 (0.6290 -- 0.9900)  data: 0.0013 (0.0001 -- 0.0129)  max mem: 21487
Epoch: [20]  [180/483]  eta: 0:04:04  lr: 0.000015  min_lr: 0.000004  loss: 1.2017 (1.1940)  loss_scale: 32768.0000 (26974.7624)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6803 (5.3163)  time: 0.8128 (0.6727 -- 1.0126)  data: 0.0006 (0.0001 -- 0.0095)  max mem: 21487
[2025-06-27 14:25:48,457] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:25:48,457] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [200/483]  eta: 0:03:48  lr: 0.000015  min_lr: 0.000004  loss: 1.0739 (1.1873)  loss_scale: 32768.0000 (29018.4279)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0984 (5.2646)  time: 0.8262 (0.6160 -- 1.0096)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 14:26:04,316] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 9871
[2025-06-27 14:26:04,317] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:26:04,317] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [220/483]  eta: 0:03:33  lr: 0.000015  min_lr: 0.000004  loss: 1.1628 (1.1873)  loss_scale: 32768.0000 (30840.4706)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8636 (5.2634)  time: 0.8359 (0.6255 -- 1.0373)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [20]  [240/483]  eta: 0:03:18  lr: 0.000015  min_lr: 0.000004  loss: 1.1855 (1.1907)  loss_scale: 32768.0000 (31000.4315)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7075 (5.2933)  time: 0.8535 (0.6595 -- 0.9865)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [20]  [260/483]  eta: 0:03:01  lr: 0.000015  min_lr: 0.000004  loss: 1.0596 (1.1840)  loss_scale: 32768.0000 (31135.8774)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2953 (5.2729)  time: 0.8200 (0.6363 -- 0.9914)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [20]  [280/483]  eta: 0:02:43  lr: 0.000015  min_lr: 0.000004  loss: 1.1788 (1.1834)  loss_scale: 32768.0000 (31252.0427)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5610 (5.2846)  time: 0.7071 (0.4592 -- 0.9534)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [20]  [300/483]  eta: 0:02:27  lr: 0.000015  min_lr: 0.000004  loss: 1.1321 (1.1812)  loss_scale: 32768.0000 (31352.7708)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7130 (5.2801)  time: 0.7943 (0.6114 -- 0.9962)  data: 0.0003 (0.0001 -- 0.0024)  max mem: 21487
Epoch: [20]  [320/483]  eta: 0:02:11  lr: 0.000015  min_lr: 0.000004  loss: 1.2504 (1.1848)  loss_scale: 32768.0000 (31440.9470)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2020 (5.2837)  time: 0.8202 (0.6456 -- 0.9968)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 14:27:47,342] [INFO] [logging.py:107:log_dist] [Rank 0] step=10000, skipped=51, lr=[np.float64(3.6842781581345535e-06), np.float64(3.6842781581345535e-06), np.float64(4.093642397927282e-06), np.float64(4.093642397927282e-06), np.float64(4.548491553252534e-06), np.float64(4.548491553252534e-06), np.float64(5.053879503613928e-06), np.float64(5.053879503613928e-06), np.float64(5.6154216706821416e-06), np.float64(5.6154216706821416e-06), np.float64(6.239357411869046e-06), np.float64(6.239357411869046e-06), np.float64(6.9326193465211624e-06), np.float64(6.9326193465211624e-06), np.float64(7.702910385023513e-06), np.float64(7.702910385023513e-06), np.float64(8.558789316692793e-06), np.float64(8.558789316692793e-06), np.float64(9.509765907436435e-06), np.float64(9.509765907436435e-06), np.float64(1.0566406563818263e-05), np.float64(1.0566406563818263e-05), np.float64(1.1740451737575846e-05), np.float64(1.1740451737575846e-05), np.float64(1.3044946375084273e-05), np.float64(1.3044946375084273e-05), np.float64(1.4494384861204748e-05), np.float64(1.4494384861204748e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 14:27:47,343] [INFO] [timer.py:264:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=12.189748496604143, CurrSamplesPerSec=10.81504362467044, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
[2025-06-27 14:27:48,280] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:27:48,280] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [340/483]  eta: 0:01:55  lr: 0.000014  min_lr: 0.000004  loss: 1.0797 (1.1794)  loss_scale: 32768.0000 (31614.8739)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7296 (5.2488)  time: 0.8343 (0.6747 -- 1.0241)  data: 0.0007 (0.0001 -- 0.0117)  max mem: 21487
Epoch: [20]  [360/483]  eta: 0:01:39  lr: 0.000014  min_lr: 0.000004  loss: 1.2028 (1.1822)  loss_scale: 65536.0000 (33494.1607)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5143 (5.2533)  time: 0.7858 (0.5726 -- 0.9613)  data: 0.0006 (0.0001 -- 0.0102)  max mem: 21487
Epoch: [20]  [380/483]  eta: 0:01:23  lr: 0.000014  min_lr: 0.000004  loss: 1.1990 (1.1807)  loss_scale: 65536.0000 (35176.1470)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8111 (5.2962)  time: 0.8378 (0.5969 -- 1.0592)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [20]  [400/483]  eta: 0:01:07  lr: 0.000014  min_lr: 0.000004  loss: 1.0922 (1.1785)  loss_scale: 65536.0000 (36690.3541)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7877 (5.3019)  time: 0.8253 (0.5963 -- 1.0487)  data: 0.0006 (0.0001 -- 0.0090)  max mem: 21487
Epoch: [20]  [420/483]  eta: 0:00:51  lr: 0.000014  min_lr: 0.000004  loss: 1.1555 (1.1801)  loss_scale: 65536.0000 (38060.6936)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3724 (5.3070)  time: 0.8000 (0.5545 -- 1.0247)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [20]  [440/483]  eta: 0:00:34  lr: 0.000014  min_lr: 0.000004  loss: 1.2396 (1.1805)  loss_scale: 65536.0000 (39306.7392)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2190 (5.3576)  time: 0.8088 (0.6401 -- 1.0104)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [20]  [460/483]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000004  loss: 1.1055 (1.1831)  loss_scale: 65536.0000 (40444.6681)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0887 (5.3867)  time: 0.7910 (0.5663 -- 0.9938)  data: 0.0005 (0.0001 -- 0.0050)  max mem: 21487
[2025-06-27 14:29:32,215] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:29:32,215] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [480/483]  eta: 0:00:02  lr: 0.000014  min_lr: 0.000004  loss: 1.1476 (1.1804)  loss_scale: 131072.0000 (43259.2100)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7461 (5.3618)  time: 0.8597 (0.6984 -- 1.0269)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [20]  [482/483]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 1.1494 (1.1800)  loss_scale: 131072.0000 (43622.8240)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1981 (5.3528)  time: 0.8521 (0.6343 -- 1.0269)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [20] Total time: 0:06:31 (0.8111 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 1.1494 (1.1800)  loss_scale: 131072.0000 (43622.8240)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1981 (5.3528)
Val:  [ 0/23]  eta: 0:00:32  loss: 0.1372 (0.1372)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3943 (1.3943 -- 1.3943)  data: 1.1239 (1.1239 -- 1.1239)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1563 (0.2197)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3591 (0.2408 -- 1.3943)  data: 0.1024 (0.0002 -- 1.1239)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1395 (0.2006)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.2604 (0.2390 -- 0.2900)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1395 (0.2047)  acc1: 100.0000 (96.6790)  acc5: 100.0000 (99.6310)  time: 0.2573 (0.1644 -- 0.2900)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:07 (0.3060 s / it)
* Acc@1 96.679 Acc@5 99.631 loss 0.205
Accuracy of the network on the 271 val images: 96.68%
Max accuracy: 97.42%
Epoch: [21]  [  0/483]  eta: 0:30:08  lr: 0.000014  min_lr: 0.000004  loss: 1.1448 (1.1448)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.6267 (7.6267)  time: 3.7446 (3.7446 -- 3.7446)  data: 2.7653 (2.7653 -- 2.7653)  max mem: 21487
[2025-06-27 14:29:55,526] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 10144
[2025-06-27 14:29:55,526] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 14:29:55,526] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 20/483]  eta: 0:07:13  lr: 0.000014  min_lr: 0.000004  loss: 1.1431 (1.0996)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3132 (5.7399)  time: 0.7964 (0.5583 -- 1.0105)  data: 0.0020 (0.0001 -- 0.0227)  max mem: 21487
[2025-06-27 14:30:20,623] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 10176
[2025-06-27 14:30:20,623] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:30:20,623] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 40/483]  eta: 0:06:09  lr: 0.000014  min_lr: 0.000004  loss: 1.2321 (1.1582)  loss_scale: 65536.0000 (60740.6829)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0469 (5.6674)  time: 0.7279 (0.4115 -- 0.9493)  data: 0.0003 (0.0001 -- 0.0013)  max mem: 21487
Epoch: [21]  [ 60/483]  eta: 0:05:56  lr: 0.000014  min_lr: 0.000004  loss: 1.2127 (1.1731)  loss_scale: 32768.0000 (51569.3115)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8797 (5.5918)  time: 0.8590 (0.6396 -- 1.0309)  data: 0.0005 (0.0001 -- 0.0072)  max mem: 21487
Epoch: [21]  [ 80/483]  eta: 0:05:39  lr: 0.000014  min_lr: 0.000003  loss: 1.0351 (1.1410)  loss_scale: 32768.0000 (46927.0123)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4730 (5.4681)  time: 0.8403 (0.6265 -- 1.0135)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [21]  [100/483]  eta: 0:05:19  lr: 0.000014  min_lr: 0.000003  loss: 1.0948 (1.1417)  loss_scale: 32768.0000 (44123.2475)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9550 (5.2572)  time: 0.8060 (0.6220 -- 1.0029)  data: 0.0008 (0.0001 -- 0.0088)  max mem: 21487
Epoch: [21]  [120/483]  eta: 0:04:59  lr: 0.000014  min_lr: 0.000003  loss: 1.2392 (1.1648)  loss_scale: 32768.0000 (42246.3471)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6890 (5.3480)  time: 0.7783 (0.5346 -- 1.0161)  data: 0.0003 (0.0001 -- 0.0020)  max mem: 21487
Epoch: [21]  [140/483]  eta: 0:04:42  lr: 0.000014  min_lr: 0.000003  loss: 1.1364 (1.1572)  loss_scale: 32768.0000 (40901.9007)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2515 (5.2863)  time: 0.8013 (0.6323 -- 1.0032)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [21]  [160/483]  eta: 0:04:25  lr: 0.000013  min_lr: 0.000003  loss: 1.2210 (1.1660)  loss_scale: 32768.0000 (39891.4783)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5557 (5.3385)  time: 0.8205 (0.6295 -- 1.0263)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 14:32:04,838] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:32:04,839] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [180/483]  eta: 0:04:07  lr: 0.000013  min_lr: 0.000003  loss: 1.0143 (1.1531)  loss_scale: 65536.0000 (42544.0884)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0392 (5.2674)  time: 0.7681 (0.5443 -- 1.0003)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [21]  [200/483]  eta: 0:03:50  lr: 0.000013  min_lr: 0.000003  loss: 1.1966 (1.1574)  loss_scale: 65536.0000 (44831.8408)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5798 (5.3106)  time: 0.8077 (0.6484 -- 1.0108)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [21]  [220/483]  eta: 0:03:34  lr: 0.000013  min_lr: 0.000003  loss: 1.2261 (1.1609)  loss_scale: 65536.0000 (46705.5204)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9168 (5.3463)  time: 0.8060 (0.6017 -- 0.9857)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [21]  [240/483]  eta: 0:03:17  lr: 0.000013  min_lr: 0.000003  loss: 1.1598 (1.1586)  loss_scale: 65536.0000 (48268.2158)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6493 (5.4009)  time: 0.8038 (0.5950 -- 1.0638)  data: 0.0005 (0.0001 -- 0.0071)  max mem: 21487
Epoch: [21]  [260/483]  eta: 0:03:01  lr: 0.000013  min_lr: 0.000003  loss: 1.0766 (1.1525)  loss_scale: 65536.0000 (49591.4176)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1499 (5.3711)  time: 0.8090 (0.5967 -- 0.9703)  data: 0.0003 (0.0001 -- 0.0033)  max mem: 21487
Epoch: [21]  [280/483]  eta: 0:02:45  lr: 0.000013  min_lr: 0.000003  loss: 1.2305 (1.1556)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6198 (5.4309)  time: 0.8215 (0.5157 -- 1.0150)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
[2025-06-27 14:33:47,971] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:33:47,971] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-06-27 14:33:49,343] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 10435
[2025-06-27 14:33:49,343] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 14:33:49,343] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [300/483]  eta: 0:02:28  lr: 0.000013  min_lr: 0.000003  loss: 1.1815 (1.1581)  loss_scale: 65536.0000 (52145.7542)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1815 (5.4685)  time: 0.7711 (0.5478 -- 1.0169)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [21]  [320/483]  eta: 0:02:12  lr: 0.000013  min_lr: 0.000003  loss: 1.2225 (1.1593)  loss_scale: 65536.0000 (52980.0374)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0937 (5.4998)  time: 0.8162 (0.6575 -- 0.9947)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [21]  [340/483]  eta: 0:01:55  lr: 0.000013  min_lr: 0.000003  loss: 1.2649 (1.1623)  loss_scale: 65536.0000 (53716.4575)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0298 (5.4986)  time: 0.7321 (0.4426 -- 0.9818)  data: 0.0005 (0.0001 -- 0.0039)  max mem: 21487
[2025-06-27 14:34:28,357] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 10486
[2025-06-27 14:34:28,357] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:34:28,357] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [360/483]  eta: 0:01:39  lr: 0.000013  min_lr: 0.000003  loss: 1.2327 (1.1618)  loss_scale: 32768.0000 (52737.4183)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5359 (5.4755)  time: 0.7908 (0.5693 -- 1.0066)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [21]  [380/483]  eta: 0:01:23  lr: 0.000013  min_lr: 0.000003  loss: 1.1350 (1.1600)  loss_scale: 32768.0000 (51689.1549)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9386 (5.4381)  time: 0.8546 (0.7095 -- 0.9919)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [21]  [400/483]  eta: 0:01:07  lr: 0.000013  min_lr: 0.000003  loss: 0.9835 (1.1557)  loss_scale: 32768.0000 (50745.4564)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.5784 (5.3793)  time: 0.8333 (0.6031 -- 1.0193)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [21]  [420/483]  eta: 0:00:51  lr: 0.000013  min_lr: 0.000003  loss: 1.0468 (1.1532)  loss_scale: 32768.0000 (49891.4204)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6567 (5.3605)  time: 0.8122 (0.6552 -- 1.0039)  data: 0.0003 (0.0001 -- 0.0030)  max mem: 21487
Epoch: [21]  [440/483]  eta: 0:00:34  lr: 0.000013  min_lr: 0.000003  loss: 1.1700 (1.1514)  loss_scale: 32768.0000 (49114.8481)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8947 (5.3618)  time: 0.8337 (0.5976 -- 1.0158)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [21]  [460/483]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000003  loss: 1.0514 (1.1479)  loss_scale: 32768.0000 (48405.6573)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6974 (5.3516)  time: 0.7715 (0.6101 -- 0.9723)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 14:36:13,595] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:36:13,596] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [480/483]  eta: 0:00:02  lr: 0.000012  min_lr: 0.000003  loss: 1.2018 (1.1491)  loss_scale: 32768.0000 (48368.5655)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0737 (5.3842)  time: 0.8131 (0.6190 -- 0.9701)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [21]  [482/483]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.2439 (1.1489)  loss_scale: 65536.0000 (48439.6522)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4410 (5.3811)  time: 0.8240 (0.6190 -- 0.9701)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [21] Total time: 0:06:31 (0.8098 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.2439 (1.1489)  loss_scale: 65536.0000 (48439.6522)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4410 (5.3811)
Val:  [ 0/23]  eta: 0:00:31  loss: 0.1458 (0.1458)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3710 (1.3710 -- 1.3710)  data: 1.1160 (1.1160 -- 1.1160)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1537 (0.2185)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3471 (0.1787 -- 1.3710)  data: 0.1016 (0.0002 -- 1.1160)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1394 (0.1978)  acc1: 100.0000 (97.2222)  acc5: 100.0000 (99.6032)  time: 0.2386 (0.1564 -- 0.2855)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1394 (0.2027)  acc1: 100.0000 (97.0480)  acc5: 100.0000 (99.6310)  time: 0.2367 (0.1564 -- 0.2855)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:06 (0.2866 s / it)
* Acc@1 97.048 Acc@5 99.631 loss 0.203
Accuracy of the network on the 271 val images: 97.05%
Max accuracy: 97.42%
Epoch: [22]  [  0/483]  eta: 0:34:31  lr: 0.000012  min_lr: 0.000003  loss: 1.3806 (1.3806)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6927 (3.6927)  time: 4.2879 (4.2879 -- 4.2879)  data: 3.3890 (3.3890 -- 3.3890)  max mem: 21487
Epoch: [22]  [ 20/483]  eta: 0:07:40  lr: 0.000012  min_lr: 0.000003  loss: 1.1905 (1.1760)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1644 (5.4593)  time: 0.8298 (0.6216 -- 1.0207)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 14:36:55,900] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 10654
[2025-06-27 14:36:55,901] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:36:55,901] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 40/483]  eta: 0:06:24  lr: 0.000012  min_lr: 0.000003  loss: 1.1076 (1.1583)  loss_scale: 32768.0000 (55146.1463)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6582 (5.4563)  time: 0.7368 (0.4427 -- 0.9899)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [22]  [ 60/483]  eta: 0:05:57  lr: 0.000012  min_lr: 0.000003  loss: 1.2407 (1.1739)  loss_scale: 32768.0000 (47809.0492)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3322 (5.6123)  time: 0.7996 (0.6247 -- 1.0385)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [22]  [ 80/483]  eta: 0:05:37  lr: 0.000012  min_lr: 0.000003  loss: 1.1679 (1.1699)  loss_scale: 32768.0000 (44095.2099)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3717 (5.5999)  time: 0.8155 (0.6400 -- 1.0380)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [22]  [100/483]  eta: 0:05:19  lr: 0.000012  min_lr: 0.000003  loss: 1.0458 (1.1657)  loss_scale: 32768.0000 (41852.1980)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0592 (5.5599)  time: 0.8163 (0.6500 -- 1.0085)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [22]  [120/483]  eta: 0:05:01  lr: 0.000012  min_lr: 0.000003  loss: 1.2424 (1.1741)  loss_scale: 32768.0000 (40350.6777)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6998 (5.4780)  time: 0.8204 (0.6218 -- 1.0295)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [22]  [140/483]  eta: 0:04:44  lr: 0.000012  min_lr: 0.000003  loss: 1.1961 (1.1747)  loss_scale: 32768.0000 (39275.1206)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2371 (5.4073)  time: 0.8200 (0.6694 -- 1.0333)  data: 0.0003 (0.0001 -- 0.0026)  max mem: 21487
[2025-06-27 14:38:40,371] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:38:40,371] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [160/483]  eta: 0:04:28  lr: 0.000012  min_lr: 0.000003  loss: 1.1516 (1.1655)  loss_scale: 32768.0000 (39280.8944)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3233 (5.2766)  time: 0.8445 (0.7005 -- 1.0058)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [22]  [180/483]  eta: 0:04:12  lr: 0.000012  min_lr: 0.000003  loss: 1.1391 (1.1639)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9333 (5.2895)  time: 0.8590 (0.6901 -- 1.0396)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [22]  [200/483]  eta: 0:03:55  lr: 0.000012  min_lr: 0.000003  loss: 1.1338 (1.1602)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1620 (5.4140)  time: 0.8138 (0.6275 -- 1.0509)  data: 0.0004 (0.0001 -- 0.0051)  max mem: 21487
Epoch: [22]  [220/483]  eta: 0:03:36  lr: 0.000012  min_lr: 0.000003  loss: 1.1947 (1.1594)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8937 (5.4070)  time: 0.7292 (0.4441 -- 0.9990)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [22]  [240/483]  eta: 0:03:19  lr: 0.000012  min_lr: 0.000003  loss: 1.1764 (1.1660)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0675 (5.5005)  time: 0.7802 (0.6069 -- 0.9907)  data: 0.0005 (0.0001 -- 0.0071)  max mem: 21487
Epoch: [22]  [260/483]  eta: 0:03:02  lr: 0.000012  min_lr: 0.000003  loss: 1.1220 (1.1635)  loss_scale: 65536.0000 (49340.3218)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2509 (5.4445)  time: 0.8072 (0.6446 -- 1.0090)  data: 0.0008 (0.0001 -- 0.0098)  max mem: 21487
Epoch: [22]  [280/483]  eta: 0:02:46  lr: 0.000011  min_lr: 0.000003  loss: 1.0877 (1.1600)  loss_scale: 65536.0000 (50493.0391)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7329 (5.4476)  time: 0.8220 (0.5671 -- 1.0287)  data: 0.0005 (0.0001 -- 0.0066)  max mem: 21487
[2025-06-27 14:40:22,960] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:40:22,961] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-06-27 14:40:26,040] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 10915
[2025-06-27 14:40:26,040] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 14:40:26,040] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [300/483]  eta: 0:02:29  lr: 0.000011  min_lr: 0.000003  loss: 1.1989 (1.1624)  loss_scale: 65536.0000 (52363.4817)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0796 (5.4453)  time: 0.7972 (0.5774 -- 1.0043)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [22]  [320/483]  eta: 0:02:13  lr: 0.000011  min_lr: 0.000003  loss: 1.1670 (1.1621)  loss_scale: 65536.0000 (53184.1994)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2483 (5.4808)  time: 0.8051 (0.5757 -- 1.0361)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 14:40:53,332] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 10949
[2025-06-27 14:40:53,332] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:40:53,332] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [340/483]  eta: 0:01:56  lr: 0.000011  min_lr: 0.000003  loss: 1.1576 (1.1617)  loss_scale: 32768.0000 (52178.9560)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4029 (5.4699)  time: 0.8345 (0.6006 -- 0.9895)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [22]  [360/483]  eta: 0:01:40  lr: 0.000011  min_lr: 0.000003  loss: 1.1912 (1.1622)  loss_scale: 32768.0000 (51103.5568)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7606 (5.4683)  time: 0.8358 (0.6175 -- 1.0167)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 14:41:34,859] [INFO] [logging.py:107:log_dist] [Rank 0] step=11000, skipped=58, lr=[np.float64(2.8398874374387937e-06), np.float64(2.8398874374387937e-06), np.float64(3.1554304860431044e-06), np.float64(3.1554304860431044e-06), np.float64(3.5060338733812266e-06), np.float64(3.5060338733812266e-06), np.float64(3.895593192645807e-06), np.float64(3.895593192645807e-06), np.float64(4.328436880717564e-06), np.float64(4.328436880717564e-06), np.float64(4.809374311908404e-06), np.float64(4.809374311908404e-06), np.float64(5.343749235453782e-06), np.float64(5.343749235453782e-06), np.float64(5.9374991505042014e-06), np.float64(5.9374991505042014e-06), np.float64(6.597221278338002e-06), np.float64(6.597221278338002e-06), np.float64(7.330245864820002e-06), np.float64(7.330245864820002e-06), np.float64(8.14471762757778e-06), np.float64(8.14471762757778e-06), np.float64(9.0496862528642e-06), np.float64(9.0496862528642e-06), np.float64(1.0055206947626888e-05), np.float64(1.0055206947626888e-05), np.float64(1.1172452164029876e-05), np.float64(1.1172452164029876e-05)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 14:41:34,860] [INFO] [timer.py:264:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=11.956819235936738, CurrSamplesPerSec=11.699804085248848, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [22]  [380/483]  eta: 0:01:24  lr: 0.000011  min_lr: 0.000003  loss: 1.2685 (1.1636)  loss_scale: 32768.0000 (50141.0604)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3934 (5.4621)  time: 0.7674 (0.6060 -- 1.0054)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [22]  [400/483]  eta: 0:01:07  lr: 0.000011  min_lr: 0.000003  loss: 1.2530 (1.1655)  loss_scale: 32768.0000 (49274.5736)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4040 (5.4272)  time: 0.8295 (0.6831 -- 0.9805)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [22]  [420/483]  eta: 0:00:51  lr: 0.000011  min_lr: 0.000003  loss: 1.2119 (1.1664)  loss_scale: 32768.0000 (48490.4133)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2654 (5.4354)  time: 0.7880 (0.5703 -- 1.0366)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [22]  [440/483]  eta: 0:00:35  lr: 0.000011  min_lr: 0.000003  loss: 1.1198 (1.1642)  loss_scale: 32768.0000 (47777.3787)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0525 (5.3966)  time: 0.8229 (0.5692 -- 1.0320)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 14:42:38,243] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:42:38,243] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [460/483]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.1495 (1.1621)  loss_scale: 32768.0000 (47765.9349)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4708 (5.3853)  time: 0.7889 (0.4697 -- 0.9931)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [22]  [480/483]  eta: 0:00:02  lr: 0.000011  min_lr: 0.000003  loss: 1.1643 (1.1620)  loss_scale: 65536.0000 (48504.8150)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8815 (5.3868)  time: 0.7948 (0.6942 -- 0.9781)  data: 0.0009 (0.0001 -- 0.0156)  max mem: 21487
Epoch: [22]  [482/483]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.1643 (1.1613)  loss_scale: 65536.0000 (48575.3375)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7736 (5.3852)  time: 0.8142 (0.6942 -- 0.9861)  data: 0.0009 (0.0001 -- 0.0156)  max mem: 21487
Epoch: [22] Total time: 0:06:33 (0.8143 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.1643 (1.1613)  loss_scale: 65536.0000 (48575.3375)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7736 (5.3852)
Val:  [ 0/23]  eta: 0:00:31  loss: 0.1631 (0.1631)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3793 (1.3793 -- 1.3793)  data: 1.1207 (1.1207 -- 1.1207)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1631 (0.2109)  acc1: 100.0000 (96.9697)  acc5: 100.0000 (99.2424)  time: 0.3602 (0.2305 -- 1.3793)  data: 0.1021 (0.0002 -- 1.1207)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1441 (0.1976)  acc1: 100.0000 (97.2222)  acc5: 100.0000 (99.6032)  time: 0.2631 (0.2305 -- 0.2869)  data: 0.0003 (0.0002 -- 0.0005)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1441 (0.2030)  acc1: 100.0000 (97.0480)  acc5: 100.0000 (99.6310)  time: 0.2593 (0.1685 -- 0.2869)  data: 0.0003 (0.0002 -- 0.0005)  max mem: 21487
Val: Total time: 0:00:07 (0.3080 s / it)
* Acc@1 97.048 Acc@5 99.631 loss 0.203
Accuracy of the network on the 271 val images: 97.05%
Max accuracy: 97.42%
[2025-06-27 14:43:13,094] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 11109
[2025-06-27 14:43:13,095] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:43:13,095] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [  0/483]  eta: 0:29:32  lr: 0.000011  min_lr: 0.000003  loss: 1.6416 (1.6416)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9063 (3.9063)  time: 3.6698 (3.6698 -- 3.6698)  data: 3.0130 (3.0130 -- 3.0130)  max mem: 21487
Epoch: [23]  [ 20/483]  eta: 0:07:25  lr: 0.000011  min_lr: 0.000003  loss: 1.1201 (1.1476)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6692 (5.3151)  time: 0.8265 (0.5932 -- 1.0134)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [23]  [ 40/483]  eta: 0:06:37  lr: 0.000011  min_lr: 0.000003  loss: 1.2281 (1.1828)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8053 (5.6588)  time: 0.8286 (0.6267 -- 1.0154)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [23]  [ 60/483]  eta: 0:06:03  lr: 0.000011  min_lr: 0.000003  loss: 1.0904 (1.1546)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9281 (5.4642)  time: 0.7799 (0.5970 -- 0.9733)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [23]  [ 80/483]  eta: 0:05:39  lr: 0.000011  min_lr: 0.000003  loss: 1.0382 (1.1346)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0121 (5.2660)  time: 0.7974 (0.6299 -- 1.0053)  data: 0.0003 (0.0001 -- 0.0012)  max mem: 21487
Epoch: [23]  [100/483]  eta: 0:05:18  lr: 0.000010  min_lr: 0.000003  loss: 1.2492 (1.1559)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1289 (5.3487)  time: 0.7837 (0.6349 -- 0.9685)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [23]  [120/483]  eta: 0:04:57  lr: 0.000010  min_lr: 0.000003  loss: 1.1638 (1.1507)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3480 (5.5371)  time: 0.7572 (0.5642 -- 0.9559)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 14:44:55,705] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:44:55,706] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [140/483]  eta: 0:04:40  lr: 0.000010  min_lr: 0.000003  loss: 1.0300 (1.1409)  loss_scale: 65536.0000 (35556.7660)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0493 (5.4362)  time: 0.8097 (0.6846 -- 1.0409)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 14:45:11,101] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 11257
[2025-06-27 14:45:11,101] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:45:11,102] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [160/483]  eta: 0:04:24  lr: 0.000010  min_lr: 0.000003  loss: 1.0953 (1.1418)  loss_scale: 32768.0000 (36635.0311)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6010 (5.3523)  time: 0.8149 (0.5876 -- 0.9714)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [23]  [180/483]  eta: 0:04:07  lr: 0.000010  min_lr: 0.000003  loss: 1.1690 (1.1408)  loss_scale: 32768.0000 (36207.7348)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8035 (5.2752)  time: 0.8043 (0.6262 -- 1.0001)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [23]  [200/483]  eta: 0:03:50  lr: 0.000010  min_lr: 0.000003  loss: 1.1945 (1.1408)  loss_scale: 32768.0000 (35865.4726)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7044 (5.3094)  time: 0.8116 (0.6313 -- 1.0367)  data: 0.0003 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [23]  [220/483]  eta: 0:03:35  lr: 0.000010  min_lr: 0.000003  loss: 1.1825 (1.1462)  loss_scale: 32768.0000 (35585.1584)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7432 (5.3017)  time: 0.8470 (0.6005 -- 1.0319)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [23]  [240/483]  eta: 0:03:19  lr: 0.000010  min_lr: 0.000003  loss: 1.1545 (1.1515)  loss_scale: 32768.0000 (35351.3693)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3006 (5.3481)  time: 0.8290 (0.6204 -- 1.0073)  data: 0.0003 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [23]  [260/483]  eta: 0:03:02  lr: 0.000010  min_lr: 0.000003  loss: 1.1973 (1.1514)  loss_scale: 32768.0000 (35153.4100)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3442 (5.3571)  time: 0.8129 (0.6703 -- 1.0079)  data: 0.0003 (0.0001 -- 0.0010)  max mem: 21487
[2025-06-27 14:46:56,841] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:46:56,841] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [280/483]  eta: 0:02:46  lr: 0.000010  min_lr: 0.000003  loss: 1.1923 (1.1514)  loss_scale: 32768.0000 (35450.0783)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4194 (5.3764)  time: 0.8137 (0.6425 -- 0.9983)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [23]  [300/483]  eta: 0:02:29  lr: 0.000010  min_lr: 0.000003  loss: 1.2728 (1.1563)  loss_scale: 65536.0000 (37449.1429)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1232 (5.4011)  time: 0.7914 (0.6032 -- 0.9912)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [23]  [320/483]  eta: 0:02:12  lr: 0.000010  min_lr: 0.000002  loss: 1.2074 (1.1606)  loss_scale: 65536.0000 (39199.1028)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1905 (5.4035)  time: 0.7743 (0.6008 -- 1.0299)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
[2025-06-27 14:47:33,437] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 11433
[2025-06-27 14:47:33,437] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:47:33,437] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-06-27 14:47:34,676] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 11435
[2025-06-27 14:47:34,677] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 14:47:34,677] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [340/483]  eta: 0:01:55  lr: 0.000010  min_lr: 0.000002  loss: 1.1068 (1.1592)  loss_scale: 16384.0000 (38389.4897)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6358 (5.4301)  time: 0.7264 (0.3834 -- 0.9282)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [23]  [360/483]  eta: 0:01:39  lr: 0.000010  min_lr: 0.000002  loss: 1.1921 (1.1590)  loss_scale: 16384.0000 (37170.3490)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.3957 (5.3590)  time: 0.8009 (0.6467 -- 0.9509)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [23]  [380/483]  eta: 0:01:23  lr: 0.000010  min_lr: 0.000002  loss: 1.2026 (1.1601)  loss_scale: 16384.0000 (36079.2021)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8524 (5.3880)  time: 0.7879 (0.6123 -- 1.0016)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [23]  [400/483]  eta: 0:01:07  lr: 0.000010  min_lr: 0.000002  loss: 1.2175 (1.1620)  loss_scale: 16384.0000 (35096.8978)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0868 (5.3519)  time: 0.8542 (0.6087 -- 1.0121)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [23]  [420/483]  eta: 0:00:51  lr: 0.000009  min_lr: 0.000002  loss: 1.1445 (1.1611)  loss_scale: 16384.0000 (34207.9240)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1869 (5.3571)  time: 0.8316 (0.5522 -- 1.0579)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [23]  [440/483]  eta: 0:00:34  lr: 0.000009  min_lr: 0.000002  loss: 1.2333 (1.1619)  loss_scale: 16384.0000 (33399.5828)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5620 (5.3639)  time: 0.4857 (0.2811 -- 0.6074)  data: 0.0010 (0.0001 -- 0.0104)  max mem: 21487
[2025-06-27 14:49:08,632] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:49:08,632] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [460/483]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000002  loss: 1.0953 (1.1596)  loss_scale: 16384.0000 (32874.6204)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1719 (5.4134)  time: 0.5360 (0.4573 -- 0.6024)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [23]  [480/483]  eta: 0:00:02  lr: 0.000009  min_lr: 0.000002  loss: 0.9967 (1.1548)  loss_scale: 32768.0000 (32870.1871)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2793 (5.3886)  time: 0.5506 (0.4163 -- 0.6081)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [23]  [482/483]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.0939 (1.1547)  loss_scale: 32768.0000 (32869.7640)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8465 (5.3881)  time: 0.5560 (0.4163 -- 0.6081)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [23] Total time: 0:06:14 (0.7744 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.0939 (1.1547)  loss_scale: 32768.0000 (32869.7640)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8465 (5.3881)
Val:  [ 0/23]  eta: 0:00:34  loss: 0.1730 (0.1730)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.4898 (1.4898 -- 1.4898)  data: 1.2886 (1.2886 -- 1.2886)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1730 (0.2119)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3167 (0.1928 -- 1.4898)  data: 0.1174 (0.0002 -- 1.2886)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1461 (0.2010)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.1967 (0.1419 -- 0.2075)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1461 (0.2066)  acc1: 100.0000 (96.6790)  acc5: 100.0000 (99.6310)  time: 0.1925 (0.1196 -- 0.2075)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Val: Total time: 0:00:05 (0.2497 s / it)
* Acc@1 96.679 Acc@5 99.631 loss 0.207
Accuracy of the network on the 271 val images: 96.68%
Max accuracy: 97.42%
Epoch: [24]  [  0/483]  eta: 0:33:41  lr: 0.000009  min_lr: 0.000002  loss: 1.2615 (1.2615)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7508 (4.7508)  time: 4.1862 (4.1862 -- 4.1862)  data: 3.7284 (3.7284 -- 3.7284)  max mem: 21487
Epoch: [24]  [ 20/483]  eta: 0:05:20  lr: 0.000009  min_lr: 0.000002  loss: 1.1342 (1.1269)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1839 (5.2235)  time: 0.5175 (0.4053 -- 0.7035)  data: 0.0064 (0.0001 -- 0.1251)  max mem: 21487
Epoch: [24]  [ 40/483]  eta: 0:04:24  lr: 0.000009  min_lr: 0.000002  loss: 1.1753 (1.1505)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7120 (5.3657)  time: 0.4967 (0.3970 -- 0.5860)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [24]  [ 60/483]  eta: 0:04:01  lr: 0.000009  min_lr: 0.000002  loss: 1.2408 (1.1638)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8346 (5.4218)  time: 0.5204 (0.4365 -- 0.5939)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [24]  [ 80/483]  eta: 0:03:45  lr: 0.000009  min_lr: 0.000002  loss: 1.0417 (1.1447)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9699 (5.3818)  time: 0.5211 (0.4377 -- 0.5946)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 14:50:24,794] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:50:24,794] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [100/483]  eta: 0:03:30  lr: 0.000009  min_lr: 0.000002  loss: 1.1588 (1.1434)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9358 (5.3085)  time: 0.5131 (0.4454 -- 0.5676)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [24]  [120/483]  eta: 0:03:16  lr: 0.000009  min_lr: 0.000002  loss: 1.2499 (1.1525)  loss_scale: 65536.0000 (38455.0083)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5474 (5.2393)  time: 0.5020 (0.4509 -- 0.5404)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [24]  [140/483]  eta: 0:03:03  lr: 0.000009  min_lr: 0.000002  loss: 1.0015 (1.1355)  loss_scale: 65536.0000 (42296.2837)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2937 (5.0756)  time: 0.4980 (0.4005 -- 0.5837)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [24]  [160/483]  eta: 0:02:52  lr: 0.000009  min_lr: 0.000002  loss: 1.1503 (1.1309)  loss_scale: 65536.0000 (45183.2050)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1903 (5.0935)  time: 0.5200 (0.4572 -- 0.5861)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [24]  [180/483]  eta: 0:02:40  lr: 0.000009  min_lr: 0.000002  loss: 1.2293 (1.1365)  loss_scale: 65536.0000 (47432.1326)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2973 (5.1462)  time: 0.5038 (0.4195 -- 0.5947)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [24]  [200/483]  eta: 0:02:29  lr: 0.000009  min_lr: 0.000002  loss: 1.1749 (1.1401)  loss_scale: 65536.0000 (49233.5124)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2808 (5.1458)  time: 0.4985 (0.4043 -- 0.5847)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [24]  [220/483]  eta: 0:02:18  lr: 0.000009  min_lr: 0.000002  loss: 1.1234 (1.1413)  loss_scale: 65536.0000 (50708.8507)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0310 (5.1580)  time: 0.4984 (0.4022 -- 0.5982)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 14:51:29,338] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:51:29,338] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-06-27 14:51:34,516] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 11830
[2025-06-27 14:51:34,516] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 14:51:34,516] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [240/483]  eta: 0:02:07  lr: 0.000009  min_lr: 0.000002  loss: 1.2037 (1.1425)  loss_scale: 65536.0000 (54658.6556)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4951 (5.2067)  time: 0.5118 (0.3986 -- 0.5826)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 14:51:44,620] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 11850
[2025-06-27 14:51:44,620] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:51:44,620] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [260/483]  eta: 0:01:56  lr: 0.000008  min_lr: 0.000002  loss: 1.1894 (1.1479)  loss_scale: 65536.0000 (55115.5249)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8304 (5.2710)  time: 0.5108 (0.3309 -- 0.5988)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [24]  [280/483]  eta: 0:01:45  lr: 0.000008  min_lr: 0.000002  loss: 1.0608 (1.1445)  loss_scale: 32768.0000 (53524.9537)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8046 (5.2851)  time: 0.5014 (0.4397 -- 0.5868)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [24]  [300/483]  eta: 0:01:35  lr: 0.000008  min_lr: 0.000002  loss: 1.2552 (1.1457)  loss_scale: 32768.0000 (52145.7542)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0224 (5.5095)  time: 0.5031 (0.4588 -- 0.5857)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [24]  [320/483]  eta: 0:01:24  lr: 0.000008  min_lr: 0.000002  loss: 1.1421 (1.1468)  loss_scale: 32768.0000 (50938.4174)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4317 (5.5123)  time: 0.5077 (0.4406 -- 0.5894)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [24]  [340/483]  eta: 0:01:14  lr: 0.000008  min_lr: 0.000002  loss: 1.0330 (1.1431)  loss_scale: 32768.0000 (49872.7038)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.5644 (5.4076)  time: 0.5055 (0.4061 -- 0.5784)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [24]  [360/483]  eta: 0:01:03  lr: 0.000008  min_lr: 0.000002  loss: 1.1384 (1.1442)  loss_scale: 32768.0000 (48925.0748)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5246 (5.3556)  time: 0.4938 (0.4127 -- 0.5579)  data: 0.0004 (0.0001 -- 0.0045)  max mem: 21487
Epoch: [24]  [380/483]  eta: 0:00:53  lr: 0.000008  min_lr: 0.000002  loss: 1.0817 (1.1414)  loss_scale: 32768.0000 (48076.9344)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1677 (5.3651)  time: 0.5047 (0.4485 -- 0.5849)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 14:52:49,564] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:52:49,565] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-06-27 14:52:50,052] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 11980
[2025-06-27 14:52:50,052] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:52:50,052] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [400/483]  eta: 0:00:42  lr: 0.000008  min_lr: 0.000002  loss: 1.0188 (1.1393)  loss_scale: 32768.0000 (47395.1122)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4477 (5.3456)  time: 0.5026 (0.4389 -- 0.5843)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 14:52:59,499] [INFO] [logging.py:107:log_dist] [Rank 0] step=12000, skipped=65, lr=[np.float64(2.048676143603239e-06), np.float64(2.048676143603239e-06), np.float64(2.2763068262258212e-06), np.float64(2.2763068262258212e-06), np.float64(2.5292298069175787e-06), np.float64(2.5292298069175787e-06), np.float64(2.8102553410195323e-06), np.float64(2.8102553410195323e-06), np.float64(3.1225059344661467e-06), np.float64(3.1225059344661467e-06), np.float64(3.4694510382957187e-06), np.float64(3.4694510382957187e-06), np.float64(3.854945598106354e-06), np.float64(3.854945598106354e-06), np.float64(4.283272886784837e-06), np.float64(4.283272886784837e-06), np.float64(4.759192096427597e-06), np.float64(4.759192096427597e-06), np.float64(5.287991218252885e-06), np.float64(5.287991218252885e-06), np.float64(5.875545798058762e-06), np.float64(5.875545798058762e-06), np.float64(6.528384220065291e-06), np.float64(6.528384220065291e-06), np.float64(7.253760244516989e-06), np.float64(7.253760244516989e-06), np.float64(8.059733605018877e-06), np.float64(8.059733605018877e-06)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 14:52:59,502] [INFO] [timer.py:264:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=11.977856064583909, CurrSamplesPerSec=14.700709897372912, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
[2025-06-27 14:53:02,491] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 12005
[2025-06-27 14:53:02,491] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 14:53:02,491] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [420/483]  eta: 0:00:32  lr: 0.000008  min_lr: 0.000002  loss: 1.1264 (1.1412)  loss_scale: 32768.0000 (46388.9026)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1215 (5.3309)  time: 0.4972 (0.4003 -- 0.5768)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [24]  [440/483]  eta: 0:00:22  lr: 0.000008  min_lr: 0.000002  loss: 1.0723 (1.1419)  loss_scale: 16384.0000 (45028.1361)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6480 (5.3480)  time: 0.4948 (0.3956 -- 0.5440)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [24]  [460/483]  eta: 0:00:11  lr: 0.000008  min_lr: 0.000002  loss: 1.1826 (1.1433)  loss_scale: 16384.0000 (43785.4403)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7819 (5.3715)  time: 0.4908 (0.4411 -- 0.5771)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [24]  [480/483]  eta: 0:00:01  lr: 0.000008  min_lr: 0.000002  loss: 1.2070 (1.1451)  loss_scale: 16384.0000 (42646.0873)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7505 (5.3697)  time: 0.4980 (0.4940 -- 0.5279)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [24]  [482/483]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.2173 (1.1461)  loss_scale: 16384.0000 (42537.3416)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7103 (5.3647)  time: 0.4964 (0.4940 -- 0.5034)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [24] Total time: 0:04:07 (0.5123 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.2173 (1.1461)  loss_scale: 16384.0000 (42537.3416)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7103 (5.3647)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.1675 (0.1675)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3259 (1.3259 -- 1.3259)  data: 1.1175 (1.1175 -- 1.1175)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1609 (0.2137)  acc1: 100.0000 (96.9697)  acc5: 100.0000 (99.2424)  time: 0.2976 (0.1841 -- 1.3259)  data: 0.1018 (0.0001 -- 1.1175)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1482 (0.2006)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.1974 (0.1841 -- 0.2092)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1482 (0.2053)  acc1: 100.0000 (96.6790)  acc5: 100.0000 (99.6310)  time: 0.1927 (0.1264 -- 0.2069)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Val: Total time: 0:00:05 (0.2433 s / it)
* Acc@1 96.679 Acc@5 99.631 loss 0.205
Accuracy of the network on the 271 val images: 96.68%
Max accuracy: 97.42%
Epoch: [25]  [  0/483]  eta: 0:35:22  lr: 0.000008  min_lr: 0.000002  loss: 1.5894 (1.5894)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9983 (3.9983)  time: 4.3942 (4.3942 -- 4.3942)  data: 3.8799 (3.8799 -- 3.8799)  max mem: 21487
Epoch: [25]  [ 20/483]  eta: 0:05:25  lr: 0.000008  min_lr: 0.000002  loss: 0.9831 (1.0741)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6439 (5.5577)  time: 0.5192 (0.4146 -- 0.6025)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [25]  [ 40/483]  eta: 0:04:28  lr: 0.000008  min_lr: 0.000002  loss: 1.1309 (1.1233)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0438 (5.5063)  time: 0.5021 (0.4058 -- 0.5710)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 14:54:17,326] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:54:17,326] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 60/483]  eta: 0:04:06  lr: 0.000008  min_lr: 0.000002  loss: 1.0614 (1.1088)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8204 (5.5606)  time: 0.5348 (0.4508 -- 0.5991)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [25]  [ 80/483]  eta: 0:03:50  lr: 0.000008  min_lr: 0.000002  loss: 1.0753 (1.1016)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6154 (5.4814)  time: 0.5428 (0.4633 -- 0.5980)  data: 0.0002 (0.0001 -- 0.0014)  max mem: 21487
Epoch: [25]  [100/483]  eta: 0:03:35  lr: 0.000008  min_lr: 0.000002  loss: 1.1552 (1.1151)  loss_scale: 32768.0000 (23197.1485)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9022 (5.3968)  time: 0.5293 (0.4129 -- 0.6051)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [25]  [120/483]  eta: 0:03:22  lr: 0.000007  min_lr: 0.000002  loss: 1.0827 (1.1113)  loss_scale: 32768.0000 (24779.1074)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7539 (5.3626)  time: 0.5227 (0.4460 -- 0.5936)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [25]  [140/483]  eta: 0:03:09  lr: 0.000007  min_lr: 0.000002  loss: 1.0804 (1.1091)  loss_scale: 32768.0000 (25912.2837)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6814 (5.3419)  time: 0.5158 (0.3568 -- 0.5934)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [25]  [160/483]  eta: 0:02:56  lr: 0.000007  min_lr: 0.000002  loss: 1.1598 (1.1186)  loss_scale: 32768.0000 (26763.9255)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4969 (5.4279)  time: 0.5049 (0.3726 -- 0.6068)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [25]  [180/483]  eta: 0:02:43  lr: 0.000007  min_lr: 0.000002  loss: 1.0871 (1.1226)  loss_scale: 32768.0000 (27427.3591)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3828 (5.4189)  time: 0.5009 (0.4095 -- 0.5862)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 14:55:23,666] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:55:23,667] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [200/483]  eta: 0:02:32  lr: 0.000007  min_lr: 0.000002  loss: 1.1534 (1.1222)  loss_scale: 65536.0000 (30241.1144)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3329 (5.4703)  time: 0.5065 (0.3944 -- 0.5729)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 14:55:32,902] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 12280
[2025-06-27 14:55:32,902] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:55:32,902] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [220/483]  eta: 0:02:21  lr: 0.000007  min_lr: 0.000002  loss: 1.1197 (1.1269)  loss_scale: 32768.0000 (31062.8778)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4146 (5.4715)  time: 0.5323 (0.4569 -- 0.6174)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [25]  [240/483]  eta: 0:02:10  lr: 0.000007  min_lr: 0.000002  loss: 1.1106 (1.1314)  loss_scale: 32768.0000 (31204.3817)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8707 (5.3936)  time: 0.5379 (0.4169 -- 0.6142)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [25]  [260/483]  eta: 0:01:59  lr: 0.000007  min_lr: 0.000002  loss: 1.1403 (1.1299)  loss_scale: 32768.0000 (31324.1992)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9936 (5.4466)  time: 0.5113 (0.4120 -- 0.5964)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [25]  [280/483]  eta: 0:01:48  lr: 0.000007  min_lr: 0.000002  loss: 1.1550 (1.1333)  loss_scale: 32768.0000 (31426.9609)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9373 (5.4471)  time: 0.5467 (0.4995 -- 0.5921)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [25]  [300/483]  eta: 0:01:38  lr: 0.000007  min_lr: 0.000002  loss: 1.1545 (1.1344)  loss_scale: 32768.0000 (31516.0664)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0568 (5.4576)  time: 0.5404 (0.4466 -- 0.6140)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [25]  [320/483]  eta: 0:01:27  lr: 0.000007  min_lr: 0.000002  loss: 1.2444 (1.1368)  loss_scale: 32768.0000 (31594.0685)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6949 (5.4450)  time: 0.5895 (0.4626 -- 0.7005)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
[2025-06-27 14:56:42,659] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:56:42,660] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [340/483]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000002  loss: 1.0597 (1.1330)  loss_scale: 32768.0000 (32335.5777)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0981 (5.3903)  time: 0.5152 (0.4149 -- 0.6053)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 14:56:49,375] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 12422
[2025-06-27 14:56:49,375] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 14:56:49,375] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [360/483]  eta: 0:01:05  lr: 0.000007  min_lr: 0.000002  loss: 1.0665 (1.1326)  loss_scale: 32768.0000 (32904.1551)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0277 (5.3918)  time: 0.5131 (0.4384 -- 0.6046)  data: 0.0002 (0.0001 -- 0.0017)  max mem: 21487
Epoch: [25]  [380/483]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000002  loss: 0.9993 (1.1282)  loss_scale: 32768.0000 (32897.0079)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9267 (5.3525)  time: 0.5251 (0.4663 -- 0.6044)  data: 0.0004 (0.0001 -- 0.0056)  max mem: 21487
Epoch: [25]  [400/483]  eta: 0:00:44  lr: 0.000007  min_lr: 0.000002  loss: 1.2175 (1.1296)  loss_scale: 32768.0000 (32890.5736)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3922 (5.4115)  time: 0.5319 (0.4597 -- 0.5956)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [25]  [420/483]  eta: 0:00:33  lr: 0.000007  min_lr: 0.000002  loss: 1.0945 (1.1307)  loss_scale: 32768.0000 (32884.7506)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8181 (5.4027)  time: 0.5355 (0.4657 -- 0.6011)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [25]  [440/483]  eta: 0:00:23  lr: 0.000007  min_lr: 0.000002  loss: 1.1829 (1.1332)  loss_scale: 32768.0000 (32879.4558)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7773 (5.3962)  time: 0.5224 (0.4176 -- 0.5930)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 14:57:43,449] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 12525
[2025-06-27 14:57:43,449] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 14:57:43,450] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [460/483]  eta: 0:00:12  lr: 0.000007  min_lr: 0.000002  loss: 1.1852 (1.1359)  loss_scale: 16384.0000 (32483.6790)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5469 (5.3720)  time: 0.5052 (0.3013 -- 0.5874)  data: 0.0002 (0.0001 -- 0.0018)  max mem: 21487
Epoch: [25]  [480/483]  eta: 0:00:01  lr: 0.000006  min_lr: 0.000002  loss: 1.1031 (1.1341)  loss_scale: 16384.0000 (31814.2536)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9977 (5.3682)  time: 0.5164 (0.2851 -- 0.5975)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [25]  [482/483]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.1314 (1.1341)  loss_scale: 16384.0000 (31750.3602)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9977 (5.3692)  time: 0.5131 (0.2851 -- 0.5975)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [25] Total time: 0:04:17 (0.5330 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.1314 (1.1341)  loss_scale: 16384.0000 (31750.3602)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9977 (5.3692)
Val:  [ 0/23]  eta: 0:00:33  loss: 0.1720 (0.1720)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.4351 (1.4351 -- 1.4351)  data: 1.2344 (1.2344 -- 1.2344)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1720 (0.2145)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3002 (0.1239 -- 1.4351)  data: 0.1124 (0.0001 -- 1.2344)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1473 (0.2041)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.1926 (0.1239 -- 0.2113)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1473 (0.2086)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.1881 (0.1239 -- 0.2113)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:05 (0.2438 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.209
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [26]  [  0/483]  eta: 0:31:41  lr: 0.000006  min_lr: 0.000002  loss: 1.2788 (1.2788)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6577 (5.6577)  time: 3.9364 (3.9364 -- 3.9364)  data: 3.5096 (3.5096 -- 3.5096)  max mem: 21487
Epoch: [26]  [ 20/483]  eta: 0:05:14  lr: 0.000006  min_lr: 0.000002  loss: 1.1019 (1.1407)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3196 (5.5270)  time: 0.5158 (0.4499 -- 0.5905)  data: 0.0002 (0.0001 -- 0.0013)  max mem: 21487
Epoch: [26]  [ 40/483]  eta: 0:04:33  lr: 0.000006  min_lr: 0.000002  loss: 1.2363 (1.1725)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7826 (5.5017)  time: 0.5551 (0.4617 -- 0.6084)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [26]  [ 60/483]  eta: 0:04:09  lr: 0.000006  min_lr: 0.000002  loss: 1.1445 (1.1548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0773 (5.3665)  time: 0.5292 (0.4575 -- 0.5848)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [26]  [ 80/483]  eta: 0:03:51  lr: 0.000006  min_lr: 0.000002  loss: 1.1178 (1.1413)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7569 (5.3073)  time: 0.5251 (0.4473 -- 0.5984)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 14:59:00,088] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 14:59:00,089] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [100/483]  eta: 0:03:35  lr: 0.000006  min_lr: 0.000002  loss: 1.1460 (1.1514)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0773 (5.3147)  time: 0.5212 (0.4045 -- 0.6018)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [26]  [120/483]  eta: 0:03:21  lr: 0.000006  min_lr: 0.000002  loss: 1.2344 (1.1607)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8638 (5.4293)  time: 0.5177 (0.4155 -- 0.6097)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [26]  [140/483]  eta: 0:03:09  lr: 0.000006  min_lr: 0.000002  loss: 1.1587 (1.1634)  loss_scale: 32768.0000 (21612.9362)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5425 (5.3795)  time: 0.5415 (0.4755 -- 0.5987)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [26]  [160/483]  eta: 0:02:57  lr: 0.000006  min_lr: 0.000002  loss: 1.2056 (1.1664)  loss_scale: 32768.0000 (22998.6584)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6406 (5.3788)  time: 0.5334 (0.4520 -- 0.6061)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [26]  [180/483]  eta: 0:02:47  lr: 0.000006  min_lr: 0.000002  loss: 1.1832 (1.1650)  loss_scale: 32768.0000 (24078.1436)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5884 (5.3913)  time: 0.5629 (0.4393 -- 0.6293)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [26]  [200/483]  eta: 0:02:35  lr: 0.000006  min_lr: 0.000002  loss: 1.0155 (1.1552)  loss_scale: 32768.0000 (24942.8060)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4924 (5.4534)  time: 0.5134 (0.2849 -- 0.6073)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [26]  [220/483]  eta: 0:02:24  lr: 0.000006  min_lr: 0.000001  loss: 1.1413 (1.1540)  loss_scale: 32768.0000 (25650.9683)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6362 (5.5450)  time: 0.5474 (0.3493 -- 0.6049)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 15:00:08,842] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:00:08,842] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [240/483]  eta: 0:02:13  lr: 0.000006  min_lr: 0.000001  loss: 1.1156 (1.1547)  loss_scale: 65536.0000 (28553.0290)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6275 (5.5625)  time: 0.5514 (0.4911 -- 0.6033)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [26]  [260/483]  eta: 0:02:02  lr: 0.000006  min_lr: 0.000001  loss: 1.1131 (1.1549)  loss_scale: 65536.0000 (31386.9732)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0272 (5.4822)  time: 0.5470 (0.4405 -- 0.6202)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [26]  [280/483]  eta: 0:01:51  lr: 0.000006  min_lr: 0.000001  loss: 1.1611 (1.1534)  loss_scale: 65536.0000 (33817.5089)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6922 (5.4558)  time: 0.5284 (0.4133 -- 0.5973)  data: 0.0004 (0.0001 -- 0.0057)  max mem: 21487
Epoch: [26]  [300/483]  eta: 0:01:39  lr: 0.000006  min_lr: 0.000001  loss: 1.1219 (1.1538)  loss_scale: 65536.0000 (35925.0498)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9330 (5.4751)  time: 0.5373 (0.4707 -- 0.5997)  data: 0.0003 (0.0001 -- 0.0029)  max mem: 21487
Epoch: [26]  [320/483]  eta: 0:01:29  lr: 0.000006  min_lr: 0.000001  loss: 1.0937 (1.1566)  loss_scale: 65536.0000 (37769.9688)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5089 (5.4390)  time: 0.5537 (0.4936 -- 0.6051)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [26]  [340/483]  eta: 0:01:18  lr: 0.000006  min_lr: 0.000001  loss: 1.2192 (1.1565)  loss_scale: 65536.0000 (39398.4751)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1718 (5.3950)  time: 0.5499 (0.4439 -- 0.6228)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 15:01:18,195] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:01:18,195] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-06-27 15:01:19,291] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 12912
[2025-06-27 15:01:19,292] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 15:01:19,292] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-06-27 15:01:20,272] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 12914
[2025-06-27 15:01:20,272] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:01:20,273] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [360/483]  eta: 0:01:07  lr: 0.000006  min_lr: 0.000001  loss: 1.1789 (1.1593)  loss_scale: 65536.0000 (40755.7673)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2875 (5.3574)  time: 0.5223 (0.4501 -- 0.5966)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [26]  [380/483]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 1.1148 (1.1572)  loss_scale: 32768.0000 (40336.4619)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9575 (5.3460)  time: 0.5205 (0.4146 -- 0.5940)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 15:01:41,534] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 12954
[2025-06-27 15:01:41,535] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 15:01:41,535] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [400/483]  eta: 0:00:45  lr: 0.000005  min_lr: 0.000001  loss: 1.1768 (1.1547)  loss_scale: 32768.0000 (39754.6933)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4403 (5.3195)  time: 0.5561 (0.4472 -- 0.6186)  data: 0.0003 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [26]  [420/483]  eta: 0:00:34  lr: 0.000005  min_lr: 0.000001  loss: 1.1986 (1.1566)  loss_scale: 16384.0000 (38644.4466)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6860 (5.3508)  time: 0.5704 (0.4264 -- 0.6360)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [26]  [440/483]  eta: 0:00:23  lr: 0.000005  min_lr: 0.000001  loss: 1.0254 (1.1526)  loss_scale: 16384.0000 (37634.9025)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5080 (5.3583)  time: 0.5299 (0.4152 -- 0.6063)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 15:02:06,511] [INFO] [logging.py:107:log_dist] [Rank 0] step=13000, skipped=72, lr=[np.float64(1.3476913191265027e-06), np.float64(1.3476913191265027e-06), np.float64(1.4974347990294475e-06), np.float64(1.4974347990294475e-06), np.float64(1.6638164433660524e-06), np.float64(1.6638164433660524e-06), np.float64(1.8486849370733919e-06), np.float64(1.8486849370733919e-06), np.float64(2.0540943745259906e-06), np.float64(2.0540943745259906e-06), np.float64(2.2823270828066565e-06), np.float64(2.2823270828066565e-06), np.float64(2.535918980896285e-06), np.float64(2.535918980896285e-06), np.float64(2.817687756551427e-06), np.float64(2.817687756551427e-06), np.float64(3.1307641739460305e-06), np.float64(3.1307641739460305e-06), np.float64(3.4786268599400335e-06), np.float64(3.4786268599400335e-06), np.float64(3.865140955488926e-06), np.float64(3.865140955488926e-06), np.float64(4.2946010616543625e-06), np.float64(4.2946010616543625e-06), np.float64(4.771778957393736e-06), np.float64(4.771778957393736e-06), np.float64(5.301976619326373e-06), np.float64(5.301976619326373e-06)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 15:02:06,512] [INFO] [timer.py:264:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=12.1875345669906, CurrSamplesPerSec=14.278516977331446, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [26]  [460/483]  eta: 0:00:12  lr: 0.000005  min_lr: 0.000001  loss: 1.2022 (1.1536)  loss_scale: 16384.0000 (36712.9544)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2316 (5.3539)  time: 0.5155 (0.3439 -- 0.5755)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [26]  [480/483]  eta: 0:00:01  lr: 0.000005  min_lr: 0.000001  loss: 1.1940 (1.1556)  loss_scale: 16384.0000 (35867.6757)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2649 (5.3852)  time: 0.5277 (0.4532 -- 0.5915)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [26]  [482/483]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.1940 (1.1563)  loss_scale: 16384.0000 (35786.9979)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8152 (5.3814)  time: 0.5283 (0.4532 -- 0.5915)  data: 0.0003 (0.0001 -- 0.0047)  max mem: 21487
Epoch: [26] Total time: 0:04:22 (0.5434 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.1940 (1.1563)  loss_scale: 16384.0000 (35786.9979)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8152 (5.3814)
Val:  [ 0/23]  eta: 0:00:34  loss: 0.1846 (0.1846)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.4805 (1.4805 -- 1.4805)  data: 1.2678 (1.2678 -- 1.2678)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1846 (0.2176)  acc1: 91.6667 (95.4545)  acc5: 100.0000 (99.2424)  time: 0.3181 (0.1905 -- 1.4805)  data: 0.1155 (0.0002 -- 1.2678)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1461 (0.2082)  acc1: 100.0000 (96.0317)  acc5: 100.0000 (99.6032)  time: 0.2008 (0.1905 -- 0.2194)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1461 (0.2126)  acc1: 100.0000 (95.9410)  acc5: 100.0000 (99.6310)  time: 0.1975 (0.1248 -- 0.2194)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Val: Total time: 0:00:05 (0.2532 s / it)
* Acc@1 95.941 Acc@5 99.631 loss 0.213
Accuracy of the network on the 271 val images: 95.94%
Max accuracy: 97.42%
Epoch: [27]  [  0/483]  eta: 0:32:27  lr: 0.000005  min_lr: 0.000001  loss: 1.1363 (1.1363)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7014 (4.7014)  time: 4.0314 (4.0314 -- 4.0314)  data: 3.4719 (3.4719 -- 3.4719)  max mem: 21487
Epoch: [27]  [ 20/483]  eta: 0:05:24  lr: 0.000005  min_lr: 0.000001  loss: 1.1322 (1.1265)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9453 (5.4661)  time: 0.5347 (0.4789 -- 0.5890)  data: 0.0005 (0.0001 -- 0.0042)  max mem: 21487
Epoch: [27]  [ 40/483]  eta: 0:04:27  lr: 0.000005  min_lr: 0.000001  loss: 1.1358 (1.1336)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8708 (5.5355)  time: 0.4999 (0.3993 -- 0.5972)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 15:02:59,491] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:02:59,492] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [ 60/483]  eta: 0:04:04  lr: 0.000005  min_lr: 0.000001  loss: 1.2065 (1.1380)  loss_scale: 32768.0000 (21487.2131)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6428 (5.6243)  time: 0.5244 (0.4015 -- 0.6216)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [27]  [ 80/483]  eta: 0:03:48  lr: 0.000005  min_lr: 0.000001  loss: 1.0820 (1.1257)  loss_scale: 32768.0000 (24272.5926)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4907 (5.5136)  time: 0.5340 (0.4654 -- 0.6019)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [27]  [100/483]  eta: 0:03:34  lr: 0.000005  min_lr: 0.000001  loss: 1.1454 (1.1403)  loss_scale: 32768.0000 (25954.8515)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3376 (5.5247)  time: 0.5362 (0.4618 -- 0.6030)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [27]  [120/483]  eta: 0:03:21  lr: 0.000005  min_lr: 0.000001  loss: 1.1696 (1.1447)  loss_scale: 32768.0000 (27080.9917)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3461 (5.4958)  time: 0.5319 (0.4431 -- 0.6026)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [27]  [140/483]  eta: 0:03:11  lr: 0.000005  min_lr: 0.000001  loss: 1.2691 (1.1541)  loss_scale: 32768.0000 (27887.6596)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3994 (5.3918)  time: 0.5714 (0.4646 -- 0.6128)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [27]  [160/483]  eta: 0:02:59  lr: 0.000005  min_lr: 0.000001  loss: 1.0637 (1.1509)  loss_scale: 32768.0000 (28493.9130)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5966 (5.4078)  time: 0.5419 (0.4559 -- 0.6025)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 15:04:08,409] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:04:08,410] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [180/483]  eta: 0:02:47  lr: 0.000005  min_lr: 0.000001  loss: 1.2874 (1.1617)  loss_scale: 65536.0000 (30957.6133)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1377 (5.4127)  time: 0.5281 (0.4596 -- 0.6040)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 15:04:21,337] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 13235
[2025-06-27 15:04:21,338] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:04:21,338] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [200/483]  eta: 0:02:36  lr: 0.000005  min_lr: 0.000001  loss: 1.0657 (1.1522)  loss_scale: 65536.0000 (33257.0746)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8204 (5.3966)  time: 0.5464 (0.4576 -- 0.6273)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [27]  [220/483]  eta: 0:02:25  lr: 0.000005  min_lr: 0.000001  loss: 0.9508 (1.1490)  loss_scale: 32768.0000 (33212.8145)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9467 (5.4773)  time: 0.5542 (0.4206 -- 0.6118)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [27]  [240/483]  eta: 0:02:14  lr: 0.000005  min_lr: 0.000001  loss: 1.1339 (1.1500)  loss_scale: 32768.0000 (33175.9004)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3327 (5.4430)  time: 0.5418 (0.4556 -- 0.6055)  data: 0.0002 (0.0001 -- 0.0023)  max mem: 21487
Epoch: [27]  [260/483]  eta: 0:02:02  lr: 0.000005  min_lr: 0.000001  loss: 1.1714 (1.1499)  loss_scale: 32768.0000 (33144.6437)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5093 (5.4753)  time: 0.5217 (0.4199 -- 0.5992)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [27]  [280/483]  eta: 0:01:51  lr: 0.000005  min_lr: 0.000001  loss: 1.0283 (1.1454)  loss_scale: 32768.0000 (33117.8363)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0780 (5.5066)  time: 0.5277 (0.3990 -- 0.6026)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [27]  [300/483]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000001  loss: 1.1857 (1.1490)  loss_scale: 32768.0000 (33094.5914)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6897 (5.4805)  time: 0.5258 (0.3957 -- 0.6024)  data: 0.0003 (0.0001 -- 0.0042)  max mem: 21487
Epoch: [27]  [320/483]  eta: 0:01:28  lr: 0.000004  min_lr: 0.000001  loss: 1.1177 (1.1522)  loss_scale: 32768.0000 (33074.2430)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4524 (5.4725)  time: 0.5367 (0.4741 -- 0.5935)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
[2025-06-27 15:05:30,458] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:05:30,458] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [340/483]  eta: 0:01:17  lr: 0.000004  min_lr: 0.000001  loss: 1.1820 (1.1546)  loss_scale: 65536.0000 (34785.9707)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1386 (5.4774)  time: 0.5101 (0.4152 -- 0.5953)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 15:05:43,411] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 13389
[2025-06-27 15:05:43,411] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:05:43,412] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [360/483]  eta: 0:01:06  lr: 0.000004  min_lr: 0.000001  loss: 1.0518 (1.1502)  loss_scale: 32768.0000 (35309.5623)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5379 (5.4748)  time: 0.5429 (0.4167 -- 0.6370)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [27]  [380/483]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000001  loss: 1.0012 (1.1442)  loss_scale: 32768.0000 (35176.1470)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8851 (5.4713)  time: 0.5378 (0.4788 -- 0.5938)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [27]  [400/483]  eta: 0:00:45  lr: 0.000004  min_lr: 0.000001  loss: 1.1559 (1.1457)  loss_scale: 32768.0000 (35056.0399)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2724 (5.4209)  time: 0.5364 (0.4674 -- 0.6034)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [27]  [420/483]  eta: 0:00:34  lr: 0.000004  min_lr: 0.000001  loss: 1.0765 (1.1467)  loss_scale: 32768.0000 (34947.3444)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5259 (5.3936)  time: 0.5314 (0.4142 -- 0.6132)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [27]  [440/483]  eta: 0:00:23  lr: 0.000004  min_lr: 0.000001  loss: 1.1583 (1.1478)  loss_scale: 32768.0000 (34848.5079)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9640 (5.4127)  time: 0.5272 (0.3948 -- 0.6108)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [27]  [460/483]  eta: 0:00:12  lr: 0.000004  min_lr: 0.000001  loss: 1.1388 (1.1463)  loss_scale: 32768.0000 (34758.2473)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7297 (5.4067)  time: 0.5342 (0.4706 -- 0.5984)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 15:06:52,097] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:06:52,098] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [480/483]  eta: 0:00:01  lr: 0.000004  min_lr: 0.000001  loss: 1.0909 (1.1444)  loss_scale: 32768.0000 (34947.9917)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0643 (5.3882)  time: 0.5216 (0.4545 -- 0.5828)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [27]  [482/483]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.0900 (1.1445)  loss_scale: 32768.0000 (35074.6501)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9635 (5.3909)  time: 0.5273 (0.4545 -- 0.5828)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [27] Total time: 0:04:21 (0.5407 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.0900 (1.1445)  loss_scale: 32768.0000 (35074.6501)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9635 (5.3909)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.1854 (0.1854)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.3057 (1.3057 -- 1.3057)  data: 1.0955 (1.0955 -- 1.0955)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1854 (0.2185)  acc1: 91.6667 (95.4545)  acc5: 100.0000 (99.2424)  time: 0.2960 (0.1534 -- 1.3057)  data: 0.0998 (0.0002 -- 1.0955)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1475 (0.2088)  acc1: 100.0000 (96.0317)  acc5: 100.0000 (99.6032)  time: 0.1971 (0.1534 -- 0.2090)  data: 0.0003 (0.0002 -- 0.0004)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1475 (0.2138)  acc1: 100.0000 (95.9410)  acc5: 100.0000 (99.6310)  time: 0.1936 (0.1246 -- 0.2090)  data: 0.0003 (0.0002 -- 0.0004)  max mem: 21487
Val: Total time: 0:00:05 (0.2423 s / it)
* Acc@1 95.941 Acc@5 99.631 loss 0.214
Accuracy of the network on the 271 val images: 95.94%
Max accuracy: 97.42%
Epoch: [28]  [  0/483]  eta: 0:30:41  lr: 0.000004  min_lr: 0.000001  loss: 0.8311 (0.8311)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9369 (5.9369)  time: 3.8135 (3.8135 -- 3.8135)  data: 3.3082 (3.3082 -- 3.3082)  max mem: 21487
Epoch: [28]  [ 20/483]  eta: 0:05:12  lr: 0.000004  min_lr: 0.000001  loss: 1.1414 (1.1049)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3432 (5.5583)  time: 0.5183 (0.4472 -- 0.6010)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [28]  [ 40/483]  eta: 0:04:29  lr: 0.000004  min_lr: 0.000001  loss: 1.1207 (1.1204)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4243 (5.2396)  time: 0.5364 (0.4915 -- 0.6034)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [28]  [ 60/483]  eta: 0:04:04  lr: 0.000004  min_lr: 0.000001  loss: 1.1026 (1.1135)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5961 (5.4336)  time: 0.5160 (0.3946 -- 0.6007)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [28]  [ 80/483]  eta: 0:03:51  lr: 0.000004  min_lr: 0.000001  loss: 1.0303 (1.0983)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1441 (5.5691)  time: 0.5602 (0.4742 -- 0.6251)  data: 0.0003 (0.0001 -- 0.0021)  max mem: 21487
Epoch: [28]  [100/483]  eta: 0:03:36  lr: 0.000004  min_lr: 0.000001  loss: 1.1130 (1.1158)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3817 (5.4165)  time: 0.5315 (0.4724 -- 0.5898)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [28]  [120/483]  eta: 0:03:23  lr: 0.000004  min_lr: 0.000001  loss: 1.0292 (1.1088)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2982 (5.3545)  time: 0.5333 (0.4415 -- 0.5913)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 15:08:09,248] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:08:09,248] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-06-27 15:08:09,733] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 13647
[2025-06-27 15:08:09,733] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-06-27 15:08:09,733] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [140/483]  eta: 0:03:11  lr: 0.000004  min_lr: 0.000001  loss: 1.1576 (1.1073)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9453 (5.2834)  time: 0.5471 (0.4403 -- 0.6071)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [28]  [160/483]  eta: 0:02:58  lr: 0.000004  min_lr: 0.000001  loss: 1.2755 (1.1248)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9025 (5.2653)  time: 0.5265 (0.4489 -- 0.6023)  data: 0.0003 (0.0001 -- 0.0036)  max mem: 21487
Epoch: [28]  [180/483]  eta: 0:02:47  lr: 0.000004  min_lr: 0.000001  loss: 1.1237 (1.1211)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7913 (5.2776)  time: 0.5329 (0.3435 -- 0.6000)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [28]  [200/483]  eta: 0:02:34  lr: 0.000004  min_lr: 0.000001  loss: 1.1869 (1.1251)  loss_scale: 65536.0000 (65862.0498)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4305 (5.3366)  time: 0.5031 (0.4113 -- 0.5942)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 15:08:53,486] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 13730
[2025-06-27 15:08:53,486] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:08:53,486] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [220/483]  eta: 0:02:23  lr: 0.000004  min_lr: 0.000001  loss: 1.2489 (1.1315)  loss_scale: 32768.0000 (63608.4706)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7269 (5.4682)  time: 0.5329 (0.4075 -- 0.6332)  data: 0.0002 (0.0001 -- 0.0024)  max mem: 21487
Epoch: [28]  [240/483]  eta: 0:02:12  lr: 0.000004  min_lr: 0.000001  loss: 1.2183 (1.1392)  loss_scale: 32768.0000 (61049.0954)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9616 (5.5081)  time: 0.5386 (0.4482 -- 0.6014)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [28]  [260/483]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000001  loss: 1.1345 (1.1386)  loss_scale: 32768.0000 (58881.9617)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3311 (5.4952)  time: 0.5404 (0.4736 -- 0.6022)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [28]  [280/483]  eta: 0:01:50  lr: 0.000003  min_lr: 0.000001  loss: 1.1946 (1.1411)  loss_scale: 32768.0000 (57023.3167)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8828 (5.5371)  time: 0.5479 (0.4603 -- 0.6092)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [28]  [300/483]  eta: 0:01:39  lr: 0.000003  min_lr: 0.000001  loss: 1.1580 (1.1413)  loss_scale: 32768.0000 (55411.6678)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3743 (5.5211)  time: 0.5463 (0.4120 -- 0.6032)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [28]  [320/483]  eta: 0:01:28  lr: 0.000003  min_lr: 0.000001  loss: 1.2242 (1.1467)  loss_scale: 32768.0000 (54000.8474)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2294 (5.5555)  time: 0.5434 (0.4488 -- 0.6109)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 15:10:03,171] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:10:03,171] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-06-27 15:10:04,738] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 13862
[2025-06-27 15:10:04,738] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:10:04,738] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [340/483]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000001  loss: 1.1233 (1.1449)  loss_scale: 32768.0000 (53043.8006)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6221 (5.4719)  time: 0.5237 (0.4141 -- 0.5969)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [28]  [360/483]  eta: 0:01:07  lr: 0.000003  min_lr: 0.000001  loss: 1.0386 (1.1448)  loss_scale: 32768.0000 (51920.4875)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1740 (5.4807)  time: 0.5632 (0.4915 -- 0.6076)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [28]  [380/483]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000001  loss: 1.1186 (1.1438)  loss_scale: 32768.0000 (50915.1076)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1858 (5.4818)  time: 0.5156 (0.4412 -- 0.5976)  data: 0.0003 (0.0001 -- 0.0025)  max mem: 21487
Epoch: [28]  [400/483]  eta: 0:00:44  lr: 0.000003  min_lr: 0.000001  loss: 1.1670 (1.1402)  loss_scale: 32768.0000 (50010.0150)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8718 (5.4282)  time: 0.5105 (0.2808 -- 0.6512)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
[2025-06-27 15:10:42,415] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 13933
[2025-06-27 15:10:42,415] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 15:10:42,415] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [420/483]  eta: 0:00:34  lr: 0.000003  min_lr: 0.000001  loss: 1.0560 (1.1384)  loss_scale: 16384.0000 (48723.9145)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2807 (5.3957)  time: 0.5383 (0.4509 -- 0.5900)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [28]  [440/483]  eta: 0:00:23  lr: 0.000003  min_lr: 0.000001  loss: 1.0260 (1.1332)  loss_scale: 16384.0000 (47257.2517)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4721 (5.3629)  time: 0.5601 (0.4799 -- 0.6285)  data: 0.0003 (0.0001 -- 0.0026)  max mem: 21487
Epoch: [28]  [460/483]  eta: 0:00:12  lr: 0.000003  min_lr: 0.000001  loss: 1.1996 (1.1348)  loss_scale: 16384.0000 (45917.8482)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9341 (5.3425)  time: 0.5335 (0.4358 -- 0.5992)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 15:11:18,371] [INFO] [logging.py:107:log_dist] [Rank 0] step=14000, skipped=78, lr=[np.float64(7.697553147328493e-07), np.float64(7.697553147328493e-07), np.float64(8.552836830364993e-07), np.float64(8.552836830364993e-07), np.float64(9.50315203373888e-07), np.float64(9.50315203373888e-07), np.float64(1.0559057815265422e-06), np.float64(1.0559057815265422e-06), np.float64(1.1732286461406024e-06), np.float64(1.1732286461406024e-06), np.float64(1.3035873846006695e-06), np.float64(1.3035873846006695e-06), np.float64(1.448430427334077e-06), np.float64(1.448430427334077e-06), np.float64(1.6093671414823077e-06), np.float64(1.6093671414823077e-06), np.float64(1.7881857127581196e-06), np.float64(1.7881857127581196e-06), np.float64(1.9868730141756883e-06), np.float64(1.9868730141756883e-06), np.float64(2.2076366824174315e-06), np.float64(2.2076366824174315e-06), np.float64(2.4529296471304794e-06), np.float64(2.4529296471304794e-06), np.float64(2.7254773857005327e-06), np.float64(2.7254773857005327e-06), np.float64(3.028308206333925e-06), np.float64(3.028308206333925e-06)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 15:11:18,371] [INFO] [timer.py:264:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=12.365663673886688, CurrSamplesPerSec=15.866846223034441, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [28]  [480/483]  eta: 0:00:01  lr: 0.000003  min_lr: 0.000001  loss: 1.1505 (1.1372)  loss_scale: 16384.0000 (44689.8295)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8841 (5.3395)  time: 0.5439 (0.4624 -- 0.6093)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [28]  [482/483]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.1534 (1.1372)  loss_scale: 16384.0000 (44572.6211)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6144 (5.3362)  time: 0.5439 (0.4624 -- 0.6093)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [28] Total time: 0:04:21 (0.5421 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.1534 (1.1372)  loss_scale: 16384.0000 (44572.6211)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6144 (5.3362)
Val:  [ 0/23]  eta: 0:00:30  loss: 0.1693 (0.1693)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3361 (1.3361 -- 1.3361)  data: 1.1266 (1.1266 -- 1.1266)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1693 (0.2171)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3052 (0.1924 -- 1.3361)  data: 0.1026 (0.0001 -- 1.1266)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1375 (0.2038)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.1977 (0.1315 -- 0.2093)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1375 (0.2085)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.1935 (0.1183 -- 0.2093)  data: 0.0002 (0.0001 -- 0.0002)  max mem: 21487
Val: Total time: 0:00:05 (0.2440 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.209
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [29]  [  0/483]  eta: 0:34:24  lr: 0.000003  min_lr: 0.000001  loss: 0.8838 (0.8838)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2007 (4.2007)  time: 4.2751 (4.2751 -- 4.2751)  data: 3.7583 (3.7583 -- 3.7583)  max mem: 21487
Epoch: [29]  [ 20/483]  eta: 0:05:31  lr: 0.000003  min_lr: 0.000001  loss: 1.1044 (1.1349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1709 (4.7164)  time: 0.5373 (0.4539 -- 0.6041)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [29]  [ 40/483]  eta: 0:04:41  lr: 0.000003  min_lr: 0.000001  loss: 1.1500 (1.1486)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1469 (4.8179)  time: 0.5527 (0.5055 -- 0.5974)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 15:12:02,061] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:12:02,061] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 60/483]  eta: 0:04:15  lr: 0.000003  min_lr: 0.000001  loss: 1.1579 (1.1537)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9451 (4.9736)  time: 0.5397 (0.4615 -- 0.6079)  data: 0.0001 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [29]  [ 80/483]  eta: 0:04:00  lr: 0.000003  min_lr: 0.000001  loss: 1.1635 (1.1661)  loss_scale: 32768.0000 (21643.0617)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1117 (5.0937)  time: 0.5754 (0.3589 -- 0.8734)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [29]  [100/483]  eta: 0:03:43  lr: 0.000003  min_lr: 0.000001  loss: 1.1584 (1.1680)  loss_scale: 32768.0000 (23846.0198)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3130 (5.2849)  time: 0.5290 (0.4232 -- 0.6058)  data: 0.0001 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [29]  [120/483]  eta: 0:03:30  lr: 0.000003  min_lr: 0.000001  loss: 1.0965 (1.1608)  loss_scale: 32768.0000 (25320.7273)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7233 (5.4297)  time: 0.5561 (0.4467 -- 0.6051)  data: 0.0001 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [29]  [140/483]  eta: 0:03:17  lr: 0.000003  min_lr: 0.000001  loss: 1.2051 (1.1664)  loss_scale: 32768.0000 (26377.0780)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8629 (5.3858)  time: 0.5589 (0.4150 -- 0.6181)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [29]  [160/483]  eta: 0:03:04  lr: 0.000003  min_lr: 0.000001  loss: 1.1012 (1.1588)  loss_scale: 32768.0000 (27170.9814)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6496 (5.3695)  time: 0.5348 (0.4471 -- 0.6045)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [29]  [180/483]  eta: 0:02:51  lr: 0.000003  min_lr: 0.000001  loss: 1.1130 (1.1572)  loss_scale: 32768.0000 (27789.4365)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4567 (5.2933)  time: 0.5316 (0.3486 -- 0.6039)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 15:13:12,143] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:13:12,144] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-06-27 15:13:13,153] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 14192
[2025-06-27 15:13:13,153] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:13:13,153] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [200/483]  eta: 0:02:39  lr: 0.000003  min_lr: 0.000001  loss: 1.1258 (1.1539)  loss_scale: 32768.0000 (28610.8657)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8534 (5.3155)  time: 0.5272 (0.4620 -- 0.5962)  data: 0.0011 (0.0001 -- 0.0191)  max mem: 21487
Epoch: [29]  [220/483]  eta: 0:02:27  lr: 0.000003  min_lr: 0.000001  loss: 1.0984 (1.1465)  loss_scale: 32768.0000 (28987.0769)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0030 (5.2893)  time: 0.5352 (0.3947 -- 0.6328)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [29]  [240/483]  eta: 0:02:15  lr: 0.000003  min_lr: 0.000001  loss: 1.1721 (1.1541)  loss_scale: 32768.0000 (29300.8465)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0499 (5.3633)  time: 0.5333 (0.3978 -- 0.6061)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [29]  [260/483]  eta: 0:02:03  lr: 0.000003  min_lr: 0.000001  loss: 1.0958 (1.1484)  loss_scale: 32768.0000 (29566.5287)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6433 (5.3221)  time: 0.5017 (0.3555 -- 0.5965)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [29]  [280/483]  eta: 0:01:51  lr: 0.000002  min_lr: 0.000001  loss: 1.0959 (1.1485)  loss_scale: 32768.0000 (29794.3915)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8741 (5.2936)  time: 0.5242 (0.4342 -- 0.6040)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [29]  [300/483]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000001  loss: 1.1899 (1.1527)  loss_scale: 32768.0000 (29991.9734)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2190 (5.3322)  time: 0.5257 (0.4164 -- 0.6085)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 15:14:20,745] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:14:20,745] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [320/483]  eta: 0:01:29  lr: 0.000002  min_lr: 0.000001  loss: 1.1437 (1.1545)  loss_scale: 32768.0000 (30879.5016)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3448 (5.3607)  time: 0.5242 (0.3815 -- 0.6001)  data: 0.0003 (0.0001 -- 0.0017)  max mem: 21487
Epoch: [29]  [340/483]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000001  loss: 1.0993 (1.1540)  loss_scale: 65536.0000 (32912.1408)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2361 (5.3439)  time: 0.5269 (0.4787 -- 0.5959)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
[2025-06-27 15:14:36,484] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 14351
[2025-06-27 15:14:36,484] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:14:36,485] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [360/483]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000001  loss: 1.1717 (1.1542)  loss_scale: 32768.0000 (33176.4654)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9572 (5.3027)  time: 0.5048 (0.3812 -- 0.5885)  data: 0.0006 (0.0001 -- 0.0062)  max mem: 21487
Epoch: [29]  [380/483]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000001  loss: 1.0507 (1.1511)  loss_scale: 32768.0000 (33155.0236)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0147 (5.2400)  time: 0.5516 (0.4700 -- 0.6077)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [29]  [400/483]  eta: 0:00:45  lr: 0.000002  min_lr: 0.000001  loss: 1.1802 (1.1535)  loss_scale: 32768.0000 (33135.7207)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1580 (5.2372)  time: 0.5405 (0.4724 -- 0.6042)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [29]  [420/483]  eta: 0:00:34  lr: 0.000002  min_lr: 0.000001  loss: 1.1883 (1.1562)  loss_scale: 32768.0000 (33118.2518)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0953 (5.2390)  time: 0.5268 (0.4593 -- 0.6098)  data: 0.0005 (0.0001 -- 0.0075)  max mem: 21487
Epoch: [29]  [440/483]  eta: 0:00:23  lr: 0.000002  min_lr: 0.000001  loss: 1.1647 (1.1579)  loss_scale: 32768.0000 (33102.3673)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6258 (5.2257)  time: 0.5371 (0.4145 -- 0.6096)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [29]  [460/483]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000001  loss: 1.2061 (1.1587)  loss_scale: 32768.0000 (33087.8612)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9287 (5.2351)  time: 0.5489 (0.4651 -- 0.6209)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 15:15:45,801] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:15:45,801] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [480/483]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000001  loss: 1.1463 (1.1597)  loss_scale: 32768.0000 (33619.5593)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1941 (5.2264)  time: 0.5510 (0.3965 -- 0.6083)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [29]  [482/483]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.1882 (1.1611)  loss_scale: 32768.0000 (33751.7184)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6056 (5.2254)  time: 0.5512 (0.3965 -- 0.6083)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [29] Total time: 0:04:22 (0.5441 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.1882 (1.1611)  loss_scale: 32768.0000 (33751.7184)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6056 (5.2254)
[2025-06-27 15:15:50,783] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-29 is begin to save!
[2025-06-27 15:15:50,789] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-29/mp_rank_00_model_states.pt
Val:  [ 0/23]  eta: 0:00:29  loss: 0.1670 (0.1670)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3000 (1.3000 -- 1.3000)  data: 1.0976 (1.0976 -- 1.0976)  max mem: 21487
Val:  [10/23]  eta: 0:00:03  loss: 0.1670 (0.2155)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.2997 (0.1901 -- 1.3000)  data: 0.1000 (0.0002 -- 1.0976)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1365 (0.2034)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.1976 (0.1375 -- 0.2221)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1365 (0.2080)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.1946 (0.1206 -- 0.2221)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:05 (0.2428 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.208
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [30]  [  0/483]  eta: 0:33:01  lr: 0.000002  min_lr: 0.000001  loss: 1.6033 (1.6033)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3316 (4.3316)  time: 4.1019 (4.1019 -- 4.1019)  data: 3.6870 (3.6870 -- 3.6870)  max mem: 21487
Epoch: [30]  [ 20/483]  eta: 0:05:31  lr: 0.000002  min_lr: 0.000001  loss: 1.0668 (1.1240)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1578 (5.3117)  time: 0.5461 (0.4347 -- 0.6082)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [30]  [ 40/483]  eta: 0:04:33  lr: 0.000002  min_lr: 0.000001  loss: 1.1301 (1.1524)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7354 (5.0185)  time: 0.5160 (0.4486 -- 0.5858)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
Epoch: [30]  [ 60/483]  eta: 0:04:07  lr: 0.000002  min_lr: 0.000001  loss: 1.1014 (1.1364)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9187 (5.3639)  time: 0.5181 (0.3952 -- 0.5977)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 15:16:36,844] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 14551
[2025-06-27 15:16:36,845] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:16:36,845] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 80/483]  eta: 0:03:50  lr: 0.000002  min_lr: 0.000001  loss: 1.0946 (1.1236)  loss_scale: 32768.0000 (57445.1358)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7900 (5.2161)  time: 0.5336 (0.2850 -- 0.6244)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [30]  [100/483]  eta: 0:03:35  lr: 0.000002  min_lr: 0.000000  loss: 1.2081 (1.1261)  loss_scale: 32768.0000 (52558.5743)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7267 (5.3076)  time: 0.5175 (0.4522 -- 0.5976)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [30]  [120/483]  eta: 0:03:22  lr: 0.000002  min_lr: 0.000000  loss: 1.2137 (1.1423)  loss_scale: 32768.0000 (49287.4050)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3915 (5.3555)  time: 0.5379 (0.4708 -- 0.5987)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [30]  [140/483]  eta: 0:03:10  lr: 0.000002  min_lr: 0.000000  loss: 1.1628 (1.1482)  loss_scale: 32768.0000 (46944.2270)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8316 (5.2494)  time: 0.5337 (0.4129 -- 0.6229)  data: 0.0006 (0.0001 -- 0.0108)  max mem: 21487
Epoch: [30]  [160/483]  eta: 0:02:57  lr: 0.000002  min_lr: 0.000000  loss: 1.1156 (1.1426)  loss_scale: 32768.0000 (45183.2050)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6364 (5.3740)  time: 0.5228 (0.4605 -- 0.6028)  data: 0.0002 (0.0001 -- 0.0013)  max mem: 21487
Epoch: [30]  [180/483]  eta: 0:02:45  lr: 0.000002  min_lr: 0.000000  loss: 1.1461 (1.1412)  loss_scale: 32768.0000 (43811.3591)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7129 (5.3416)  time: 0.5129 (0.3956 -- 0.5980)  data: 0.0001 (0.0001 -- 0.0001)  max mem: 21487
[2025-06-27 15:17:44,777] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:17:44,777] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [200/483]  eta: 0:02:33  lr: 0.000002  min_lr: 0.000000  loss: 1.0124 (1.1365)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8204 (5.4036)  time: 0.5202 (0.4584 -- 0.5943)  data: 0.0002 (0.0001 -- 0.0012)  max mem: 21487
Epoch: [30]  [220/483]  eta: 0:02:23  lr: 0.000002  min_lr: 0.000000  loss: 0.9883 (1.1244)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5623 (5.5370)  time: 0.5530 (0.4554 -- 0.6244)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [30]  [240/483]  eta: 0:02:12  lr: 0.000002  min_lr: 0.000000  loss: 1.1100 (1.1266)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1325 (5.5215)  time: 0.5369 (0.3907 -- 0.6884)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 15:18:20,155] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 14746
[2025-06-27 15:18:20,155] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:18:20,155] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [260/483]  eta: 0:02:00  lr: 0.000002  min_lr: 0.000000  loss: 1.1197 (1.1296)  loss_scale: 65536.0000 (48712.5824)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4330 (5.5028)  time: 0.5241 (0.4330 -- 0.6071)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [30]  [280/483]  eta: 0:01:49  lr: 0.000002  min_lr: 0.000000  loss: 1.0904 (1.1286)  loss_scale: 32768.0000 (47577.7367)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8331 (5.5065)  time: 0.5170 (0.3933 -- 0.5987)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [30]  [300/483]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.1614 (1.1320)  loss_scale: 32768.0000 (46593.7010)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6798 (5.4803)  time: 0.5460 (0.3947 -- 0.6083)  data: 0.0004 (0.0001 -- 0.0041)  max mem: 21487
Epoch: [30]  [320/483]  eta: 0:01:27  lr: 0.000002  min_lr: 0.000000  loss: 1.2046 (1.1377)  loss_scale: 32768.0000 (45732.2866)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7585 (5.4678)  time: 0.5237 (0.4560 -- 0.5980)  data: 0.0005 (0.0001 -- 0.0039)  max mem: 21487
Epoch: [30]  [340/483]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000000  loss: 1.0027 (1.1296)  loss_scale: 32768.0000 (44971.9179)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0568 (5.4806)  time: 0.5454 (0.3854 -- 0.5974)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [30]  [360/483]  eta: 0:01:06  lr: 0.000002  min_lr: 0.000000  loss: 1.2342 (1.1339)  loss_scale: 32768.0000 (44295.8006)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0772 (5.4610)  time: 0.5418 (0.4642 -- 0.6251)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [30]  [380/483]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.0744 (1.1317)  loss_scale: 32768.0000 (43690.6667)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6255 (5.4458)  time: 0.5352 (0.3898 -- 0.5999)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 15:19:29,080] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:19:29,080] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-06-27 15:19:34,817] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 14886
[2025-06-27 15:19:34,817] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:19:34,817] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [400/483]  eta: 0:00:44  lr: 0.000001  min_lr: 0.000000  loss: 1.0464 (1.1291)  loss_scale: 65536.0000 (44044.7681)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8261 (5.4198)  time: 0.5182 (0.4206 -- 0.6036)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [30]  [420/483]  eta: 0:00:34  lr: 0.000001  min_lr: 0.000000  loss: 1.1038 (1.1315)  loss_scale: 32768.0000 (43509.0546)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4577 (5.4129)  time: 0.7630 (0.5274 -- 1.0074)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [30]  [440/483]  eta: 0:00:24  lr: 0.000001  min_lr: 0.000000  loss: 1.1410 (1.1309)  loss_scale: 32768.0000 (43021.9320)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7145 (5.3727)  time: 0.7606 (0.4144 -- 1.0106)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [30]  [460/483]  eta: 0:00:13  lr: 0.000001  min_lr: 0.000000  loss: 1.2062 (1.1327)  loss_scale: 32768.0000 (42577.0759)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4914 (5.3803)  time: 0.7904 (0.5932 -- 0.9845)  data: 0.0001 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [30]  [480/483]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000000  loss: 1.0874 (1.1331)  loss_scale: 32768.0000 (42169.2141)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3917 (5.3824)  time: 0.7723 (0.4992 -- 1.0111)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [30]  [482/483]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.0874 (1.1330)  loss_scale: 32768.0000 (42130.2857)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4040 (5.3915)  time: 0.7816 (0.4992 -- 1.0111)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [30] Total time: 0:04:39 (0.5789 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.0874 (1.1330)  loss_scale: 32768.0000 (42130.2857)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4040 (5.3915)
Val:  [ 0/23]  eta: 0:00:31  loss: 0.1580 (0.1580)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3624 (1.3624 -- 1.3624)  data: 1.0975 (1.0975 -- 1.0975)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1580 (0.2150)  acc1: 100.0000 (96.9697)  acc5: 100.0000 (99.2424)  time: 0.3595 (0.2518 -- 1.3624)  data: 0.1000 (0.0002 -- 1.0975)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1488 (0.2009)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.6032)  time: 0.2590 (0.2513 -- 0.2680)  data: 0.0002 (0.0002 -- 0.0004)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1488 (0.2050)  acc1: 100.0000 (96.6790)  acc5: 100.0000 (99.6310)  time: 0.2543 (0.1711 -- 0.2680)  data: 0.0002 (0.0002 -- 0.0004)  max mem: 21487
Val: Total time: 0:00:06 (0.3032 s / it)
* Acc@1 96.679 Acc@5 99.631 loss 0.205
Accuracy of the network on the 271 val images: 96.68%
Max accuracy: 97.42%
Epoch: [31]  [  0/483]  eta: 0:27:11  lr: 0.000001  min_lr: 0.000000  loss: 1.1785 (1.1785)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2544 (4.2544)  time: 3.3773 (3.3773 -- 3.3773)  data: 2.6740 (2.6740 -- 2.6740)  max mem: 21487
[2025-06-27 15:20:57,620] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 14982
[2025-06-27 15:20:57,621] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 15:20:57,621] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [ 20/483]  eta: 0:06:44  lr: 0.000001  min_lr: 0.000000  loss: 1.1476 (1.1391)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3483 (4.7756)  time: 0.7474 (0.5698 -- 1.0523)  data: 0.0123 (0.0001 -- 0.2426)  max mem: 21487
[2025-06-27 15:21:11,305] [INFO] [logging.py:107:log_dist] [Rank 0] step=15000, skipped=84, lr=[np.float64(3.419289419691632e-07), np.float64(3.419289419691632e-07), np.float64(3.799210466324036e-07), np.float64(3.799210466324036e-07), np.float64(4.221344962582262e-07), np.float64(4.221344962582262e-07), np.float64(4.6903832917580687e-07), np.float64(4.6903832917580687e-07), np.float64(5.211536990842299e-07), np.float64(5.211536990842299e-07), np.float64(5.790596656491442e-07), np.float64(5.790596656491442e-07), np.float64(6.433996284990491e-07), np.float64(6.433996284990491e-07), np.float64(7.148884761100546e-07), np.float64(7.148884761100546e-07), np.float64(7.943205290111717e-07), np.float64(7.943205290111717e-07), np.float64(8.825783655679685e-07), np.float64(8.825783655679685e-07), np.float64(9.80642628408854e-07), np.float64(9.80642628408854e-07), np.float64(1.0896029204542821e-06), np.float64(1.0896029204542821e-06), np.float64(1.210669911615869e-06), np.float64(1.210669911615869e-06), np.float64(1.345188790684299e-06), np.float64(1.345188790684299e-06)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 15:21:11,308] [INFO] [timer.py:264:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=12.490087475977829, CurrSamplesPerSec=8.155146157239253, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [31]  [ 40/483]  eta: 0:06:13  lr: 0.000001  min_lr: 0.000000  loss: 1.2083 (1.1714)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8928 (5.1436)  time: 0.8134 (0.6403 -- 1.0021)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [31]  [ 60/483]  eta: 0:05:47  lr: 0.000001  min_lr: 0.000000  loss: 1.0704 (1.1493)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9109 (5.0631)  time: 0.7739 (0.6034 -- 1.0002)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [31]  [ 80/483]  eta: 0:05:29  lr: 0.000001  min_lr: 0.000000  loss: 1.0514 (1.1357)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9814 (5.1549)  time: 0.8045 (0.6922 -- 0.9463)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [31]  [100/483]  eta: 0:05:11  lr: 0.000001  min_lr: 0.000000  loss: 1.0890 (1.1222)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0987 (5.1766)  time: 0.8058 (0.6033 -- 0.9946)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [31]  [120/483]  eta: 0:04:56  lr: 0.000001  min_lr: 0.000000  loss: 1.1128 (1.1314)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3967 (5.2933)  time: 0.8321 (0.4834 -- 1.0322)  data: 0.0002 (0.0001 -- 0.0016)  max mem: 21487
[2025-06-27 15:22:40,138] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:22:40,139] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [140/483]  eta: 0:04:38  lr: 0.000001  min_lr: 0.000000  loss: 1.2187 (1.1328)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8545 (5.3214)  time: 0.7885 (0.4108 -- 1.0245)  data: 0.0003 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [31]  [160/483]  eta: 0:04:22  lr: 0.000001  min_lr: 0.000000  loss: 1.0397 (1.1239)  loss_scale: 32768.0000 (19640.4472)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1692 (5.2839)  time: 0.8021 (0.6152 -- 1.0674)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [31]  [180/483]  eta: 0:04:06  lr: 0.000001  min_lr: 0.000000  loss: 1.2652 (1.1388)  loss_scale: 32768.0000 (21091.0055)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2812 (5.2122)  time: 0.8324 (0.6728 -- 0.9859)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [31]  [200/483]  eta: 0:03:49  lr: 0.000001  min_lr: 0.000000  loss: 1.0368 (1.1307)  loss_scale: 32768.0000 (22252.8955)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7526 (5.1580)  time: 0.7940 (0.6324 -- 1.0473)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [31]  [220/483]  eta: 0:03:33  lr: 0.000001  min_lr: 0.000000  loss: 1.1402 (1.1325)  loss_scale: 32768.0000 (23204.4887)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7666 (5.1317)  time: 0.8014 (0.5010 -- 1.0460)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [31]  [240/483]  eta: 0:03:17  lr: 0.000001  min_lr: 0.000000  loss: 1.0967 (1.1359)  loss_scale: 32768.0000 (23998.1411)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8525 (5.0930)  time: 0.8361 (0.6028 -- 1.0414)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [31]  [260/483]  eta: 0:03:00  lr: 0.000001  min_lr: 0.000000  loss: 1.0770 (1.1363)  loss_scale: 32768.0000 (24670.1609)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4202 (5.1158)  time: 0.7795 (0.5989 -- 0.9871)  data: 0.0002 (0.0001 -- 0.0012)  max mem: 21487
[2025-06-27 15:24:23,905] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:24:23,906] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [280/483]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.1810 (1.1404)  loss_scale: 65536.0000 (26995.7011)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7412 (5.2543)  time: 0.8719 (0.6360 -- 1.0346)  data: 0.0003 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [31]  [300/483]  eta: 0:02:28  lr: 0.000001  min_lr: 0.000000  loss: 1.3058 (1.1477)  loss_scale: 65536.0000 (29556.5183)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1789 (5.3005)  time: 0.7953 (0.6420 -- 0.9856)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [31]  [320/483]  eta: 0:02:12  lr: 0.000001  min_lr: 0.000000  loss: 1.1295 (1.1517)  loss_scale: 65536.0000 (31798.2305)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8796 (5.2601)  time: 0.8387 (0.6925 -- 1.0135)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [31]  [340/483]  eta: 0:01:56  lr: 0.000001  min_lr: 0.000000  loss: 1.0984 (1.1467)  loss_scale: 65536.0000 (33776.9853)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5637 (5.2575)  time: 0.8160 (0.5636 -- 1.0300)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [31]  [360/483]  eta: 0:01:40  lr: 0.000001  min_lr: 0.000000  loss: 1.0953 (1.1418)  loss_scale: 65536.0000 (35536.4875)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3716 (5.2258)  time: 0.8100 (0.6085 -- 1.0329)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [31]  [380/483]  eta: 0:01:24  lr: 0.000001  min_lr: 0.000000  loss: 1.0130 (1.1383)  loss_scale: 65536.0000 (37111.2651)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6829 (5.2273)  time: 0.8444 (0.6091 -- 1.0586)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 15:26:06,359] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 15363
[2025-06-27 15:26:06,359] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:26:06,360] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [400/483]  eta: 0:01:07  lr: 0.000001  min_lr: 0.000000  loss: 1.1599 (1.1402)  loss_scale: 32768.0000 (37630.0848)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4368 (5.2013)  time: 0.7877 (0.5468 -- 1.0120)  data: 0.0002 (0.0001 -- 0.0012)  max mem: 21487
Epoch: [31]  [420/483]  eta: 0:00:51  lr: 0.000001  min_lr: 0.000000  loss: 1.1003 (1.1401)  loss_scale: 32768.0000 (37399.1069)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5404 (5.2074)  time: 0.8337 (0.6437 -- 1.0430)  data: 0.0003 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [31]  [440/483]  eta: 0:00:35  lr: 0.000001  min_lr: 0.000000  loss: 1.0652 (1.1380)  loss_scale: 32768.0000 (37189.0794)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3021 (5.2103)  time: 0.8009 (0.5915 -- 0.9981)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [31]  [460/483]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.0422 (1.1349)  loss_scale: 32768.0000 (36997.2755)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0325 (5.2029)  time: 0.8412 (0.6262 -- 1.0462)  data: 0.0003 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [31]  [480/483]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000000  loss: 1.0743 (1.1342)  loss_scale: 32768.0000 (36821.4220)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7495 (5.2119)  time: 0.7781 (0.6077 -- 1.0011)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [31]  [482/483]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.1612 (1.1352)  loss_scale: 32768.0000 (36804.6377)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1205 (5.2207)  time: 0.7753 (0.6077 -- 1.0011)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [31] Total time: 0:06:33 (0.8146 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.1612 (1.1352)  loss_scale: 32768.0000 (36804.6377)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1205 (5.2207)
Val:  [ 0/23]  eta: 0:00:34  loss: 0.1605 (0.1605)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.4797 (1.4797 -- 1.4797)  data: 1.2118 (1.2118 -- 1.2118)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1605 (0.2145)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3697 (0.2377 -- 1.4797)  data: 0.1104 (0.0002 -- 1.2118)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1430 (0.2010)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.2446 (0.1649 -- 0.2740)  data: 0.0002 (0.0002 -- 0.0003)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1430 (0.2053)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.2400 (0.1649 -- 0.2740)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Val: Total time: 0:00:06 (0.2958 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.205
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [32]  [  0/483]  eta: 0:31:34  lr: 0.000001  min_lr: 0.000000  loss: 1.6644 (1.6644)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4851 (5.4851)  time: 3.9215 (3.9215 -- 3.9215)  data: 2.9418 (2.9418 -- 2.9418)  max mem: 21487
Epoch: [32]  [ 20/483]  eta: 0:07:13  lr: 0.000001  min_lr: 0.000000  loss: 1.0023 (1.0900)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2189 (5.5679)  time: 0.7881 (0.6217 -- 1.0091)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 15:28:00,206] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:28:00,207] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 40/483]  eta: 0:06:29  lr: 0.000001  min_lr: 0.000000  loss: 1.2343 (1.1478)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6473 (5.4334)  time: 0.8200 (0.4725 -- 1.0326)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [32]  [ 60/483]  eta: 0:06:05  lr: 0.000001  min_lr: 0.000000  loss: 1.1343 (1.1464)  loss_scale: 65536.0000 (46197.5082)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1686 (5.7081)  time: 0.8304 (0.6407 -- 1.0472)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [32]  [ 80/483]  eta: 0:05:42  lr: 0.000001  min_lr: 0.000000  loss: 1.1324 (1.1485)  loss_scale: 65536.0000 (50972.4444)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1245 (5.5029)  time: 0.8028 (0.6507 -- 1.0326)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [32]  [100/483]  eta: 0:05:18  lr: 0.000001  min_lr: 0.000000  loss: 1.1886 (1.1593)  loss_scale: 65536.0000 (53856.3168)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3706 (5.4418)  time: 0.7649 (0.6093 -- 0.9957)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
[2025-06-27 15:29:01,492] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 15568
[2025-06-27 15:29:01,493] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:29:01,493] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [120/483]  eta: 0:05:01  lr: 0.000001  min_lr: 0.000000  loss: 1.1719 (1.1603)  loss_scale: 65536.0000 (53349.5537)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7531 (5.5159)  time: 0.8213 (0.5798 -- 1.0222)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [32]  [140/483]  eta: 0:04:44  lr: 0.000001  min_lr: 0.000000  loss: 1.0048 (1.1427)  loss_scale: 32768.0000 (50430.1844)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4953 (5.3859)  time: 0.8308 (0.4128 -- 1.0121)  data: 0.0002 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [32]  [160/483]  eta: 0:04:27  lr: 0.000001  min_lr: 0.000000  loss: 1.1236 (1.1458)  loss_scale: 32768.0000 (48236.1242)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7451 (5.4145)  time: 0.8127 (0.5983 -- 0.9916)  data: 0.0003 (0.0001 -- 0.0013)  max mem: 21487
Epoch: [32]  [180/483]  eta: 0:04:10  lr: 0.000001  min_lr: 0.000000  loss: 1.1860 (1.1509)  loss_scale: 32768.0000 (46526.9392)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7329 (5.4165)  time: 0.8279 (0.6371 -- 1.0091)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [32]  [200/483]  eta: 0:03:52  lr: 0.000001  min_lr: 0.000000  loss: 1.0257 (1.1429)  loss_scale: 32768.0000 (45157.8905)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4061 (5.4404)  time: 0.7506 (0.6354 -- 0.9686)  data: 0.0001 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [32]  [220/483]  eta: 0:03:36  lr: 0.000001  min_lr: 0.000000  loss: 1.1785 (1.1463)  loss_scale: 32768.0000 (44036.6335)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5865 (5.4994)  time: 0.8467 (0.6477 -- 1.0305)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [32]  [240/483]  eta: 0:03:19  lr: 0.000001  min_lr: 0.000000  loss: 1.1274 (1.1462)  loss_scale: 32768.0000 (43101.4772)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3835 (5.4892)  time: 0.7780 (0.5086 -- 1.0350)  data: 0.0001 (0.0001 -- 0.0008)  max mem: 21487
[2025-06-27 15:30:45,955] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:30:45,955] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [260/483]  eta: 0:03:02  lr: 0.000001  min_lr: 0.000000  loss: 1.2240 (1.1465)  loss_scale: 65536.0000 (44820.5977)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8443 (5.4577)  time: 0.8352 (0.6456 -- 1.0010)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [32]  [280/483]  eta: 0:02:46  lr: 0.000001  min_lr: 0.000000  loss: 1.1805 (1.1476)  loss_scale: 65536.0000 (46295.0036)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8830 (5.5113)  time: 0.7975 (0.5788 -- 1.0465)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 15:31:24,776] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 15745
[2025-06-27 15:31:24,776] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:31:24,776] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [300/483]  eta: 0:02:30  lr: 0.000001  min_lr: 0.000000  loss: 1.2252 (1.1546)  loss_scale: 32768.0000 (46267.1096)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1734 (5.5662)  time: 0.8545 (0.5801 -- 1.0103)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [32]  [320/483]  eta: 0:02:13  lr: 0.000000  min_lr: 0.000000  loss: 1.1753 (1.1570)  loss_scale: 32768.0000 (45426.0436)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4136 (5.6190)  time: 0.8357 (0.6603 -- 1.0043)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [32]  [340/483]  eta: 0:01:57  lr: 0.000000  min_lr: 0.000000  loss: 1.0504 (1.1525)  loss_scale: 32768.0000 (44683.6364)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.3773 (5.5745)  time: 0.8061 (0.6322 -- 1.0313)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [32]  [360/483]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.1443 (1.1536)  loss_scale: 32768.0000 (44023.4903)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7260 (5.6131)  time: 0.8168 (0.5784 -- 0.9616)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 15:32:39,171] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 15835
[2025-06-27 15:32:39,172] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 15:32:39,172] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [380/483]  eta: 0:01:24  lr: 0.000000  min_lr: 0.000000  loss: 1.1255 (1.1522)  loss_scale: 32768.0000 (43346.6457)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7958 (5.6084)  time: 0.8127 (0.5978 -- 1.0273)  data: 0.0003 (0.0001 -- 0.0011)  max mem: 21487
Epoch: [32]  [400/483]  eta: 0:01:08  lr: 0.000000  min_lr: 0.000000  loss: 1.1239 (1.1501)  loss_scale: 16384.0000 (42001.8753)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3136 (5.5478)  time: 0.8193 (0.6059 -- 1.0006)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [32]  [420/483]  eta: 0:00:51  lr: 0.000000  min_lr: 0.000000  loss: 1.1802 (1.1529)  loss_scale: 16384.0000 (40784.8741)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0043 (5.5497)  time: 0.8300 (0.6736 -- 1.0242)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [32]  [440/483]  eta: 0:00:35  lr: 0.000000  min_lr: 0.000000  loss: 1.1646 (1.1543)  loss_scale: 16384.0000 (39678.2585)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3256 (5.5718)  time: 0.7475 (0.5655 -- 0.9964)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [32]  [460/483]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.0961 (1.1521)  loss_scale: 16384.0000 (38667.6616)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2211 (5.6035)  time: 0.8074 (0.6538 -- 0.9850)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [32]  [480/483]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.0040 (1.1483)  loss_scale: 16384.0000 (37741.1060)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7816 (5.6256)  time: 0.8377 (0.6067 -- 1.0065)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [32]  [482/483]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.1192 (1.1483)  loss_scale: 16384.0000 (37652.6708)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1695 (5.6189)  time: 0.8227 (0.6067 -- 1.0065)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [32] Total time: 0:06:35 (0.8178 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.1192 (1.1483)  loss_scale: 16384.0000 (37652.6708)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1695 (5.6189)
Val:  [ 0/23]  eta: 0:00:31  loss: 0.1585 (0.1585)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.3512 (1.3512 -- 1.3512)  data: 1.0966 (1.0966 -- 1.0966)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1585 (0.2146)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3598 (0.2419 -- 1.3512)  data: 0.0999 (0.0002 -- 1.0966)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1452 (0.2004)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.2575 (0.1791 -- 0.2811)  data: 0.0003 (0.0002 -- 0.0005)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1452 (0.2047)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.2525 (0.1791 -- 0.2798)  data: 0.0003 (0.0002 -- 0.0005)  max mem: 21487
Val: Total time: 0:00:06 (0.3022 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.205
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [33]  [  0/483]  eta: 0:32:39  lr: 0.000000  min_lr: 0.000000  loss: 1.3887 (1.3887)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2720 (7.2720)  time: 4.0564 (4.0564 -- 4.0564)  data: 3.2487 (3.2487 -- 3.2487)  max mem: 21487
Epoch: [33]  [ 20/483]  eta: 0:07:29  lr: 0.000000  min_lr: 0.000000  loss: 1.0107 (1.0692)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5253 (4.6736)  time: 0.8162 (0.6080 -- 1.0376)  data: 0.0004 (0.0001 -- 0.0047)  max mem: 21487
[2025-06-27 15:34:34,219] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:34:34,219] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [ 40/483]  eta: 0:06:22  lr: 0.000000  min_lr: 0.000000  loss: 1.1824 (1.1136)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2803 (4.8801)  time: 0.7500 (0.4237 -- 1.0122)  data: 0.0010 (0.0001 -- 0.0085)  max mem: 21487
[2025-06-27 15:35:01,956] [INFO] [logging.py:107:log_dist] [Rank 0] step=16000, skipped=88, lr=[np.float64(8.424439948742461e-08), np.float64(8.424439948742461e-08), np.float64(9.360488831936067e-08), np.float64(9.360488831936067e-08), np.float64(1.0400543146595629e-07), np.float64(1.0400543146595629e-07), np.float64(1.1556159051772922e-07), np.float64(1.1556159051772922e-07), np.float64(1.2840176724192136e-07), np.float64(1.2840176724192136e-07), np.float64(1.4266863026880149e-07), np.float64(1.4266863026880149e-07), np.float64(1.5852070029866833e-07), np.float64(1.5852070029866833e-07), np.float64(1.7613411144296479e-07), np.float64(1.7613411144296479e-07), np.float64(1.9570456826996087e-07), np.float64(1.9570456826996087e-07), np.float64(2.174495202999565e-07), np.float64(2.174495202999565e-07), np.float64(2.4161057811106284e-07), np.float64(2.4161057811106284e-07), np.float64(2.684561979011809e-07), np.float64(2.684561979011809e-07), np.float64(2.9828466433464543e-07), np.float64(2.9828466433464543e-07), np.float64(3.314274048162727e-07), np.float64(3.314274048162727e-07)], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-06-27 15:35:01,959] [INFO] [timer.py:264:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=12.298253446800944, CurrSamplesPerSec=8.032383452285268, MemAllocated=1.25GB, MaxMemAllocated=20.98GB
Epoch: [33]  [ 60/483]  eta: 0:06:03  lr: 0.000000  min_lr: 0.000000  loss: 1.1237 (1.1120)  loss_scale: 32768.0000 (26053.2459)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5374 (4.9777)  time: 0.8488 (0.6144 -- 1.0057)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [33]  [ 80/483]  eta: 0:05:40  lr: 0.000000  min_lr: 0.000000  loss: 1.0661 (1.1063)  loss_scale: 32768.0000 (27711.2099)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1352 (4.8399)  time: 0.8051 (0.6005 -- 0.9822)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [33]  [100/483]  eta: 0:05:21  lr: 0.000000  min_lr: 0.000000  loss: 1.2366 (1.1263)  loss_scale: 32768.0000 (28712.5545)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2869 (4.9359)  time: 0.8198 (0.6564 -- 1.0094)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [33]  [120/483]  eta: 0:05:02  lr: 0.000000  min_lr: 0.000000  loss: 1.1164 (1.1193)  loss_scale: 32768.0000 (29382.8760)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0803 (5.0475)  time: 0.7927 (0.6158 -- 1.0158)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 15:35:53,490] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 16063
[2025-06-27 15:35:53,490] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 15:35:53,490] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [140/483]  eta: 0:04:39  lr: 0.000000  min_lr: 0.000000  loss: 1.1946 (1.1272)  loss_scale: 16384.0000 (27887.6596)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9527 (5.0375)  time: 0.7038 (0.4275 -- 1.0158)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [33]  [160/483]  eta: 0:04:22  lr: 0.000000  min_lr: 0.000000  loss: 1.0669 (1.1225)  loss_scale: 16384.0000 (26458.6335)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4376 (5.0443)  time: 0.8101 (0.5904 -- 0.9861)  data: 0.0002 (0.0001 -- 0.0010)  max mem: 21487
Epoch: [33]  [180/483]  eta: 0:04:07  lr: 0.000000  min_lr: 0.000000  loss: 1.1773 (1.1318)  loss_scale: 16384.0000 (25345.4144)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2329 (5.0492)  time: 0.8290 (0.6225 -- 1.0012)  data: 0.0002 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [33]  [200/483]  eta: 0:03:51  lr: 0.000000  min_lr: 0.000000  loss: 1.1097 (1.1338)  loss_scale: 16384.0000 (24453.7313)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4173 (5.1343)  time: 0.8387 (0.6304 -- 1.0279)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
Epoch: [33]  [220/483]  eta: 0:03:35  lr: 0.000000  min_lr: 0.000000  loss: 1.1586 (1.1364)  loss_scale: 16384.0000 (23723.4389)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3199 (5.1795)  time: 0.8477 (0.6377 -- 0.9907)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [33]  [240/483]  eta: 0:03:19  lr: 0.000000  min_lr: 0.000000  loss: 1.0728 (1.1374)  loss_scale: 16384.0000 (23114.3568)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2872 (5.1883)  time: 0.8398 (0.5326 -- 0.9990)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
[2025-06-27 15:37:37,825] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:37:37,826] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [260/483]  eta: 0:03:02  lr: 0.000000  min_lr: 0.000000  loss: 1.1649 (1.1394)  loss_scale: 16384.0000 (23100.8123)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0928 (5.1881)  time: 0.8004 (0.5934 -- 1.0003)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [33]  [280/483]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.1250 (1.1403)  loss_scale: 32768.0000 (23788.8683)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5309 (5.2952)  time: 0.8071 (0.6317 -- 1.0269)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [33]  [300/483]  eta: 0:02:29  lr: 0.000000  min_lr: 0.000000  loss: 1.1326 (1.1417)  loss_scale: 32768.0000 (24385.4884)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2184 (5.3171)  time: 0.7746 (0.6271 -- 0.9723)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [33]  [320/483]  eta: 0:02:12  lr: 0.000000  min_lr: 0.000000  loss: 1.1972 (1.1452)  loss_scale: 32768.0000 (24907.7632)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3477 (5.3543)  time: 0.8063 (0.5490 -- 0.9878)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [33]  [340/483]  eta: 0:01:56  lr: 0.000000  min_lr: 0.000000  loss: 1.0591 (1.1413)  loss_scale: 32768.0000 (25368.7742)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5537 (5.3612)  time: 0.8398 (0.5187 -- 1.0624)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
Epoch: [33]  [360/483]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.1822 (1.1421)  loss_scale: 32768.0000 (25778.7036)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7203 (5.3564)  time: 0.7788 (0.6315 -- 1.0009)  data: 0.0003 (0.0001 -- 0.0025)  max mem: 21487
Epoch: [33]  [380/483]  eta: 0:01:24  lr: 0.000000  min_lr: 0.000000  loss: 1.2087 (1.1457)  loss_scale: 32768.0000 (26145.5958)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6542 (5.4181)  time: 0.8394 (0.6085 -- 1.0171)  data: 0.0002 (0.0001 -- 0.0009)  max mem: 21487
[2025-06-27 15:39:21,553] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:39:21,554] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [400/483]  eta: 0:01:07  lr: 0.000000  min_lr: 0.000000  loss: 1.1116 (1.1444)  loss_scale: 65536.0000 (28110.2045)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1097 (5.3548)  time: 0.8049 (0.5831 -- 1.0135)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
[2025-06-27 15:39:47,054] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 16353
[2025-06-27 15:39:47,054] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:39:47,054] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [420/483]  eta: 0:00:51  lr: 0.000000  min_lr: 0.000000  loss: 1.0787 (1.1430)  loss_scale: 65536.0000 (29343.3159)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3603 (5.3766)  time: 0.7549 (0.5258 -- 0.9960)  data: 0.0003 (0.0001 -- 0.0013)  max mem: 21487
Epoch: [33]  [440/483]  eta: 0:00:35  lr: 0.000000  min_lr: 0.000000  loss: 1.0801 (1.1426)  loss_scale: 32768.0000 (29498.6304)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0765 (5.3545)  time: 0.8422 (0.6568 -- 0.9870)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [33]  [460/483]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.0851 (1.1405)  loss_scale: 32768.0000 (29640.4685)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7628 (5.3462)  time: 0.8261 (0.6381 -- 1.0190)  data: 0.0001 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [33]  [480/483]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.0265 (1.1394)  loss_scale: 32768.0000 (29770.5114)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.7377 (5.3406)  time: 0.8252 (0.6526 -- 0.9942)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [33]  [482/483]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.0982 (1.1402)  loss_scale: 32768.0000 (29782.9234)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9101 (5.3740)  time: 0.8326 (0.6526 -- 0.9942)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
Epoch: [33] Total time: 0:06:33 (0.8157 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.0982 (1.1402)  loss_scale: 32768.0000 (29782.9234)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9101 (5.3740)
Val:  [ 0/23]  eta: 0:00:32  loss: 0.1579 (0.1579)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.4316 (1.4316 -- 1.4316)  data: 1.2195 (1.2195 -- 1.2195)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1579 (0.2145)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3463 (0.1428 -- 1.4316)  data: 0.1114 (0.0002 -- 1.2195)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1465 (0.2002)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.2498 (0.1428 -- 0.2703)  data: 0.0004 (0.0001 -- 0.0036)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1465 (0.2044)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.2501 (0.1380 -- 0.2703)  data: 0.0003 (0.0001 -- 0.0036)  max mem: 21487
Val: Total time: 0:00:06 (0.2971 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.204
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Epoch: [34]  [  0/483]  eta: 0:28:57  lr: 0.000000  min_lr: 0.000000  loss: 0.9546 (0.9546)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3133 (6.3133)  time: 3.5982 (3.5982 -- 3.5982)  data: 2.8510 (2.8510 -- 2.8510)  max mem: 21487
Epoch: [34]  [ 20/483]  eta: 0:07:17  lr: 0.000000  min_lr: 0.000000  loss: 1.0930 (1.1281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7610 (5.4317)  time: 0.8132 (0.6020 -- 1.2731)  data: 0.0143 (0.0001 -- 0.2822)  max mem: 21487
Epoch: [34]  [ 40/483]  eta: 0:06:13  lr: 0.000000  min_lr: 0.000000  loss: 1.0926 (1.1240)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4739 (5.6360)  time: 0.7334 (0.5471 -- 1.0076)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 15:41:27,413] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 16465
[2025-06-27 15:41:27,414] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 15:41:27,414] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 60/483]  eta: 0:05:52  lr: 0.000000  min_lr: 0.000000  loss: 1.1856 (1.1429)  loss_scale: 16384.0000 (27933.3770)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6754 (5.4575)  time: 0.8132 (0.5769 -- 1.0045)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [34]  [ 80/483]  eta: 0:05:33  lr: 0.000000  min_lr: 0.000000  loss: 1.0928 (1.1427)  loss_scale: 16384.0000 (25081.6790)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4985 (5.6301)  time: 0.8127 (0.6155 -- 1.0228)  data: 0.0003 (0.0001 -- 0.0031)  max mem: 21487
Epoch: [34]  [100/483]  eta: 0:05:16  lr: 0.000000  min_lr: 0.000000  loss: 1.1552 (1.1486)  loss_scale: 16384.0000 (23359.3663)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5437 (5.7262)  time: 0.8147 (0.6832 -- 0.9918)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [34]  [120/483]  eta: 0:04:56  lr: 0.000000  min_lr: 0.000000  loss: 1.1814 (1.1535)  loss_scale: 16384.0000 (22206.4132)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1802 (5.7121)  time: 0.7788 (0.6602 -- 1.0009)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [34]  [140/483]  eta: 0:04:41  lr: 0.000000  min_lr: 0.000000  loss: 1.0416 (1.1445)  loss_scale: 16384.0000 (21380.5390)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9503 (5.6594)  time: 0.8336 (0.6643 -- 1.0121)  data: 0.0003 (0.0001 -- 0.0026)  max mem: 21487
Epoch: [34]  [160/483]  eta: 0:04:23  lr: 0.000000  min_lr: 0.000000  loss: 1.1605 (1.1456)  loss_scale: 16384.0000 (20759.8509)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9370 (5.6757)  time: 0.7871 (0.5989 -- 1.0304)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 15:43:12,183] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:43:12,183] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [180/483]  eta: 0:04:08  lr: 0.000000  min_lr: 0.000000  loss: 1.1824 (1.1457)  loss_scale: 16384.0000 (21091.0055)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7760 (5.6530)  time: 0.8648 (0.6231 -- 1.0241)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [34]  [200/483]  eta: 0:03:52  lr: 0.000000  min_lr: 0.000000  loss: 1.1070 (1.1416)  loss_scale: 32768.0000 (22252.8955)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8332 (5.6588)  time: 0.8096 (0.6167 -- 1.0121)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [34]  [220/483]  eta: 0:03:35  lr: 0.000000  min_lr: 0.000000  loss: 1.1061 (1.1396)  loss_scale: 32768.0000 (23204.4887)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5640 (5.6598)  time: 0.8317 (0.6623 -- 1.0264)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [34]  [240/483]  eta: 0:03:19  lr: 0.000000  min_lr: 0.000000  loss: 1.1884 (1.1410)  loss_scale: 32768.0000 (23998.1411)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5341 (5.5945)  time: 0.8297 (0.6061 -- 1.0178)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
Epoch: [34]  [260/483]  eta: 0:03:03  lr: 0.000000  min_lr: 0.000000  loss: 1.1100 (1.1368)  loss_scale: 32768.0000 (24670.1609)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7125 (5.5650)  time: 0.8279 (0.6496 -- 0.9658)  data: 0.0002 (0.0001 -- 0.0007)  max mem: 21487
Epoch: [34]  [280/483]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.1473 (1.1399)  loss_scale: 32768.0000 (25246.5196)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2586 (5.5651)  time: 0.8236 (0.6416 -- 1.0593)  data: 0.0001 (0.0001 -- 0.0003)  max mem: 21487
[2025-06-27 15:44:56,842] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:44:56,842] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [300/483]  eta: 0:02:29  lr: 0.000000  min_lr: 0.000000  loss: 1.1561 (1.1434)  loss_scale: 32768.0000 (25855.1495)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4959 (5.6029)  time: 0.7675 (0.4633 -- 0.9868)  data: 0.0009 (0.0001 -- 0.0091)  max mem: 21487
Epoch: [34]  [320/483]  eta: 0:02:13  lr: 0.000000  min_lr: 0.000000  loss: 1.1525 (1.1438)  loss_scale: 65536.0000 (28327.4766)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5374 (5.5445)  time: 0.8557 (0.6596 -- 1.0278)  data: 0.0002 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 15:45:16,898] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 16746
[2025-06-27 15:45:16,898] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-06-27 15:45:16,898] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [340/483]  eta: 0:01:57  lr: 0.000000  min_lr: 0.000000  loss: 1.2346 (1.1443)  loss_scale: 32768.0000 (28876.1994)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5893 (5.5082)  time: 0.7915 (0.5395 -- 1.0364)  data: 0.0001 (0.0001 -- 0.0004)  max mem: 21487
[2025-06-27 15:45:38,881] [INFO] [fused_optimizer.py:404:_update_scale] 
Grad overflow on iteration 16773
[2025-06-27 15:45:38,881] [INFO] [fused_optimizer.py:405:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-06-27 15:45:38,881] [INFO] [logging.py:107:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [360/483]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.2250 (1.1489)  loss_scale: 16384.0000 (28637.9612)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5722 (5.4803)  time: 0.8295 (0.6056 -- 1.0024)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [34]  [380/483]  eta: 0:01:24  lr: 0.000000  min_lr: 0.000000  loss: 0.9954 (1.1417)  loss_scale: 16384.0000 (27994.7087)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9381 (5.4268)  time: 0.8496 (0.6708 -- 1.0333)  data: 0.0001 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [34]  [400/483]  eta: 0:01:08  lr: 0.000000  min_lr: 0.000000  loss: 1.0063 (1.1387)  loss_scale: 16384.0000 (27415.6209)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0956 (5.4270)  time: 0.8313 (0.6210 -- 1.0258)  data: 0.0002 (0.0001 -- 0.0005)  max mem: 21487
Epoch: [34]  [420/483]  eta: 0:00:51  lr: 0.000000  min_lr: 0.000000  loss: 1.2284 (1.1439)  loss_scale: 16384.0000 (26891.5534)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9297 (5.4371)  time: 0.8380 (0.6527 -- 1.0217)  data: 0.0002 (0.0001 -- 0.0008)  max mem: 21487
Epoch: [34]  [440/483]  eta: 0:00:35  lr: 0.000000  min_lr: 0.000000  loss: 1.0524 (1.1425)  loss_scale: 16384.0000 (26415.0204)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7256 (5.4543)  time: 0.8079 (0.5979 -- 1.0194)  data: 0.0003 (0.0001 -- 0.0036)  max mem: 21487
Epoch: [34]  [460/483]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.1451 (1.1442)  loss_scale: 16384.0000 (25979.8351)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6899 (5.4717)  time: 0.8504 (0.6025 -- 1.0381)  data: 0.0002 (0.0001 -- 0.0006)  max mem: 21487
[2025-06-27 15:47:25,577] [INFO] [fused_optimizer.py:412:_update_scale] No Grad overflow for 128 iterations
[2025-06-27 15:47:25,577] [INFO] [fused_optimizer.py:413:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [480/483]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 1.1278 (1.1430)  loss_scale: 16384.0000 (25614.9023)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1846 (5.4715)  time: 0.7811 (0.6146 -- 0.9867)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [34]  [482/483]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.1363 (1.1442)  loss_scale: 16384.0000 (25644.5217)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1846 (5.4704)  time: 0.8042 (0.6146 -- 0.9867)  data: 0.0001 (0.0001 -- 0.0002)  max mem: 21487
Epoch: [34] Total time: 0:06:37 (0.8221 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.1363 (1.1442)  loss_scale: 16384.0000 (25644.5217)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1846 (5.4704)
[2025-06-27 15:47:27,549] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-34 is begin to save!
[2025-06-27 15:47:27,553] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./work_dir/checkpoint-34/mp_rank_00_model_states.pt
Val:  [ 0/23]  eta: 0:00:33  loss: 0.1576 (0.1576)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.4617 (1.4617 -- 1.4617)  data: 1.2181 (1.2181 -- 1.2181)  max mem: 21487
Val:  [10/23]  eta: 0:00:04  loss: 0.1576 (0.2146)  acc1: 100.0000 (96.2121)  acc5: 100.0000 (99.2424)  time: 0.3742 (0.2024 -- 1.4617)  data: 0.1207 (0.0002 -- 1.2181)  max mem: 21487
Val:  [20/23]  eta: 0:00:00  loss: 0.1472 (0.2001)  acc1: 100.0000 (96.4286)  acc5: 100.0000 (99.6032)  time: 0.2638 (0.2024 -- 0.3876)  data: 0.0056 (0.0002 -- 0.1073)  max mem: 21487
Val:  [22/23]  eta: 0:00:00  loss: 0.1472 (0.2044)  acc1: 100.0000 (96.3100)  acc5: 100.0000 (99.6310)  time: 0.2547 (0.1758 -- 0.2777)  data: 0.0002 (0.0002 -- 0.0004)  max mem: 21487
Val: Total time: 0:00:07 (0.3127 s / it)
* Acc@1 96.310 Acc@5 99.631 loss 0.204
Accuracy of the network on the 271 val images: 96.31%
Max accuracy: 97.42%
Test:  [  0/509]  eta: 0:34:35  loss: 0.2868 (0.2868)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 4.0767 (4.0767 -- 4.0767)  data: 3.2310 (3.2310 -- 3.2310)  max mem: 21487
Test:  [ 10/509]  eta: 0:04:38  loss: 0.3390 (0.3560)  acc1: 87.5000 (90.9091)  acc5: 100.0000 (98.8636)  time: 0.5584 (0.1045 -- 4.0767)  data: 0.3253 (0.0002 -- 3.2310)  max mem: 21487
Test:  [ 20/509]  eta: 0:03:32  loss: 0.3390 (0.3712)  acc1: 87.5000 (89.8810)  acc5: 100.0000 (98.8095)  time: 0.2517 (0.1045 -- 1.3831)  data: 0.0793 (0.0002 -- 1.2342)  max mem: 21487
Test:  [ 30/509]  eta: 0:02:57  loss: 0.1610 (0.3129)  acc1: 100.0000 (92.7419)  acc5: 100.0000 (98.7903)  time: 0.2680 (0.1084 -- 1.3831)  data: 0.1124 (0.0002 -- 1.2342)  max mem: 21487
Test:  [ 40/509]  eta: 0:02:55  loss: 0.1512 (0.3180)  acc1: 100.0000 (91.7683)  acc5: 100.0000 (98.7805)  time: 0.3129 (0.1084 -- 1.5619)  data: 0.1561 (0.0002 -- 1.3710)  max mem: 21487
Test:  [ 50/509]  eta: 0:02:37  loss: 0.3139 (0.3257)  acc1: 87.5000 (91.4216)  acc5: 100.0000 (99.0196)  time: 0.3004 (0.1312 -- 1.5619)  data: 0.1227 (0.0002 -- 1.3710)  max mem: 21487
Test:  [ 60/509]  eta: 0:02:26  loss: 0.1590 (0.2988)  acc1: 100.0000 (92.6230)  acc5: 100.0000 (98.9754)  time: 0.2268 (0.1553 -- 0.7289)  data: 0.0458 (0.0002 -- 0.5689)  max mem: 21487
Test:  [ 70/509]  eta: 0:02:18  loss: 0.1381 (0.3006)  acc1: 100.0000 (92.9577)  acc5: 100.0000 (98.7676)  time: 0.2449 (0.1553 -- 0.8806)  data: 0.0623 (0.0002 -- 0.6724)  max mem: 21487
Test:  [ 80/509]  eta: 0:02:14  loss: 0.2778 (0.3082)  acc1: 87.5000 (92.7469)  acc5: 100.0000 (98.9198)  time: 0.2734 (0.1455 -- 0.8806)  data: 0.0934 (0.0002 -- 0.6775)  max mem: 21487
Test:  [ 90/509]  eta: 0:02:10  loss: 0.1838 (0.3016)  acc1: 87.5000 (92.8571)  acc5: 100.0000 (98.9011)  time: 0.2928 (0.1455 -- 1.2630)  data: 0.1139 (0.0002 -- 1.0810)  max mem: 21487
Test:  [100/509]  eta: 0:02:04  loss: 0.1455 (0.2979)  acc1: 100.0000 (93.1931)  acc5: 100.0000 (98.8861)  time: 0.2729 (0.1501 -- 1.2630)  data: 0.0909 (0.0002 -- 1.0810)  max mem: 21487
Test:  [110/509]  eta: 0:01:59  loss: 0.1507 (0.2979)  acc1: 100.0000 (93.2432)  acc5: 100.0000 (98.8739)  time: 0.2540 (0.1562 -- 0.9152)  data: 0.0722 (0.0002 -- 0.7309)  max mem: 21487
Test:  [120/509]  eta: 0:01:58  loss: 0.1741 (0.2989)  acc1: 100.0000 (93.0785)  acc5: 100.0000 (98.8636)  time: 0.2959 (0.1576 -- 1.0380)  data: 0.1135 (0.0002 -- 0.8456)  max mem: 21487
Test:  [130/509]  eta: 0:01:54  loss: 0.1522 (0.2912)  acc1: 100.0000 (93.3206)  acc5: 100.0000 (98.9504)  time: 0.3037 (0.1539 -- 1.0550)  data: 0.1230 (0.0002 -- 0.8999)  max mem: 21487
Test:  [140/509]  eta: 0:01:50  loss: 0.1509 (0.2922)  acc1: 100.0000 (93.1738)  acc5: 100.0000 (98.9362)  time: 0.2666 (0.1539 -- 1.0550)  data: 0.0869 (0.0002 -- 0.8999)  max mem: 21487
Test:  [150/509]  eta: 0:01:46  loss: 0.2188 (0.2910)  acc1: 87.5000 (93.2119)  acc5: 100.0000 (98.9238)  time: 0.2614 (0.1257 -- 1.0401)  data: 0.0838 (0.0002 -- 0.8364)  max mem: 21487
Test:  [160/509]  eta: 0:01:43  loss: 0.1660 (0.2831)  acc1: 100.0000 (93.4783)  acc5: 100.0000 (98.9907)  time: 0.2907 (0.1257 -- 1.1058)  data: 0.1111 (0.0002 -- 0.8984)  max mem: 21487
Test:  [170/509]  eta: 0:01:39  loss: 0.1538 (0.2787)  acc1: 100.0000 (93.7135)  acc5: 100.0000 (98.9766)  time: 0.2736 (0.1487 -- 1.1058)  data: 0.0906 (0.0002 -- 0.8984)  max mem: 21487
Test:  [180/509]  eta: 0:01:35  loss: 0.1591 (0.2782)  acc1: 100.0000 (93.7155)  acc5: 100.0000 (99.0331)  time: 0.2459 (0.1481 -- 1.1421)  data: 0.0693 (0.0002 -- 0.9546)  max mem: 21487
Test:  [190/509]  eta: 0:01:31  loss: 0.2029 (0.2787)  acc1: 100.0000 (93.7827)  acc5: 100.0000 (99.0183)  time: 0.2454 (0.1242 -- 1.1421)  data: 0.0742 (0.0002 -- 0.9546)  max mem: 21487
Test:  [200/509]  eta: 0:01:29  loss: 0.1620 (0.2727)  acc1: 100.0000 (93.9677)  acc5: 100.0000 (99.0672)  time: 0.2547 (0.1242 -- 0.8338)  data: 0.0811 (0.0002 -- 0.6537)  max mem: 21487
Test:  [210/509]  eta: 0:01:25  loss: 0.1317 (0.2707)  acc1: 100.0000 (94.1351)  acc5: 100.0000 (99.1114)  time: 0.2593 (0.1594 -- 0.8338)  data: 0.0815 (0.0002 -- 0.6537)  max mem: 21487
Test:  [220/509]  eta: 0:01:23  loss: 0.1588 (0.2761)  acc1: 100.0000 (93.8914)  acc5: 100.0000 (99.0385)  time: 0.2844 (0.1594 -- 1.1220)  data: 0.1074 (0.0002 -- 0.9598)  max mem: 21487
Test:  [230/509]  eta: 0:01:20  loss: 0.1390 (0.2705)  acc1: 100.0000 (94.1017)  acc5: 100.0000 (99.0801)  time: 0.3029 (0.1359 -- 1.1220)  data: 0.1282 (0.0002 -- 0.9598)  max mem: 21487
Test:  [240/509]  eta: 0:01:16  loss: 0.1325 (0.2689)  acc1: 100.0000 (94.1909)  acc5: 100.0000 (99.1183)  time: 0.2649 (0.1359 -- 0.8048)  data: 0.0900 (0.0002 -- 0.6414)  max mem: 21487
Test:  [250/509]  eta: 0:01:14  loss: 0.1631 (0.2689)  acc1: 100.0000 (94.1733)  acc5: 100.0000 (99.1036)  time: 0.2737 (0.1574 -- 0.9302)  data: 0.0948 (0.0002 -- 0.7391)  max mem: 21487
Test:  [260/509]  eta: 0:01:10  loss: 0.1471 (0.2657)  acc1: 100.0000 (94.3008)  acc5: 100.0000 (99.1379)  time: 0.2739 (0.1509 -- 1.0281)  data: 0.0961 (0.0002 -- 0.8697)  max mem: 21487
Test:  [270/509]  eta: 0:01:08  loss: 0.1218 (0.2624)  acc1: 100.0000 (94.4188)  acc5: 100.0000 (99.1697)  time: 0.2689 (0.1509 -- 1.1484)  data: 0.0935 (0.0002 -- 0.9956)  max mem: 21487
Test:  [280/509]  eta: 0:01:05  loss: 0.1319 (0.2614)  acc1: 100.0000 (94.4840)  acc5: 100.0000 (99.1993)  time: 0.2880 (0.1526 -- 1.1484)  data: 0.1092 (0.0002 -- 0.9956)  max mem: 21487
Test:  [290/509]  eta: 0:01:02  loss: 0.1875 (0.2630)  acc1: 100.0000 (94.4158)  acc5: 100.0000 (99.1838)  time: 0.2658 (0.1493 -- 0.8333)  data: 0.0818 (0.0002 -- 0.6350)  max mem: 21487
Test:  [300/509]  eta: 0:00:58  loss: 0.1449 (0.2595)  acc1: 100.0000 (94.5598)  acc5: 100.0000 (99.2110)  time: 0.2335 (0.1493 -- 0.7031)  data: 0.0494 (0.0002 -- 0.5347)  max mem: 21487
Test:  [310/509]  eta: 0:00:55  loss: 0.1352 (0.2603)  acc1: 100.0000 (94.5740)  acc5: 100.0000 (99.1961)  time: 0.2209 (0.1522 -- 0.7031)  data: 0.0444 (0.0002 -- 0.5347)  max mem: 21487
Test:  [320/509]  eta: 0:00:52  loss: 0.1608 (0.2617)  acc1: 100.0000 (94.4704)  acc5: 100.0000 (99.1822)  time: 0.2596 (0.1522 -- 0.9864)  data: 0.0838 (0.0002 -- 0.8196)  max mem: 21487
Test:  [330/509]  eta: 0:00:49  loss: 0.1505 (0.2596)  acc1: 100.0000 (94.5619)  acc5: 100.0000 (99.2069)  time: 0.2668 (0.1525 -- 0.9864)  data: 0.0869 (0.0002 -- 0.8196)  max mem: 21487
Test:  [340/509]  eta: 0:00:47  loss: 0.1445 (0.2597)  acc1: 100.0000 (94.5381)  acc5: 100.0000 (99.1935)  time: 0.2438 (0.0895 -- 0.5902)  data: 0.0690 (0.0002 -- 0.4082)  max mem: 21487
Test:  [350/509]  eta: 0:00:44  loss: 0.1656 (0.2616)  acc1: 100.0000 (94.5157)  acc5: 100.0000 (99.2165)  time: 0.2582 (0.0895 -- 0.5902)  data: 0.0891 (0.0002 -- 0.4323)  max mem: 21487
Test:  [360/509]  eta: 0:00:41  loss: 0.1855 (0.2607)  acc1: 100.0000 (94.5637)  acc5: 100.0000 (99.2036)  time: 0.2768 (0.1398 -- 0.7346)  data: 0.1016 (0.0002 -- 0.5544)  max mem: 21487
Test:  [370/509]  eta: 0:00:38  loss: 0.1393 (0.2597)  acc1: 100.0000 (94.6092)  acc5: 100.0000 (99.2251)  time: 0.2622 (0.1622 -- 0.7346)  data: 0.0834 (0.0002 -- 0.5544)  max mem: 21487
Test:  [380/509]  eta: 0:00:35  loss: 0.1457 (0.2622)  acc1: 100.0000 (94.5210)  acc5: 100.0000 (99.2126)  time: 0.2723 (0.1597 -- 0.8651)  data: 0.0958 (0.0002 -- 0.6716)  max mem: 21487
Test:  [390/509]  eta: 0:00:32  loss: 0.2703 (0.2636)  acc1: 87.5000 (94.4373)  acc5: 100.0000 (99.2008)  time: 0.2565 (0.1552 -- 0.8651)  data: 0.0805 (0.0002 -- 0.6716)  max mem: 21487
Test:  [400/509]  eta: 0:00:30  loss: 0.1661 (0.2613)  acc1: 100.0000 (94.5137)  acc5: 100.0000 (99.1895)  time: 0.2591 (0.1365 -- 1.0683)  data: 0.0825 (0.0002 -- 0.8681)  max mem: 21487
Test:  [410/509]  eta: 0:00:27  loss: 0.1392 (0.2620)  acc1: 100.0000 (94.4951)  acc5: 100.0000 (99.2092)  time: 0.2851 (0.1365 -- 1.0683)  data: 0.1044 (0.0002 -- 0.8681)  max mem: 21487
Test:  [420/509]  eta: 0:00:24  loss: 0.2351 (0.2648)  acc1: 87.5000 (94.3587)  acc5: 100.0000 (99.1983)  time: 0.2448 (0.1360 -- 0.9737)  data: 0.0672 (0.0002 -- 0.7627)  max mem: 21487
Test:  [430/509]  eta: 0:00:21  loss: 0.2057 (0.2636)  acc1: 100.0000 (94.4316)  acc5: 100.0000 (99.2169)  time: 0.2381 (0.1360 -- 0.9737)  data: 0.0630 (0.0002 -- 0.7627)  max mem: 21487
Test:  [440/509]  eta: 0:00:18  loss: 0.2021 (0.2640)  acc1: 100.0000 (94.4161)  acc5: 100.0000 (99.2063)  time: 0.2609 (0.0978 -- 1.0242)  data: 0.0888 (0.0002 -- 0.8511)  max mem: 21487
Test:  [450/509]  eta: 0:00:16  loss: 0.1978 (0.2656)  acc1: 100.0000 (94.4013)  acc5: 100.0000 (99.1962)  time: 0.2800 (0.0978 -- 1.0242)  data: 0.1020 (0.0002 -- 0.8511)  max mem: 21487
Test:  [460/509]  eta: 0:00:13  loss: 0.1978 (0.2646)  acc1: 100.0000 (94.4143)  acc5: 100.0000 (99.2137)  time: 0.2637 (0.1597 -- 1.0154)  data: 0.0808 (0.0002 -- 0.8522)  max mem: 21487
Test:  [470/509]  eta: 0:00:10  loss: 0.1376 (0.2635)  acc1: 100.0000 (94.4533)  acc5: 100.0000 (99.2038)  time: 0.2656 (0.1597 -- 1.0535)  data: 0.0866 (0.0002 -- 0.8760)  max mem: 21487
Test:  [480/509]  eta: 0:00:07  loss: 0.1428 (0.2631)  acc1: 100.0000 (94.4647)  acc5: 100.0000 (99.2204)  time: 0.2716 (0.1141 -- 1.0535)  data: 0.0932 (0.0002 -- 0.8760)  max mem: 21487
Test:  [490/509]  eta: 0:00:05  loss: 0.1879 (0.2629)  acc1: 100.0000 (94.4756)  acc5: 100.0000 (99.2363)  time: 0.2552 (0.1141 -- 0.7678)  data: 0.0775 (0.0002 -- 0.5831)  max mem: 21487
Test:  [500/509]  eta: 0:00:02  loss: 0.1645 (0.2605)  acc1: 100.0000 (94.5858)  acc5: 100.0000 (99.2515)  time: 0.2278 (0.1511 -- 0.7432)  data: 0.0498 (0.0002 -- 0.5626)  max mem: 21487
Test:  [508/509]  eta: 0:00:00  loss: 0.1377 (0.2605)  acc1: 100.0000 (94.5879)  acc5: 100.0000 (99.2620)  time: 0.2353 (0.1546 -- 0.8656)  data: 0.0554 (0.0002 -- 0.6736)  max mem: 21487
Test: Total time: 0:02:18 (0.2723 s / it)
* Acc@1 94.588 Acc@5 99.262 loss 0.261
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 4065 test videos: Top-1: 98.52%, Top-5: 100.00%
Training time 3:17:30
